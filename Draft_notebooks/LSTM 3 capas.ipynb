{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bbd3d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0459f861",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../raw_data/df_limpio_con_df_lunes_limpio.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2726926c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.Tweet\n",
    "y = data.label\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X , y ,test_size = 0.3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80f4d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "X_train = [word_tokenize(str(_)) for _ in X_train]\n",
    "X_test = [word_tokenize(str(_)) for _ in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e81e432",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "info = api.info()  # show info about available models/datasets\n",
    "model = api.load(\"glove-twitter-25\")  # download the model and return as object ready for use\n",
    "model.most_similar(\"depression\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f08572a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "tk = Tokenizer()\n",
    "tk.fit_on_texts(X)\n",
    "vocab_size = len(tk.word_index)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6be5ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_sentence(model, sentence):\n",
    "    embedded_sentence = []\n",
    "    for word in sentence:\n",
    "        if word in model:\n",
    "            embedded_sentence.append(model[word])\n",
    "        \n",
    "    return np.array(embedded_sentence)\n",
    "def embedding(model, sentences):\n",
    "    \n",
    "    embed = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        embedded_sentence = embed_sentence(model, sentence)\n",
    "        embed.append(embedded_sentence)\n",
    "        \n",
    "    return embed\n",
    "    \n",
    "X_train = embedding(model, X_train)\n",
    "X_test = embedding(model, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0adc927b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pad = pad_sequences(X_train, dtype='float', padding='post', maxlen= 400,truncating= 'post')\n",
    "X_test_pad = pad_sequences(X_test, dtype='float', padding='post', maxlen = 400 , truncating = 'post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1fb167",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, Sequential\n",
    "\n",
    "embedding_size = 100\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Embedding(input_dim=vocab_size+1, \n",
    "                          input_length=140, \n",
    "                          output_dim=embedding_size,\n",
    "                          mask_zero=True))\n",
    "model.add(layers.LSTM(50, return_sequences=True))\n",
    "model.add(layers.Dense(100, activation=\"relu\"))\n",
    "model.add(layers.Dense(1, activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d786398",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30fd222a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy', 'AUC','Precision','Recall'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d6e35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "es = EarlyStopping(patience=3, restore_best_weights=True)\n",
    "\n",
    "history_fit = model.fit(X_train_pad, y_train, \n",
    "          batch_size = 32,\n",
    "          epochs=100,\n",
    "          validation_split=0.3,\n",
    "          callbacks=[es],\n",
    "          verbose = 1\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ca870f",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_fit.score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c431b4d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b312019c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import transformers as ppb\n",
    "# from transformers import BertTokenizer\n",
    "\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased',\n",
    "#                                          do_lower_case = True)\n",
    "# encoded_data_train = tokenizer.batch_encode_plus(X_train.text.values,\n",
    "#                                                 add_special_tokes = True,\n",
    "#                                                 return_attention_mask = True,\n",
    "#                                                 pad_to_max_length = True,\n",
    "#                                                 max_length = 256,\n",
    "#                                                 return_tensors = 'pt')\n",
    "# input_ids_train = encoded_data_train['input_ids']\n",
    "# attention_masks_train = encoded_data_train['attention_mask']\n",
    "# labels_train = torch.tensor(X_train.label.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b0b597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import BertForSequenceClassification\n",
    "\n",
    "# model = BertForSequenceClassification.from_pretrained('bert-base-uncased',\n",
    "#                                                       num_labels = [0 , 1],\n",
    "#                                                       output_attentions = False,\n",
    "#                                                       output_hidden_states = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ef58cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f8e2d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
