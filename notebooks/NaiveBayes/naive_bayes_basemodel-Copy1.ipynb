{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6054c8d2",
   "metadata": {},
   "source": [
    "# Bring data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "526689db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline \n",
    "from sklearn.model_selection import GridSearchCV , cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "93cdc1b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PHQ8_Binary</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>302.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>i'm fine how about yourself  . i'm from los an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>&lt;laughter&gt; . um moscow . um my family moved to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>yes  . okay  . connecticut . um  . to be an ac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335.0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>yes . i'm okay  . uh i'm from here originally ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346.0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>yes  . i'm okay  . here in los angeles  . ther...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            PHQ8_Binary                                               text\n",
       "Unnamed: 0                                                                \n",
       "302.0               0.0  i'm fine how about yourself  . i'm from los an...\n",
       "307.0               0.0  <laughter> . um moscow . um my family moved to...\n",
       "331.0               0.0  yes  . okay  . connecticut . um  . to be an ac...\n",
       "335.0               1.0  yes . i'm okay  . uh i'm from here originally ...\n",
       "346.0               1.0  yes  . i'm okay  . here in los angeles  . ther..."
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#bring your data!\n",
    "path = '../../../transcriptions/targets/transcriptions_targets.csv'\n",
    "data = pd.read_csv(f'{path}').set_index('Unnamed: 0')\n",
    "data.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "4851faa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PHQ8_Binary    42\n",
       "text           42\n",
       "dtype: int64"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[(data.PHQ8_Binary == 1)].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "538b89e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_col = 'text' #input('choose the text column of your data')\n",
    "X = data[f'{text_col}']\n",
    "\n",
    "target_col = 'PHQ8_Binary' #input('choose the target column of your data')\n",
    "y = data[f'{target_col}']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4476e5b0",
   "metadata": {},
   "source": [
    "# preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "19f2e0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "84bc829d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_contractions(text):\n",
    "    \"\"\" Replace contractions in the english language by the complete phrase\"\"\"\n",
    "    # Contraction dictionary\n",
    "    contractions = {\n",
    "      \"ain't\": \"am not\",\n",
    "      \"aren't\": \"are not\",\n",
    "      \"can't\": \"cannot\",\n",
    "      \"can't've\": \"cannot have\",\n",
    "      \"'cause\": \"because\",\n",
    "      \"could've\": \"could have\",\n",
    "      \"couldn't\": \"could not\",\n",
    "      \"couldn't've\": \"could not have\",\n",
    "      \"didn't\": \"did not\",\n",
    "      \"doesn't\": \"does not\",\n",
    "      \"don't\": \"do not\",\n",
    "      \"hadn't\": \"had not\",\n",
    "      \"hadn't've\": \"had not have\",\n",
    "      \"hasn't\": \"has not\",\n",
    "      \"haven't\": \"have not\",\n",
    "      \"he'd\": \"he would\",\n",
    "      \"he'd've\": \"he would have\",\n",
    "      \"he'll\": \"he will\",\n",
    "      \"he'll've\": \"he will have\",\n",
    "      \"he's\": \"he is\",\n",
    "      \"how'd\": \"how did\",\n",
    "      \"how'd'y\": \"how do you\",\n",
    "      \"how'll\": \"how will\",\n",
    "      \"how's\": \"how is\",\n",
    "      \"I'd\": \"I would\",\n",
    "      \"I'd've\": \"I would have\",\n",
    "      \"I'll\": \"I will\",\n",
    "      \"I'll've\": \"I will have\",\n",
    "      \"I'm\": \"I am\",\n",
    "      \"I've\": \"I have\",\n",
    "      \"isn't\": \"is not\",\n",
    "      \"it'd\": \"it had\",\n",
    "      \"it'd've\": \"it would have\",\n",
    "      \"it'll\": \"it will\",\n",
    "      \"it'll've\": \"it will have\",\n",
    "      \"it's\": \"it is\",\n",
    "      \"let's\": \"let us\",\n",
    "      \"ma'am\": \"madam\",\n",
    "      \"mayn't\": \"may not\",\n",
    "      \"might've\": \"might have\",\n",
    "      \"mightn't\": \"might not\",\n",
    "      \"mightn't've\": \"might not have\",\n",
    "      \"must've\": \"must have\",\n",
    "      \"mustn't\": \"must not\",\n",
    "      \"mustn't've\": \"must not have\",\n",
    "      \"needn't\": \"need not\",\n",
    "      \"needn't've\": \"need not have\",\n",
    "      \"o'clock\": \"of the clock\",\n",
    "      \"oughtn't\": \"ought not\",\n",
    "      \"oughtn't've\": \"ought not have\",\n",
    "      \"shan't\": \"shall not\",\n",
    "      \"sha'n't\": \"shall not\",\n",
    "      \"shan't've\": \"shall not have\",\n",
    "      \"she'd\": \"she would\",\n",
    "      \"she'd've\": \"she would have\",\n",
    "      \"she'll\": \"she will\",\n",
    "      \"she'll've\": \"she will have\",\n",
    "      \"she's\": \"she is\",\n",
    "      \"should've\": \"should have\",\n",
    "      \"shouldn't\": \"should not\",\n",
    "      \"shouldn't've\": \"should not have\",\n",
    "      \"so've\": \"so have\",\n",
    "      \"so's\": \"so is\",\n",
    "      \"that'd\": \"that would\",\n",
    "      \"that'd've\": \"that would have\",\n",
    "      \"that's\": \"that is\",\n",
    "      \"there'd\": \"there had\",\n",
    "      \"there'd've\": \"there would have\",\n",
    "      \"there's\": \"there is\",\n",
    "      \"they'd\": \"they would\",\n",
    "      \"they'd've\": \"they would have\",\n",
    "      \"they'll\": \"they will\",\n",
    "      \"they'll've\": \"they will have\",\n",
    "      \"they're\": \"they are\",\n",
    "      \"they've\": \"they have\",\n",
    "      \"to've\": \"to have\",\n",
    "      \"wasn't\": \"was not\",\n",
    "      \"we'd\": \"we had\",\n",
    "      \"we'd've\": \"we would have\",\n",
    "      \"we'll\": \"we will\",\n",
    "      \"we'll've\": \"we will have\",\n",
    "      \"we're\": \"we are\",\n",
    "      \"we've\": \"we have\",\n",
    "      \"weren't\": \"were not\",\n",
    "      \"what'll\": \"what will\",\n",
    "      \"what'll've\": \"what will have\",\n",
    "      \"what're\": \"what are\",\n",
    "      \"what's\": \"what is\",\n",
    "      \"what've\": \"what have\",\n",
    "      \"when's\": \"when is\",\n",
    "      \"when've\": \"when have\",\n",
    "      \"where'd\": \"where did\",\n",
    "      \"where's\": \"where is\",\n",
    "      \"where've\": \"where have\",\n",
    "      \"who'll\": \"who will\",\n",
    "      \"who'll've\": \"who will have\",\n",
    "      \"who's\": \"who is\",\n",
    "      \"who've\": \"who have\",\n",
    "      \"why's\": \"why is\",\n",
    "      \"why've\": \"why have\",\n",
    "      \"will've\": \"will have\",\n",
    "      \"won't\": \"will not\",\n",
    "      \"won't've\": \"will not have\",\n",
    "      \"would've\": \"would have\",\n",
    "      \"wouldn't\": \"would not\",\n",
    "      \"wouldn't've\": \"would not have\",\n",
    "      \"y'all\": \"you all\",\n",
    "      \"y'alls\": \"you alls\",\n",
    "      \"y'all'd\": \"you all would\",\n",
    "      \"y'all'd've\": \"you all would have\",\n",
    "      \"y'all're\": \"you all are\",\n",
    "      \"y'all've\": \"you all have\",\n",
    "      \"you'd\": \"you had\",\n",
    "      \"you'd've\": \"you would have\",\n",
    "      \"you'll\": \"you will\",\n",
    "      \"you'll've\": \"you will have\",\n",
    "      \"you're\": \"you are\",\n",
    "      \"you've\": \"you have\"}\n",
    "\n",
    "    contractions = dict((k.lower(), v.lower()) for k,v in contractions.items())\n",
    "\n",
    "    c_re = re.compile('(%s)' % '|'.join(contractions.keys()))\n",
    "\n",
    "    def replace(match):\n",
    "        return contractions[match.group(0)]\n",
    "    return c_re.sub(replace, text)\n",
    "\n",
    "def remove_numbers(text):\n",
    "    \"\"\" Remove numbers \"\"\"\n",
    "    words_only = ''.join([w for w in text if not w.isdigit()])\n",
    "    return words_only\n",
    "\n",
    "def to_lower(text):\n",
    "    \"\"\" Lower case all the letters of the string \"\"\"\n",
    "    return text.lower()\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    for punctuation in string.punctuation:\n",
    "        text = text.replace(punctuation, ' ')\n",
    "    return text\n",
    "\n",
    "def remove_stop_words(text):\n",
    "    \"\"\" Remove Stop words from text \"\"\"\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    stopwords.append('https')\n",
    "    stopwords.append('com')\n",
    "    stopwords.append('http')\n",
    "    stopwords.append('twitter')\n",
    "    stopwords.append('m')\n",
    "    stopwords.append('www')\n",
    "    stopwords = stopwords+['uh' ,'um',  'mm', 'mhm' , 'mmm', 'er' , 'oh',\"0o\", \"0s\", \"3a\", \"3b\", \"3d\", \"6b\", \"6o\", \"a\", \"A\", \"a1\", \"a2\", \"a3\", \"a4\", \"ab\", \"able\", \"about\", \"above\", \"abst\", \"ac\", \"accordance\", \"according\", \"accordingly\", \"across\", \"act\", \"actually\", \"ad\", \"added\", \"adj\", \"ae\", \"af\", \"affected\", \"affecting\", \"after\", \"afterwards\", \"ag\", \"again\", \"against\", \"ah\", \"ain\", \"aj\", \"al\", \"all\", \"allow\", \"allows\", \"almost\", \"alone\", \"along\", \"already\", \"also\", \"although\", \"always\", \"am\", \"among\", \"amongst\", \"amoungst\", \"amount\", \"an\", \"and\", \"announce\", \"another\", \"any\", \"anybody\", \"anyhow\", \"anymore\", \"anyone\", \"anyway\", \"anyways\", \"anywhere\", \"ao\", \"ap\", \"apart\", \"apparently\", \"appreciate\", \"approximately\", \"ar\", \"are\", \"aren\", \"arent\", \"arise\", \"around\", \"as\", \"aside\", \"ask\", \"asking\", \"at\", \"au\", \"auth\", \"av\", \"available\", \"aw\", \"away\", \"awfully\", \"ax\", \"ay\", \"az\", \"b\", \"B\", \"b1\", \"b2\", \"b3\", \"ba\", \"back\", \"bc\", \"bd\", \"be\", \"became\", \"been\", \"before\", \"beforehand\", \"beginnings\", \"behind\", \"below\", \"beside\", \"besides\", \"best\", \"between\", \"beyond\", \"bi\", \"bill\", \"biol\", \"bj\", \"bk\", \"bl\", \"bn\", \"both\", \"bottom\", \"bp\", \"br\", \"brief\", \"briefly\", \"bs\", \"bt\", \"bu\", \"but\", \"bx\", \"by\", \"c\", \"C\", \"c1\", \"c2\", \"c3\", \"ca\", \"call\", \"came\", \"can\", \"cannot\", \"cant\", \"cc\", \"cd\", \"ce\", \"certain\", \"certainly\", \"cf\", \"cg\", \"ch\", \"ci\", \"cit\", \"cj\", \"cl\", \"clearly\", \"cm\", \"cn\", \"co\", \"com\", \"come\", \"comes\", \"con\", \"concerning\", \"consequently\", \"consider\", \"considering\", \"could\", \"couldn\", \"couldnt\", \"course\", \"cp\", \"cq\", \"cr\", \"cry\", \"cs\", \"ct\", \"cu\", \"cv\", \"cx\", \"cy\", \"cz\", \"d\", \"D\", \"d2\", \"da\", \"date\", \"dc\", \"dd\", \"de\", \"definitely\", \"describe\", \"described\", \"despite\", \"detail\", \"df\", \"di\", \"did\", \"didn\", \"dj\", \"dk\", \"dl\", \"do\", \"does\", \"doesn\", \"doing\", \"don\", \"done\", \"down\", \"downwards\", \"dp\", \"dr\", \"ds\", \"dt\", \"du\", \"due\", \"during\", \"dx\", \"dy\", \"e\", \"E\", \"e2\", \"e3\", \"ea\", \"each\", \"ec\", \"ed\", \"edu\", \"ee\", \"ef\", \"eg\", \"ei\", \"eight\", \"eighty\", \"either\", \"ej\", \"el\", \"eleven\", \"else\", \"elsewhere\", \"em\", \"en\", \"end\", \"ending\", \"enough\", \"entirely\", \"eo\", \"ep\", \"eq\", \"er\", \"es\", \"especially\", \"est\", \"et\", \"et-al\", \"etc\", \"eu\", \"ev\", \"even\", \"ever\", \"every\", \"everybody\", \"everyone\", \"everything\", \"everywhere\", \"ex\", \"exactly\", \"example\", \"except\", \"ey\", \"f\", \"F\", \"f2\", \"fa\", \"far\", \"fc\", \"few\", \"ff\", \"fi\", \"fifteen\", \"fifth\", \"fify\", \"fill\", \"find\", \"fire\", \"five\", \"fix\", \"fj\", \"fl\", \"fn\", \"fo\", \"followed\", \"following\", \"follows\", \"for\", \"former\", \"formerly\", \"forth\", \"forty\", \"found\", \"four\", \"fr\", \"from\", \"front\", \"fs\", \"ft\", \"fu\", \"full\", \"further\", \"furthermore\", \"fy\", \"g\", \"G\", \"ga\", \"gave\", \"ge\", \"get\", \"gets\", \"getting\", \"gi\", \"give\", \"given\", \"gives\", \"giving\", \"gj\", \"gl\", \"go\", \"goes\", \"going\", \"gone\", \"got\", \"gotten\", \"gr\", \"greetings\", \"gs\", \"gy\", \"h\", \"H\", \"h2\", \"h3\", \"had\", \"hadn\", \"happens\", \"hardly\", \"has\", \"hasn\", \"hasnt\", \"have\", \"haven\", \"having\", \"he\", \"hed\", \"hello\", \"help\", \"hence\", \"here\", \"hereafter\", \"hereby\", \"herein\", \"heres\", \"hereupon\", \"hes\", \"hh\", \"hi\", \"hid\", \"hither\", \"hj\", \"ho\", \"hopefully\", \"how\", \"howbeit\", \"however\", \"hr\", \"hs\", \"http\", \"hu\", \"hundred\", \"hy\", \"i2\", \"i3\", \"i4\", \"i6\", \"i7\", \"i8\", \"ia\", \"ib\", \"ibid\", \"ic\", \"id\", \"ie\", \"if\", \"ig\", \"ignored\", \"ih\", \"ii\", \"ij\", \"il\", \"im\", \"immediately\", \"in\", \"inasmuch\", \"inc\", \"indeed\", \"index\", \"indicate\", \"indicated\", \"indicates\", \"information\", \"inner\", \"insofar\", \"instead\", \"interest\", \"into\", \"inward\", \"io\", \"ip\", \"iq\", \"ir\", \"is\", \"isn\", \"it\", \"itd\", \"its\", \"iv\", \"ix\", \"iy\", \"iz\", \"j\", \"J\", \"jj\", \"jr\", \"js\", \"jt\", \"ju\", \"just\", \"k\", \"K\", \"ke\", \"keep\", \"keeps\", \"kept\", \"kg\", \"kj\", \"km\", \"ko\", \"l\", \"L\", \"l2\", \"la\", \"largely\", \"last\", \"lately\", \"later\", \"latter\", \"latterly\", \"lb\", \"lc\", \"le\", \"least\", \"les\", \"less\", \"lest\", \"let\", \"lets\", \"lf\", \"like\", \"liked\", \"likely\", \"line\", \"little\", \"lj\", \"ll\", \"ln\", \"lo\", \"look\", \"looking\", \"looks\", \"los\", \"lr\", \"ls\", \"lt\", \"ltd\", \"m\", \"M\", \"m2\", \"ma\", \"made\", \"mainly\", \"make\", \"makes\", \"many\", \"may\", \"maybe\", \"me\", \"meantime\", \"meanwhile\", \"merely\", \"mg\", \"might\", \"mightn\", \"mill\", \"million\", \"mine\", \"miss\", \"ml\", \"mn\", \"mo\", \"more\", \"moreover\", \"most\", \"mostly\", \"move\", \"mr\", \"mrs\", \"ms\", \"mt\", \"mu\", \"much\", \"mug\", \"must\", \"mustn\", \"my\", \"n\", \"N\", \"n2\", \"na\", \"name\", \"namely\", \"nay\", \"nc\", \"nd\", \"ne\", \"near\", \"nearly\", \"necessarily\", \"neither\", \"nevertheless\", \"new\", \"next\", \"ng\", \"ni\", \"nine\", \"ninety\", \"nj\", \"nl\", \"nn\", \"no\", \"nobody\", \"non\", \"none\", \"nonetheless\", \"noone\", \"nor\", \"normally\", \"nos\", \"not\", \"noted\", \"novel\", \"now\", \"nowhere\", \"nr\", \"ns\", \"nt\", \"ny\", \"o\", \"O\", \"oa\", \"ob\", \"obtain\", \"obtained\", \"obviously\", \"oc\", \"od\", \"of\", \"off\", \"often\", \"og\", \"oh\", \"oi\", \"oj\", \"ok\", \"okay\", \"ol\", \"old\", \"om\", \"omitted\", \"on\", \"once\", \"one\", \"ones\", \"only\", \"onto\", \"oo\", \"op\", \"oq\", \"or\", \"ord\", \"os\", \"ot\", \"otherwise\", \"ou\", \"ought\", \"our\", \"out\", \"outside\", \"over\", \"overall\", \"ow\", \"owing\", \"own\", \"ox\", \"oz\", \"p\", \"P\", \"p1\", \"p2\", \"p3\", \"page\", \"pagecount\", \"pages\", \"par\", \"part\", \"particular\", \"particularly\", \"pas\", \"past\", \"pc\", \"pd\", \"pe\", \"per\", \"perhaps\", \"pf\", \"ph\", \"pi\", \"pj\", \"pk\", \"pl\", \"placed\", \"please\", \"plus\", \"pm\", \"pn\", \"po\", \"poorly\", \"pp\", \"pq\", \"pr\", \"predominantly\", \"presumably\", \"previously\", \"primarily\", \"probably\", \"promptly\", \"proud\", \"provides\", \"ps\", \"pt\", \"pu\", \"put\", \"py\", \"q\", \"Q\", \"qj\", \"qu\", \"que\", \"quickly\", \"quite\", \"qv\", \"r\", \"R\", \"r2\", \"ra\", \"ran\", \"rather\", \"rc\", \"rd\", \"re\", \"readily\", \"really\", \"reasonably\", \"recent\", \"recently\", \"ref\", \"refs\", \"regarding\", \"regardless\", \"regards\", \"related\", \"relatively\", \"research-articl\", \"respectively\", \"resulted\", \"resulting\", \"results\", \"rf\", \"rh\", \"ri\", \"right\", \"rj\", \"rl\", \"rm\", \"rn\", \"ro\", \"rq\", \"rr\", \"rs\", \"rt\", \"ru\", \"run\", \"rv\", \"ry\", \"s\", \"S\", \"s2\", \"sa\", \"said\", \"saw\", \"say\", \"saying\", \"says\", \"sc\", \"sd\", \"se\", \"sec\", \"second\", \"secondly\", \"section\", \"seem\", \"seemed\", \"seeming\", \"seems\", \"seen\", \"sent\", \"seven\", \"several\", \"sf\", \"shall\", \"shan\", \"shed\", \"shes\", \"show\", \"showed\", \"shown\", \"showns\", \"shows\", \"si\", \"side\", \"since\", \"sincere\", \"six\", \"sixty\", \"sj\", \"sl\", \"slightly\", \"sm\", \"sn\", \"so\", \"some\", \"somehow\", \"somethan\", \"sometime\", \"sometimes\", \"somewhat\", \"somewhere\", \"soon\", \"sorry\", \"sp\", \"specifically\", \"specified\", \"specify\", \"specifying\", \"sq\", \"sr\", \"ss\", \"st\", \"still\", \"stop\", \"strongly\", \"sub\", \"substantially\", \"successfully\", \"such\", \"sufficiently\", \"suggest\", \"sup\", \"sure\", \"sy\", \"sz\", \"t\", \"T\", \"t1\", \"t2\", \"t3\", \"take\", \"taken\", \"taking\", \"tb\", \"tc\", \"td\", \"te\", \"tell\", \"ten\", \"tends\", \"tf\", \"th\", \"than\", \"thank\", \"thanks\", \"thanx\", \"that\", \"thats\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\", \"thered\", \"therefore\", \"therein\", \"thereof\", \"therere\", \"theres\", \"thereto\", \"thereupon\", \"these\", \"they\", \"theyd\", \"theyre\", \"thickv\", \"thin\", \"think\", \"third\", \"this\", \"thorough\", \"thoroughly\", \"those\", \"thou\", \"though\", \"thoughh\", \"thousand\", \"three\", \"throug\", \"through\", \"throughout\", \"thru\", \"thus\", \"ti\", \"til\", \"tip\", \"tj\", \"tl\", \"tm\", \"tn\", \"to\", \"together\", \"too\", \"took\", \"top\", \"toward\", \"towards\", \"tp\", \"tq\", \"tr\", \"tried\", \"tries\", \"truly\", \"try\", \"trying\", \"ts\", \"tt\", \"tv\", \"twelve\", \"twenty\", \"twice\", \"two\", \"tx\", \"u\", \"U\", \"u201d\", \"ue\", \"ui\", \"uj\", \"uk\", \"um\", \"un\", \"under\", \"unfortunately\", \"unless\", \"unlike\", \"unlikely\", \"until\", \"unto\", \"uo\", \"up\", \"upon\", \"ups\", \"ur\", \"us\", \"used\", \"useful\", \"usefully\", \"usefulness\", \"using\", \"usually\", \"ut\", \"v\", \"V\", \"va\", \"various\", \"vd\", \"ve\", \"very\", \"via\", \"viz\", \"vj\", \"vo\", \"vol\", \"vols\", \"volumtype\", \"vq\", \"vs\", \"vt\", \"vu\", \"w\", \"W\", \"wa\", \"was\", \"wasn\", \"wasnt\", \"way\", \"we\", \"wed\", \"welcome\", \"well\", \"well-b\", \"went\", \"were\", \"weren\", \"werent\", \"what\", \"whatever\", \"whats\", \"when\", \"whence\", \"whenever\", \"where\", \"whereafter\", \"whereas\", \"whereby\", \"wherein\", \"wheres\", \"whereupon\", \"wherever\", \"whether\", \"which\", \"while\", \"whim\", \"whither\", \"who\", \"whod\", \"whoever\", \"whole\", \"whom\", \"whomever\", \"whos\", \"whose\", \"why\", \"wi\", \"widely\", \"with\", \"within\", \"without\", \"wo\", \"won\", \"wonder\", \"wont\", \"would\", \"wouldn\", \"wouldnt\", \"www\", \"x\", \"X\", \"x1\", \"x2\", \"x3\", \"xf\", \"xi\", \"xj\", \"xk\", \"xl\", \"xn\", \"xo\", \"xs\", \"xt\", \"xv\", \"xx\", \"y\", \"Y\", \"y2\", \"yes\", \"yet\", \"yj\", \"yl\", \"you\", \"youd\", \"your\", \"youre\", \"yours\", \"yr\", \"ys\", \"yt\", \"z\", \"Z\", \"zero\", \"zi\", \"zz\"]\n",
    "\n",
    "    stop_words = set(stopwords)\n",
    "\n",
    "    word_tokens = nltk.word_tokenize(text)\n",
    "\n",
    "    filtered_text = [w for w in word_tokens if not w in stop_words]\n",
    "\n",
    "    text = ' '.join(filtered_text)\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_context_symbol(text):\n",
    "    return re.sub('<[^>]+>', '', text)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95820061",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "bec244d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(texts_sequence):\n",
    "    \"\"\" Return a preprocessed sequence of texts \"\"\"\n",
    "    return texts_sequence.apply(\n",
    "        to_lower).apply(\n",
    "        expand_contractions).apply(\n",
    "        remove_punctuation).apply(\n",
    "        remove_numbers).apply(remove_stop_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "d86b570c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0\n",
       "302.0    fine angeles california family friends mixture...\n",
       "307.0    moscow family moved moved eventually college l...\n",
       "331.0    connecticut actor moved san francisco moved at...\n",
       "335.0    originally angeles weather family moved lot cu...\n",
       "346.0    angeles lot things love beach love love sunny ...\n",
       "                               ...                        \n",
       "485.0    bad tired saint louis missouri yep months ago ...\n",
       "486.0    feel great saint louis missouri born raised co...\n",
       "487.0    fine detroit michigan moved family years ago y...\n",
       "488.0    fine san fernando valley culture love museums ...\n",
       "491.0    huh overwhelmed funeral attend tomorrow doctor...\n",
       "Name: text, Length: 142, dtype: object"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X= X.apply(remove_context_symbol)\n",
    "X = clean_text(X)\n",
    "X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "a195ed40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0\n",
       "302.0    i'm fine how about yourself  . i'm from los an...\n",
       "307.0    <laughter> . um moscow . um my family moved to...\n",
       "331.0    yes  . okay  . connecticut . um  . to be an ac...\n",
       "335.0    yes . i'm okay  . uh i'm from here originally ...\n",
       "346.0    yes  . i'm okay  . here in los angeles  . ther...\n",
       "                               ...                        \n",
       "485.0    <synch> . yes . i'm not bad i'm a little tired...\n",
       "486.0    <synch> . yes . i'm feel great . i am from sai...\n",
       "487.0    <synch> . yes . i'm fine thank you . detroit m...\n",
       "488.0    <synch> . yes . fine . oh san fernando valley ...\n",
       "491.0    <synch> . yes . huh overwhelmed . i have a fun...\n",
       "Name: text, Length: 142, dtype: object"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(data.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "a52fafe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['clean_text'] = X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "a62199c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('../../../transcriptions/targets/transcriptions_targets_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd16183",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "81ecfccc",
   "metadata": {},
   "source": [
    "# naive bayes model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3649cb0",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "0fa522e5",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0\n",
       "302.0    fine angeles california family friends mixture...\n",
       "307.0    laughter moscow family moved moved eventually ...\n",
       "331.0    connecticut actor laughter moved san francisco...\n",
       "335.0    originally angeles weather family moved lot cu...\n",
       "346.0    angeles lot things love beach love love sunny ...\n",
       "                               ...                        \n",
       "485.0    synch bad tired saint louis missouri yep month...\n",
       "486.0    synch feel great saint louis missouri born rai...\n",
       "487.0    synch fine detroit michigan moved family years...\n",
       "488.0    synch fine san fernando valley culture love mu...\n",
       "491.0    synch huh overwhelmed funeral attend tomorrow ...\n",
       "Name: text, Length: 142, dtype: object"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfid = TfidfVectorizer()\n",
    "nb = MultinomialNB()\n",
    "X = X.apply(str)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "760852be",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    ('TfidfVectorizer', tfid),\n",
    "    ('MultinomialNB()' , nb)\n",
    "    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "8ea64c7e",
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('TfidfVectorizer',\n",
       "   TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=1, ngram_range=(4, 5), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True, stop_words=None, strip_accents=None,\n",
       "                   sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=None, use_idf=True, vocabulary=None)),\n",
       "  ('MultinomialNB()',\n",
       "   MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       " 'verbose': False,\n",
       " 'TfidfVectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=1, ngram_range=(4, 5), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True, stop_words=None, strip_accents=None,\n",
       "                 sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=None, use_idf=True, vocabulary=None),\n",
       " 'MultinomialNB()': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       " 'TfidfVectorizer__analyzer': 'word',\n",
       " 'TfidfVectorizer__binary': False,\n",
       " 'TfidfVectorizer__decode_error': 'strict',\n",
       " 'TfidfVectorizer__dtype': numpy.float64,\n",
       " 'TfidfVectorizer__encoding': 'utf-8',\n",
       " 'TfidfVectorizer__input': 'content',\n",
       " 'TfidfVectorizer__lowercase': True,\n",
       " 'TfidfVectorizer__max_df': 1.0,\n",
       " 'TfidfVectorizer__max_features': None,\n",
       " 'TfidfVectorizer__min_df': 1,\n",
       " 'TfidfVectorizer__ngram_range': (4, 5),\n",
       " 'TfidfVectorizer__norm': 'l2',\n",
       " 'TfidfVectorizer__preprocessor': None,\n",
       " 'TfidfVectorizer__smooth_idf': True,\n",
       " 'TfidfVectorizer__stop_words': None,\n",
       " 'TfidfVectorizer__strip_accents': None,\n",
       " 'TfidfVectorizer__sublinear_tf': False,\n",
       " 'TfidfVectorizer__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       " 'TfidfVectorizer__tokenizer': None,\n",
       " 'TfidfVectorizer__use_idf': True,\n",
       " 'TfidfVectorizer__vocabulary': None,\n",
       " 'MultinomialNB()__alpha': 1.0,\n",
       " 'MultinomialNB()__class_prior': None,\n",
       " 'MultinomialNB()__fit_prior': True}"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "97bb330d",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pipe_grid = {\n",
    "    'TfidfVectorizer__ngram_range': [(1,2),(1,1) ,(2,2), (2,3), (3,4),(4, 5)],\n",
    "    'MultinomialNB()__alpha': [0.1 , 0.5 , 1.0]\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "84ce48a7",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "search_recall= GridSearchCV(pipe,\n",
    "    pipe_grid,\n",
    "    scoring='recall',\n",
    "    n_jobs=-1,\n",
    "    \n",
    "   \n",
    "    cv=5,\n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "fc927baf",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    2.7s\n",
      "[Parallel(n_jobs=-1)]: Done  90 out of  90 | elapsed:    5.8s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score=nan,\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('TfidfVectorizer',\n",
       "                                        TfidfVectorizer(analyzer='word',\n",
       "                                                        binary=False,\n",
       "                                                        decode_error='strict',\n",
       "                                                        dtype=<class 'numpy.float64'>,\n",
       "                                                        encoding='utf-8',\n",
       "                                                        input='content',\n",
       "                                                        lowercase=True,\n",
       "                                                        max_df=1.0,\n",
       "                                                        max_features=None,\n",
       "                                                        min_df=1,\n",
       "                                                        ngram_range=(1, 1),\n",
       "                                                        norm='l2',\n",
       "                                                        preprocessor=None,\n",
       "                                                        smooth_idf=True,\n",
       "                                                        stop_words=None...\n",
       "                                                        use_idf=True,\n",
       "                                                        vocabulary=None)),\n",
       "                                       ('MultinomialNB()',\n",
       "                                        MultinomialNB(alpha=1.0,\n",
       "                                                      class_prior=None,\n",
       "                                                      fit_prior=True))],\n",
       "                                verbose=False),\n",
       "             iid='deprecated', n_jobs=-1,\n",
       "             param_grid={'MultinomialNB()__alpha': [0.1, 0.5, 1.0],\n",
       "                         'TfidfVectorizer__ngram_range': [(1, 2), (1, 1),\n",
       "                                                          (2, 2), (2, 3),\n",
       "                                                          (3, 4), (4, 5)]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='recall', verbose=1)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_recall.fit(X , y )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "e3edd241",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the best params for recall:  {'MultinomialNB()__alpha': 0.1, 'TfidfVectorizer__ngram_range': (1, 2)}\n",
      "the best recall score:  0.0\n"
     ]
    }
   ],
   "source": [
    "print('the best params for recall: ',search_recall.best_params_)\n",
    "print('the best recall score: ' , search_recall.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "0ebcecdf",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "search_accuracy= GridSearchCV(pipe,\n",
    "    pipe_grid,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    \n",
    "   \n",
    "    cv=5,\n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "a7067f15",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  64 tasks      | elapsed:    3.5s\n",
      "[Parallel(n_jobs=-1)]: Done  79 out of  90 | elapsed:    4.5s remaining:    0.6s\n",
      "[Parallel(n_jobs=-1)]: Done  90 out of  90 | elapsed:    5.1s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score=nan,\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('TfidfVectorizer',\n",
       "                                        TfidfVectorizer(analyzer='word',\n",
       "                                                        binary=False,\n",
       "                                                        decode_error='strict',\n",
       "                                                        dtype=<class 'numpy.float64'>,\n",
       "                                                        encoding='utf-8',\n",
       "                                                        input='content',\n",
       "                                                        lowercase=True,\n",
       "                                                        max_df=1.0,\n",
       "                                                        max_features=None,\n",
       "                                                        min_df=1,\n",
       "                                                        ngram_range=(1, 1),\n",
       "                                                        norm='l2',\n",
       "                                                        preprocessor=None,\n",
       "                                                        smooth_idf=True,\n",
       "                                                        stop_words=None...\n",
       "                                                        use_idf=True,\n",
       "                                                        vocabulary=None)),\n",
       "                                       ('MultinomialNB()',\n",
       "                                        MultinomialNB(alpha=1.0,\n",
       "                                                      class_prior=None,\n",
       "                                                      fit_prior=True))],\n",
       "                                verbose=False),\n",
       "             iid='deprecated', n_jobs=-1,\n",
       "             param_grid={'MultinomialNB()__alpha': [0.1, 0.5, 1.0],\n",
       "                         'TfidfVectorizer__ngram_range': [(1, 2), (1, 1),\n",
       "                                                          (2, 2), (2, 3),\n",
       "                                                          (3, 4), (4, 5)]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='accuracy', verbose=1)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_accuracy.fit(X , y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "d33c2450",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the best params for accuracy:  {'MultinomialNB()__alpha': 0.1, 'TfidfVectorizer__ngram_range': (1, 2)}\n",
      "the best accuracy score:  0.704433497536946\n"
     ]
    }
   ],
   "source": [
    "print('the best params for accuracy: ',search_accuracy.best_params_)\n",
    "print('the best accuracy score: ' , search_accuracy.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "ded16dc5",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "search_precision= GridSearchCV(pipe,\n",
    "    pipe_grid,\n",
    "    scoring='precision',\n",
    "    n_jobs=-1,\n",
    "    \n",
    "   \n",
    "    cv=3,\n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "1375a6a9",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 18 candidates, totalling 54 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  54 out of  54 | elapsed:    2.8s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score=nan,\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('TfidfVectorizer',\n",
       "                                        TfidfVectorizer(analyzer='word',\n",
       "                                                        binary=False,\n",
       "                                                        decode_error='strict',\n",
       "                                                        dtype=<class 'numpy.float64'>,\n",
       "                                                        encoding='utf-8',\n",
       "                                                        input='content',\n",
       "                                                        lowercase=True,\n",
       "                                                        max_df=1.0,\n",
       "                                                        max_features=None,\n",
       "                                                        min_df=1,\n",
       "                                                        ngram_range=(1, 1),\n",
       "                                                        norm='l2',\n",
       "                                                        preprocessor=None,\n",
       "                                                        smooth_idf=True,\n",
       "                                                        stop_words=None...\n",
       "                                                        use_idf=True,\n",
       "                                                        vocabulary=None)),\n",
       "                                       ('MultinomialNB()',\n",
       "                                        MultinomialNB(alpha=1.0,\n",
       "                                                      class_prior=None,\n",
       "                                                      fit_prior=True))],\n",
       "                                verbose=False),\n",
       "             iid='deprecated', n_jobs=-1,\n",
       "             param_grid={'MultinomialNB()__alpha': [0.1, 0.5, 1.0],\n",
       "                         'TfidfVectorizer__ngram_range': [(1, 2), (1, 1),\n",
       "                                                          (2, 2), (2, 3),\n",
       "                                                          (3, 4), (4, 5)]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='precision', verbose=1)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_precision.fit(X , y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "daf84eaa",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the best params for precision:  {'MultinomialNB()__alpha': 0.1, 'TfidfVectorizer__ngram_range': (1, 2)}\n",
      "the best precision score:  0.0\n"
     ]
    }
   ],
   "source": [
    "print('the best params for precision: ',search_precision.best_params_)\n",
    "print('the best precision score: ' , search_precision.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "9e22be59",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "search_f1= GridSearchCV(pipe,\n",
    "    pipe_grid,\n",
    "    scoring='f1',\n",
    "    n_jobs=-1,\n",
    "    \n",
    "   \n",
    "    cv=3,\n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "3c06d5dc",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 18 candidates, totalling 54 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  54 out of  54 | elapsed:    2.7s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score=nan,\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('TfidfVectorizer',\n",
       "                                        TfidfVectorizer(analyzer='word',\n",
       "                                                        binary=False,\n",
       "                                                        decode_error='strict',\n",
       "                                                        dtype=<class 'numpy.float64'>,\n",
       "                                                        encoding='utf-8',\n",
       "                                                        input='content',\n",
       "                                                        lowercase=True,\n",
       "                                                        max_df=1.0,\n",
       "                                                        max_features=None,\n",
       "                                                        min_df=1,\n",
       "                                                        ngram_range=(1, 1),\n",
       "                                                        norm='l2',\n",
       "                                                        preprocessor=None,\n",
       "                                                        smooth_idf=True,\n",
       "                                                        stop_words=None...\n",
       "                                                        use_idf=True,\n",
       "                                                        vocabulary=None)),\n",
       "                                       ('MultinomialNB()',\n",
       "                                        MultinomialNB(alpha=1.0,\n",
       "                                                      class_prior=None,\n",
       "                                                      fit_prior=True))],\n",
       "                                verbose=False),\n",
       "             iid='deprecated', n_jobs=-1,\n",
       "             param_grid={'MultinomialNB()__alpha': [0.1, 0.5, 1.0],\n",
       "                         'TfidfVectorizer__ngram_range': [(1, 2), (1, 1),\n",
       "                                                          (2, 2), (2, 3),\n",
       "                                                          (3, 4), (4, 5)]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='f1', verbose=1)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_f1.fit(X , y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "87172314",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the best params for f1:  {'MultinomialNB()__alpha': 0.1, 'TfidfVectorizer__ngram_range': (1, 2)}\n",
      "the best f1 score:  0.0\n"
     ]
    }
   ],
   "source": [
    "print('the best params for f1: ',search_f1.best_params_)\n",
    "print('the best f1 score: ' , search_f1.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ca009c",
   "metadata": {
    "hidden": true
   },
   "source": [
    "para precision cambio el bestparams. hago un cross_validate para precision usando el bestparams de los otros scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "e4dbed19",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    0.2s finished\n"
     ]
    }
   ],
   "source": [
    "best_accuracy = search_accuracy.best_estimator_\n",
    "\n",
    "precision_cv = cross_val_score(best_accuracy,\n",
    "    X,\n",
    "    y,\n",
    "    scoring='precision',\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "2f9836d8",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision for params  {'MultinomialNB()__alpha': 0.1, 'TfidfVectorizer__ngram_range': (1, 2)}  is:  0.0\n"
     ]
    }
   ],
   "source": [
    "print('precision for params ',search_accuracy.best_params_ ,' is: ',  np.mean(precision_cv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e956dec9",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "325125af",
   "metadata": {},
   "source": [
    "# testeo y analisis del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915f9bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''{'MultinomialNB()__alpha': 1.0, 'TfidfVectorizer__ngram_range': (1, 2)}'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "02a0ca3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfid3 = TfidfVectorizer(ngram_range=(1,2))\n",
    "nb3 = MultinomialNB(alpha = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "619fab53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                min_df=1, ngram_range=(1, 2), norm='l2', preprocessor=None,\n",
       "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
       "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, use_idf=True, vocabulary=None)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfid3.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "6c3e6e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector3 = tfid3.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "da103aeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abandoned',\n",
       " 'abandoned buildings',\n",
       " 'abandoned pretty',\n",
       " 'abandoning',\n",
       " 'abandoning family',\n",
       " 'abbey',\n",
       " 'abbey premiere',\n",
       " 'abilities',\n",
       " 'abilities hard',\n",
       " 'abilities music',\n",
       " 'abilities people',\n",
       " 'ability',\n",
       " 'ability deal',\n",
       " 'ability decision',\n",
       " 'ability knack',\n",
       " 'ability multitask',\n",
       " 'ability situations',\n",
       " 'ability survive',\n",
       " 'ability work',\n",
       " 'abnormal',\n",
       " 'abnormal lotta',\n",
       " 'abo',\n",
       " 'abo hours',\n",
       " 'abo laughter',\n",
       " 'abo something',\n",
       " 'abort',\n",
       " 'abort choosing',\n",
       " 'abort chose',\n",
       " 'abortion',\n",
       " 'abortion point',\n",
       " 'abroad',\n",
       " 'abroad brazil',\n",
       " 'abroad madrid',\n",
       " 'abroad program',\n",
       " 'absent',\n",
       " 'absent minded',\n",
       " 'absolute',\n",
       " 'absolute hardest',\n",
       " 'absolutely',\n",
       " 'absolutely amazing',\n",
       " 'absolutely awful',\n",
       " 'absolutely breathtakingly',\n",
       " 'absolutely fantastic',\n",
       " 'absolutely fell',\n",
       " 'absolutely know',\n",
       " 'absolutely love',\n",
       " 'absolutely real',\n",
       " 'absolutely yeah',\n",
       " 'absorb',\n",
       " 'absorb something',\n",
       " 'absorbed',\n",
       " 'absorbed small',\n",
       " 'absorption',\n",
       " 'absorption seemingly',\n",
       " 'absurd',\n",
       " 'absurd mistake',\n",
       " 'absurd thing',\n",
       " 'abundance',\n",
       " 'abundance life',\n",
       " 'abuse',\n",
       " 'abuse children',\n",
       " 'abuse know',\n",
       " 'abuse sort',\n",
       " 'abused',\n",
       " 'abused know',\n",
       " 'abused mean',\n",
       " 'abused people',\n",
       " 'abusive',\n",
       " 'abusive happy',\n",
       " 'abusive relationship',\n",
       " 'abusive talk',\n",
       " 'academic',\n",
       " 'academic achievements',\n",
       " 'academic discussions',\n",
       " 'academic professional',\n",
       " 'academically',\n",
       " 'academically know',\n",
       " 'academies',\n",
       " 'academies seventeen',\n",
       " 'academy',\n",
       " 'academy play',\n",
       " 'academy prestige',\n",
       " 'academy program',\n",
       " 'acc',\n",
       " 'acc accomplishments',\n",
       " 'acce',\n",
       " 'acce accepting',\n",
       " 'accent',\n",
       " 'accent greatest',\n",
       " 'accept',\n",
       " 'accept annoying',\n",
       " 'accept emotions',\n",
       " 'accept feedback',\n",
       " 'accept hardest',\n",
       " 'accept know',\n",
       " 'accept moving',\n",
       " 'accept nothing',\n",
       " 'accept something',\n",
       " 'accept tame',\n",
       " 'accept yeah',\n",
       " 'acceptance',\n",
       " 'acceptance couple',\n",
       " 'acceptance rate',\n",
       " 'accepted',\n",
       " 'accepted acceptance',\n",
       " 'accepted college',\n",
       " 'accepted dominguez',\n",
       " 'accepted fact',\n",
       " 'accepted lot',\n",
       " 'accepted love',\n",
       " 'accepted pomona',\n",
       " 'accepted things',\n",
       " 'accepting',\n",
       " 'accepting acce',\n",
       " 'accepting accepted',\n",
       " 'accepting awhile',\n",
       " 'accepting people',\n",
       " 'accepting weaknesses',\n",
       " 'accepts',\n",
       " 'accepts problems',\n",
       " 'accepts thinks',\n",
       " 'access',\n",
       " 'access creative',\n",
       " 'accident',\n",
       " 'accident basically',\n",
       " 'accident car',\n",
       " 'accident distant',\n",
       " 'accident distract',\n",
       " 'accident fine',\n",
       " 'accident first',\n",
       " 'accident flip',\n",
       " 'accident hit',\n",
       " 'accident insurance',\n",
       " 'accident medical',\n",
       " 'accident memory',\n",
       " 'accident never',\n",
       " 'accident parking',\n",
       " 'accident pay',\n",
       " 'accident years',\n",
       " 'accidents',\n",
       " 'accidents know',\n",
       " 'acclimate',\n",
       " 'acclimate situation',\n",
       " 'acclimated',\n",
       " 'acclimated lifestyle',\n",
       " 'accommodating',\n",
       " 'accommodating years',\n",
       " 'accomplish',\n",
       " 'accomplish anything',\n",
       " 'accomplish longer',\n",
       " 'accomplish mind',\n",
       " 'accomplish school',\n",
       " 'accomplish started',\n",
       " 'accomplish things',\n",
       " 'accomplished',\n",
       " 'accomplished anything',\n",
       " 'accomplished attaining',\n",
       " 'accomplished extremely',\n",
       " 'accomplished goal',\n",
       " 'accomplished goals',\n",
       " 'accomplished goodbye',\n",
       " 'accomplished know',\n",
       " 'accomplished laughter',\n",
       " 'accomplished lot',\n",
       " 'accomplished road',\n",
       " 'accomplished something',\n",
       " 'accomplished thought',\n",
       " 'accomplished time',\n",
       " 'accomplished willing',\n",
       " 'accomplished years',\n",
       " 'accomplishing',\n",
       " 'accomplishing life',\n",
       " 'accomplishing something',\n",
       " 'accomplishment',\n",
       " 'accomplishment bye',\n",
       " 'accomplishment clears',\n",
       " 'accomplishment guess',\n",
       " 'accomplishment happiness',\n",
       " 'accomplishment life',\n",
       " 'accomplishment puts',\n",
       " 'accomplishment sigh',\n",
       " 'accomplishment something',\n",
       " 'accomplishment yeah',\n",
       " 'accomplishments',\n",
       " 'accomplishments acc',\n",
       " 'accomplishments goal',\n",
       " 'accomplishments grateful',\n",
       " 'accomplishments lot',\n",
       " 'accomplishments matter',\n",
       " 'accomplishments overcoming',\n",
       " 'accomplishments race',\n",
       " 'accomplishments school',\n",
       " 'accomplishments sees',\n",
       " 'accomplishments something',\n",
       " 'account',\n",
       " 'account executive',\n",
       " 'account laughter',\n",
       " 'account money',\n",
       " 'account nice',\n",
       " 'account people',\n",
       " 'account something',\n",
       " 'accountable',\n",
       " 'accountable eh',\n",
       " 'accountable share',\n",
       " 'accounting',\n",
       " 'accounting business',\n",
       " 'accounting know',\n",
       " 'accounting practice',\n",
       " 'accredit',\n",
       " 'accredit spiritual',\n",
       " 'accumulated',\n",
       " 'accumulated short',\n",
       " 'accurate',\n",
       " 'accurate sigh',\n",
       " 'accurately',\n",
       " 'accurately seeing',\n",
       " 'accuse',\n",
       " 'accuse something',\n",
       " 'accuse wrongdoing',\n",
       " 'accused',\n",
       " 'accused argument',\n",
       " 'accuses',\n",
       " 'accuses wrongdoing',\n",
       " 'accustom',\n",
       " 'accustom pretty',\n",
       " 'achieve',\n",
       " 'achieve achieving',\n",
       " 'achieve goals',\n",
       " 'achieve higher',\n",
       " 'achieve something',\n",
       " 'achieve sound',\n",
       " 'achieve things',\n",
       " 'achieve wan',\n",
       " 'achieve want',\n",
       " 'achieve yeah',\n",
       " 'achieved',\n",
       " 'achieved lot',\n",
       " 'achievement',\n",
       " 'achievement know',\n",
       " 'achievement laughter',\n",
       " 'achievements',\n",
       " 'achievements abilities',\n",
       " 'achievements goals',\n",
       " 'achievements guess',\n",
       " 'achievements never',\n",
       " 'achieving',\n",
       " 'achieving daughter',\n",
       " 'achieving sense',\n",
       " 'achieving yeah',\n",
       " 'achy',\n",
       " 'achy difficult',\n",
       " 'acknowledge',\n",
       " 'acknowledge feelings',\n",
       " 'acqu',\n",
       " 'acqu friends',\n",
       " 'acquaintances',\n",
       " 'acquaintances acqu',\n",
       " 'acquaintances friends',\n",
       " 'acquaintances people',\n",
       " 'acquaintances pretty',\n",
       " 'acquired',\n",
       " 'acquired major',\n",
       " 'acquisition',\n",
       " 'acquisition sense',\n",
       " 'acres',\n",
       " 'acres acres',\n",
       " 'acres grapes',\n",
       " 'acronym',\n",
       " 'acronym laughter',\n",
       " 'acted',\n",
       " 'acted child',\n",
       " 'acted eh',\n",
       " 'acting',\n",
       " 'acting bag',\n",
       " 'acting boyfriend',\n",
       " 'acting career',\n",
       " 'acting class',\n",
       " 'acting classes',\n",
       " 'acting gig',\n",
       " 'acting kind',\n",
       " 'acting kinda',\n",
       " 'acting know',\n",
       " 'acting laughter',\n",
       " 'acting point',\n",
       " 'acting pollyanna',\n",
       " 'acting stuff',\n",
       " 'acting theater',\n",
       " 'acting work',\n",
       " 'acting writing',\n",
       " 'action',\n",
       " 'action sniffle',\n",
       " 'action train',\n",
       " 'actions',\n",
       " 'actions behavior',\n",
       " 'actions choosing',\n",
       " 'actions know',\n",
       " 'actions takes',\n",
       " 'actions words',\n",
       " 'active',\n",
       " 'active change',\n",
       " 'active imagination',\n",
       " 'active laughter',\n",
       " 'active movies',\n",
       " 'active music',\n",
       " 'active wish',\n",
       " 'active wonderful',\n",
       " 'active yeah',\n",
       " 'actively',\n",
       " 'actively engage',\n",
       " 'actively interviews',\n",
       " 'actively pursue',\n",
       " 'actively seeking',\n",
       " 'actively work',\n",
       " 'activites',\n",
       " 'activites know',\n",
       " 'activities',\n",
       " 'activities biking',\n",
       " 'activities eat',\n",
       " 'activities expect',\n",
       " 'activities fickle',\n",
       " 'activities kinda',\n",
       " 'activities life',\n",
       " 'activities pardon',\n",
       " 'activities person',\n",
       " 'activities setting',\n",
       " 'activities theme',\n",
       " 'activities things',\n",
       " 'activities time',\n",
       " 'activities yeah',\n",
       " 'activity',\n",
       " 'activity bad',\n",
       " 'activity beach',\n",
       " 'activity big',\n",
       " 'activity know',\n",
       " 'activity lot',\n",
       " 'activity people',\n",
       " 'activity something',\n",
       " 'actor',\n",
       " 'actor business',\n",
       " 'actor hooked',\n",
       " 'actor know',\n",
       " 'actor laughter',\n",
       " 'actor pretty',\n",
       " 'actor realtor',\n",
       " 'actor something',\n",
       " 'actor theater',\n",
       " 'actor totally',\n",
       " 'actors',\n",
       " 'actors great',\n",
       " 'actors stuff',\n",
       " 'actress',\n",
       " 'actress guess',\n",
       " 'actress something',\n",
       " 'acts',\n",
       " 'acts different',\n",
       " 'acts stupid',\n",
       " 'actual',\n",
       " 'actual argument',\n",
       " 'actual combat',\n",
       " 'actual fish',\n",
       " 'actual job',\n",
       " 'actual place',\n",
       " 'actual wedding',\n",
       " 'adapt',\n",
       " 'adapt different',\n",
       " 'adapt experiencing',\n",
       " 'adapt personality',\n",
       " 'adapt wan',\n",
       " 'adaptable',\n",
       " 'adaptable different',\n",
       " 'adapted',\n",
       " 'adapted united',\n",
       " 'add',\n",
       " 'add compound',\n",
       " 'add mass',\n",
       " 'addiction',\n",
       " 'addiction yoga',\n",
       " 'addictions',\n",
       " 'addictions kinda',\n",
       " 'adding',\n",
       " 'adding things',\n",
       " 'additional',\n",
       " 'additional meetings',\n",
       " 'additional stuff',\n",
       " 'address',\n",
       " 'address communicated',\n",
       " 'address issues',\n",
       " 'adequately',\n",
       " 'adequately sports',\n",
       " 'adhere',\n",
       " 'adhere fall',\n",
       " 'adjust',\n",
       " 'adjust different',\n",
       " 'adjust people',\n",
       " 'adjust sensitivity',\n",
       " 'adjusted',\n",
       " 'adjusted aunt',\n",
       " 'adjusted comfortable',\n",
       " 'adjusted different',\n",
       " 'adjusted know',\n",
       " 'adjusted pretty',\n",
       " 'adjusted roommates',\n",
       " 'adjusting',\n",
       " 'adjusting easy',\n",
       " 'adjusting laughter',\n",
       " 'adjustment',\n",
       " 'adjustment advertising',\n",
       " 'adjustment close',\n",
       " 'adjustment hard',\n",
       " 'adjustment laughter',\n",
       " 'adjustment moderately',\n",
       " 'admin',\n",
       " 'admin assistant',\n",
       " 'administration',\n",
       " 'administration business',\n",
       " 'administration care',\n",
       " 'administration justice',\n",
       " 'administration major',\n",
       " 'administration wanted',\n",
       " 'administration wasted',\n",
       " 'administrative',\n",
       " 'administrative assistant',\n",
       " 'admirable',\n",
       " 'admirable people',\n",
       " 'admire',\n",
       " 'admire listen',\n",
       " 'admire people',\n",
       " 'admire struggles',\n",
       " 'admire value',\n",
       " 'admission',\n",
       " 'admission counselors',\n",
       " 'admit',\n",
       " 'admit awhile',\n",
       " 'admit hardest',\n",
       " 'admit lose',\n",
       " 'admitted',\n",
       " 'admitted sucks',\n",
       " 'adolescent',\n",
       " 'adolescent growing',\n",
       " 'adolescent worrying',\n",
       " 'adoption',\n",
       " 'adoption abort',\n",
       " 'adore',\n",
       " 'adore parent',\n",
       " 'adored',\n",
       " 'adored mother',\n",
       " 'adult',\n",
       " 'adult adult',\n",
       " 'adult angry',\n",
       " 'adult cats',\n",
       " 'adult child',\n",
       " 'adult decision',\n",
       " 'adult expected',\n",
       " 'adult family',\n",
       " 'adult genuine',\n",
       " 'adult girlfriend',\n",
       " 'adult gon',\n",
       " 'adult hoping',\n",
       " 'adult life',\n",
       " 'adult molding',\n",
       " 'adult school',\n",
       " 'adult see',\n",
       " 'adult started',\n",
       " 'adult talk',\n",
       " 'adult teacher',\n",
       " 'adult whe',\n",
       " 'adult woman',\n",
       " 'adult yeah',\n",
       " 'adults',\n",
       " 'adults decided',\n",
       " 'adults good',\n",
       " 'adults kinda',\n",
       " 'adults know',\n",
       " 'adults laughter',\n",
       " 'adults missing',\n",
       " 'adults steer',\n",
       " 'advance',\n",
       " 'advance delayed',\n",
       " 'advance sniffle',\n",
       " 'advance want',\n",
       " 'advanced',\n",
       " 'advanced alzheimer',\n",
       " 'advanced thinking',\n",
       " 'advancing',\n",
       " 'advancing know',\n",
       " 'advantage',\n",
       " 'advantage carpe',\n",
       " 'advantage gym',\n",
       " 'advantage meeting',\n",
       " 'advantage opportunities',\n",
       " 'advantage opportunity',\n",
       " 'advantage promote',\n",
       " 'advantage talents',\n",
       " 'adventure',\n",
       " 'adventure activity',\n",
       " 'adventure comedy',\n",
       " 'adventure know',\n",
       " 'adventure see',\n",
       " 'adventure something',\n",
       " 'adventure things',\n",
       " 'adventure yesterday',\n",
       " 'adventuresome',\n",
       " 'adventuresome spirit',\n",
       " 'adventurous',\n",
       " 'adventurous adventurous',\n",
       " 'adventurous guess',\n",
       " 'adventurous loves',\n",
       " 'adventurous person',\n",
       " 'adventurous sigh',\n",
       " 'advertising',\n",
       " 'advertising agency',\n",
       " 'advertising communications',\n",
       " 'advertising deep',\n",
       " 'advertising sales',\n",
       " 'advertising shy',\n",
       " 'advice',\n",
       " 'advice bout',\n",
       " 'advice continue',\n",
       " 'advice different',\n",
       " 'advice engaged',\n",
       " 'advice feel',\n",
       " 'advice giver',\n",
       " 'advice good',\n",
       " 'advice guess',\n",
       " 'advice kinda',\n",
       " 'advice know',\n",
       " 'advice laughter',\n",
       " 'advice listen',\n",
       " 'advice mean',\n",
       " 'advice mind',\n",
       " 'advice objective',\n",
       " 'advice people',\n",
       " 'advice pretty',\n",
       " 'advice recklessly',\n",
       " 'advice relationship',\n",
       " 'advice school',\n",
       " 'advice something',\n",
       " 'advice stay',\n",
       " 'advice thinks',\n",
       " 'advice understand',\n",
       " 'advice waste',\n",
       " 'advice years',\n",
       " 'advice young',\n",
       " 'advices',\n",
       " 'advices recommendations',\n",
       " 'advise',\n",
       " 'advise see',\n",
       " 'advised',\n",
       " 'advised job',\n",
       " 'aerospace',\n",
       " 'aerospace engineering',\n",
       " 'aff',\n",
       " 'aff mean',\n",
       " 'affairs',\n",
       " 'affairs guy',\n",
       " 'affairs small',\n",
       " 'affect',\n",
       " 'affect eh',\n",
       " 'affect kids',\n",
       " 'affect know',\n",
       " 'affect knowing',\n",
       " 'affect personally',\n",
       " 'affect sniff',\n",
       " 'affectionate',\n",
       " 'affectionate aff',\n",
       " 'affectionate mean',\n",
       " 'affective',\n",
       " 'affective disorder',\n",
       " 'affects',\n",
       " 'affects businesses',\n",
       " 'affects concentration',\n",
       " 'affects health',\n",
       " 'affects listening',\n",
       " 'affects sleep',\n",
       " 'affluent',\n",
       " 'affluent angeles',\n",
       " 'afford',\n",
       " 'afford continue',\n",
       " 'afford good',\n",
       " 'afford groceries',\n",
       " 'afford pretty',\n",
       " 'afford travel',\n",
       " 'affordable',\n",
       " 'affordable good',\n",
       " 'affordable something',\n",
       " 'affordable vehicle',\n",
       " 'afforded',\n",
       " 'afforded opportunity',\n",
       " 'affords',\n",
       " 'affords pretty',\n",
       " 'affront',\n",
       " 'affront worked',\n",
       " 'afghanistan',\n",
       " 'afghanistan met',\n",
       " 'afraid',\n",
       " 'afraid afraid',\n",
       " 'afraid authentic',\n",
       " 'afraid avoiding',\n",
       " 'afraid child',\n",
       " 'afraid eh',\n",
       " 'afraid explore',\n",
       " 'afraid fail',\n",
       " 'afraid finish',\n",
       " 'afraid helped',\n",
       " 'afraid losing',\n",
       " 'afraid lot',\n",
       " 'afraid need',\n",
       " 'afraid needed',\n",
       " 'afraid people',\n",
       " 'afraid planes',\n",
       " 'afraid sleep',\n",
       " 'afraid things',\n",
       " 'africa',\n",
       " 'africa enjoy',\n",
       " 'africa great',\n",
       " 'africa visited',\n",
       " 'african',\n",
       " 'african african',\n",
       " 'african americans',\n",
       " 'african swapmeets',\n",
       " 'afro',\n",
       " 'afro brazilian',\n",
       " 'aftermath',\n",
       " 'aftermath laughter',\n",
       " 'afternoon',\n",
       " 'afternoon called',\n",
       " 'afternoon early',\n",
       " 'afternoon kind',\n",
       " 'afterward',\n",
       " 'afterward know',\n",
       " 'age',\n",
       " 'age business',\n",
       " 'age fifty',\n",
       " 'age finishing',\n",
       " 'age grew',\n",
       " 'age group',\n",
       " 'age honest',\n",
       " 'age husband',\n",
       " 'age know',\n",
       " 'age laughter',\n",
       " 'age learn',\n",
       " 'age making',\n",
       " 'age mean',\n",
       " 'age meaning',\n",
       " 'age nineteen',\n",
       " 'age parents',\n",
       " 'age personally',\n",
       " 'age remember',\n",
       " 'age see',\n",
       " 'age seventy',\n",
       " 'age siblings',\n",
       " 'age stayed',\n",
       " 'age strong',\n",
       " 'age xxx',\n",
       " 'age year',\n",
       " 'aged',\n",
       " 'aged know',\n",
       " 'agency',\n",
       " 'agency continue',\n",
       " 'agent',\n",
       " 'agent anything',\n",
       " 'agent license',\n",
       " 'agent never',\n",
       " 'agent things',\n",
       " 'ages',\n",
       " 'ages race',\n",
       " 'ages shapes',\n",
       " 'ages shopping',\n",
       " 'ages talking',\n",
       " 'ages tune',\n",
       " 'aggravated',\n",
       " 'aggravated angry',\n",
       " 'aggression',\n",
       " 'aggression totally',\n",
       " 'aggressions',\n",
       " 'aggressions need',\n",
       " 'aggressive',\n",
       " 'aggressive anger',\n",
       " 'aggressive financial',\n",
       " 'aggressive investing',\n",
       " 'aggressive laughter',\n",
       " 'aggressive positive',\n",
       " 'aggressive work',\n",
       " 'aggressively',\n",
       " 'aggressively laughter',\n",
       " 'agitate',\n",
       " 'agitate feel',\n",
       " 'agitated',\n",
       " 'agitated guess',\n",
       " 'ago',\n",
       " 'ago advice',\n",
       " 'ago afraid',\n",
       " 'ago angry',\n",
       " 'ago anything',\n",
       " 'ago argued',\n",
       " 'ago arguing',\n",
       " 'ago awesome',\n",
       " 'ago awkward',\n",
       " 'ago bad',\n",
       " 'ago beach',\n",
       " 'ago believe',\n",
       " 'ago big',\n",
       " 'ago bit',\n",
       " 'ago board',\n",
       " 'ago bout',\n",
       " 'ago boy',\n",
       " 'ago boyfriend',\n",
       " 'ago broke',\n",
       " 'ago brother',\n",
       " 'ago bye',\n",
       " 'ago called',\n",
       " 'ago carefree',\n",
       " 'ago caught',\n",
       " 'ago caused',\n",
       " 'ago chance',\n",
       " 'ago christmas',\n",
       " 'ago classic',\n",
       " 'ago close',\n",
       " 'ago college',\n",
       " 'ago conflict',\n",
       " 'ago cutting',\n",
       " 'ago daughter',\n",
       " 'ago days',\n",
       " 'ago deborah',\n",
       " 'ago december',\n",
       " 'ago deep',\n",
       " 'ago depressed',\n",
       " 'ago detroit',\n",
       " 'ago diagnosed',\n",
       " 'ago different',\n",
       " 'ago dude',\n",
       " 'ago east',\n",
       " 'ago eh',\n",
       " 'ago eighteen',\n",
       " 'ago ended',\n",
       " 'ago enjoyed',\n",
       " 'ago enthusiastic',\n",
       " 'ago event',\n",
       " 'ago excited',\n",
       " 'ago exhilirating',\n",
       " 'ago explore',\n",
       " 'ago falling',\n",
       " 'ago february',\n",
       " 'ago feel',\n",
       " 'ago feeling',\n",
       " 'ago feels',\n",
       " 'ago felt',\n",
       " 'ago focused',\n",
       " 'ago football',\n",
       " 'ago fought',\n",
       " 'ago friend',\n",
       " 'ago friends',\n",
       " 'ago function',\n",
       " 'ago gift',\n",
       " 'ago girl',\n",
       " 'ago girlfriend',\n",
       " 'ago good',\n",
       " 'ago gosh',\n",
       " 'ago grabbed',\n",
       " 'ago graduated',\n",
       " 'ago grandmother',\n",
       " 'ago guess',\n",
       " 'ago gut',\n",
       " 'ago ha',\n",
       " 'ago half',\n",
       " 'ago hanging',\n",
       " 'ago happy',\n",
       " 'ago hard',\n",
       " 'ago heart',\n",
       " 'ago hmm',\n",
       " 'ago huh',\n",
       " 'ago immediate',\n",
       " 'ago important',\n",
       " 'ago infrequently',\n",
       " 'ago issues',\n",
       " 'ago job',\n",
       " 'ago kind',\n",
       " 'ago knew',\n",
       " 'ago know',\n",
       " 'ago knows',\n",
       " 'ago las',\n",
       " 'ago laughter',\n",
       " 'ago left',\n",
       " 'ago letting',\n",
       " 'ago living',\n",
       " 'ago long',\n",
       " 'ago lot',\n",
       " 'ago mad',\n",
       " 'ago mean',\n",
       " 'ago meds',\n",
       " 'ago mentor',\n",
       " 'ago mexico',\n",
       " 'ago mom',\n",
       " 'ago month',\n",
       " 'ago months',\n",
       " 'ago mother',\n",
       " 'ago moved',\n",
       " 'ago nephew',\n",
       " 'ago older',\n",
       " 'ago paid',\n",
       " 'ago parent',\n",
       " 'ago paris',\n",
       " 'ago percent',\n",
       " 'ago pregnant',\n",
       " 'ago pretty',\n",
       " 'ago questions',\n",
       " 'ago received',\n",
       " 'ago relaxing',\n",
       " 'ago repeat',\n",
       " 'ago road',\n",
       " 'ago sad',\n",
       " 'ago saddened',\n",
       " 'ago saint',\n",
       " 'ago satisfied',\n",
       " 'ago saved',\n",
       " 'ago school',\n",
       " 'ago see',\n",
       " 'ago seek',\n",
       " 'ago seminar',\n",
       " 'ago shy',\n",
       " 'ago sigh',\n",
       " 'ago smell',\n",
       " 'ago sniffle',\n",
       " 'ago something',\n",
       " 'ago son',\n",
       " 'ago sound',\n",
       " 'ago speak',\n",
       " 'ago stay',\n",
       " 'ago stemmed',\n",
       " 'ago studied',\n",
       " 'ago stuff',\n",
       " 'ago suggested',\n",
       " 'ago suicidal',\n",
       " 'ago tension',\n",
       " 'ago things',\n",
       " 'ago thinking',\n",
       " 'ago thought',\n",
       " 'ago time',\n",
       " 'ago timeliness',\n",
       " 'ago told',\n",
       " 'ago totally',\n",
       " 'ago tough',\n",
       " 'ago town',\n",
       " 'ago traffic',\n",
       " 'ago transportation',\n",
       " 'ago trip',\n",
       " 'ago true',\n",
       " 'ago understanding',\n",
       " 'ago unhappy',\n",
       " 'ago wanted',\n",
       " 'ago wanting',\n",
       " 'ago wedding',\n",
       " 'ago week',\n",
       " 'ago wehad',\n",
       " 'ago wife',\n",
       " 'ago word',\n",
       " 'ago working',\n",
       " 'ago xxx',\n",
       " 'ago ye',\n",
       " 'ago yeah',\n",
       " 'ago year',\n",
       " 'ago years',\n",
       " 'agree',\n",
       " 'agree believe',\n",
       " 'agree disagree',\n",
       " 'agree suppose',\n",
       " 'agree things',\n",
       " 'agree want',\n",
       " 'agreed',\n",
       " 'agreed cape',\n",
       " 'agreed pay',\n",
       " 'agreed resort',\n",
       " 'agreed roommate',\n",
       " 'agreed roommates',\n",
       " 'agreed thing',\n",
       " 'agreed time',\n",
       " 'agreed utah',\n",
       " 'agreement',\n",
       " 'agreement decided',\n",
       " 'agreements',\n",
       " 'agreements gon',\n",
       " 'ahead',\n",
       " 'ahead classes',\n",
       " 'ahead daunting',\n",
       " 'ahead different',\n",
       " 'ahead feel',\n",
       " 'ahead game',\n",
       " 'ahead grew',\n",
       " 'ahead guess',\n",
       " 'ahead know',\n",
       " 'ahead ladder',\n",
       " 'ahead major',\n",
       " 'ahead officially',\n",
       " 'ahead pursued',\n",
       " 'ahead sees',\n",
       " 'ahead sit',\n",
       " 'ahead sort',\n",
       " 'ahead sounds',\n",
       " 'ahh',\n",
       " 'ahh comparison',\n",
       " 'aide',\n",
       " 'aide know',\n",
       " 'air',\n",
       " 'air amazing',\n",
       " 'air cleaner',\n",
       " 'air force',\n",
       " 'air good',\n",
       " 'air know',\n",
       " 'air knows',\n",
       " 'air noise',\n",
       " 'air pattern',\n",
       " 'air quality',\n",
       " 'airfares',\n",
       " 'airfares bit',\n",
       " 'airheaded',\n",
       " 'airheaded spacey',\n",
       " 'airplane',\n",
       " 'airplane clears',\n",
       " 'airplane first',\n",
       " 'airplane traveling',\n",
       " 'airplanes',\n",
       " 'airplanes prefer',\n",
       " 'airport',\n",
       " 'airport crash',\n",
       " 'airport finally',\n",
       " 'airport garden',\n",
       " 'airport guess',\n",
       " 'airport never',\n",
       " 'airport yeah',\n",
       " 'alabama',\n",
       " 'alabama people',\n",
       " 'alarm',\n",
       " 'alarm hard',\n",
       " 'alarm laughter',\n",
       " 'alarm phone',\n",
       " 'alarm question',\n",
       " 'alarm wake',\n",
       " 'alarm yoga',\n",
       " 'alaska',\n",
       " 'alaska aleutian',\n",
       " 'alaska couple',\n",
       " 'alaska small',\n",
       " 'alaska want',\n",
       " 'albatross',\n",
       " 'albatross life',\n",
       " 'albert',\n",
       " 'albert einstein',\n",
       " 'alcohol',\n",
       " 'alcohol counselor',\n",
       " 'alcohol drug',\n",
       " 'alcohol life',\n",
       " 'alcohol temper',\n",
       " 'alert',\n",
       " 'alert alive',\n",
       " 'aleutian',\n",
       " 'aleutian islands',\n",
       " 'alexandria',\n",
       " 'alexandria virginia',\n",
       " 'alhambra',\n",
       " 'alhambra weather',\n",
       " 'alienating',\n",
       " 'alienating family',\n",
       " 'alive',\n",
       " 'alive advanced',\n",
       " 'alive alive',\n",
       " 'alive blessed',\n",
       " 'alive bye',\n",
       " 'alive crazy',\n",
       " 'alive focus',\n",
       " 'alive fully',\n",
       " 'alive goodbye',\n",
       " 'alive happy',\n",
       " 'alive healthy',\n",
       " 'alive interact',\n",
       " 'alive know',\n",
       " 'alive knowing',\n",
       " 'alive laughter',\n",
       " 'alive live',\n",
       " 'alive rested',\n",
       " 'alive stuff',\n",
       " 'alive ta',\n",
       " 'alive yeah',\n",
       " 'alleviate',\n",
       " 'alleviate metropolitan',\n",
       " 'alleviate nightmare',\n",
       " 'alleys',\n",
       " 'alleys speak',\n",
       " 'allo',\n",
       " 'allo regret',\n",
       " 'allowed',\n",
       " 'allowed carry',\n",
       " 'allowed know',\n",
       " 'allowed pursue',\n",
       " 'allowed step',\n",
       " 'allowing',\n",
       " 'allowing affect',\n",
       " 'allowing decide',\n",
       " 'allowing decisions',\n",
       " 'allowing people',\n",
       " 'alls',\n",
       " ...]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfid3.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "60138d06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=0.1, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb3.fit(vector3, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "b567109f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-10.97509046, -10.97509046, -10.97509046, ..., -10.97509046,\n",
       "        -10.97509046, -10.97509046]])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb3.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "021fe3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs = pd.Series(nb3.coef_[0], index = tfid3.get_feature_names())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "49bc93c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "know        -7.215093\n",
       "laughter    -7.552420\n",
       "yeah        -7.626809\n",
       "people      -7.749664\n",
       "lot         -7.832355\n",
       "things      -7.894621\n",
       "sigh        -7.998959\n",
       "good        -8.086477\n",
       "time        -8.149818\n",
       "mean        -8.230435\n",
       "feel        -8.311273\n",
       "guess       -8.365057\n",
       "pretty      -8.457704\n",
       "life        -8.507802\n",
       "kind        -8.519011\n",
       "something   -8.553809\n",
       "hard        -8.581318\n",
       "see         -8.593950\n",
       "different   -8.639767\n",
       "sleep       -8.640538\n",
       "school      -8.645240\n",
       "years       -8.681313\n",
       "better      -8.689959\n",
       "kinda       -8.698361\n",
       "person      -8.717071\n",
       "love        -8.744643\n",
       "ago         -8.776279\n",
       "anything    -8.781809\n",
       "day         -8.785429\n",
       "job         -8.804338\n",
       "dtype: float64"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coefs.sort_values(ascending=False).head(30)\n",
    "\n",
    "#agregar palabras a las stopwords , por ejemplo 'twitter', 'com'\n",
    "#agregar precision, accuracy, recall , f1 , etc\n",
    "#min, max \n",
    "#max_features:overfitting? ~ regularizacion\n",
    "\n",
    "#to do: traduccion?\n",
    "#red de deeplearning para buscar mejor score\n",
    "#testeos\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109c7188",
   "metadata": {},
   "source": [
    "# save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "6dd61f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "56ba23fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nb3_model.sav']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''joblib.dump(nb3, 'nb3_model.sav')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c8dbe069",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tfid3_vectorizer.sav']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''joblib.dump(tfid3 , 'tfid3_vectorizer.sav')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85c116f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''loaded_model = joblib.load(filename)\n",
    "result = loaded_model.score(X_test, Y_test)\n",
    "print(result'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
