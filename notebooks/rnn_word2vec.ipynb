{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a298ccb6",
   "metadata": {},
   "source": [
    "# traer data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d3d5620",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-08 18:49:23.206845: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-03-08 18:49:23.206891: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5ad1a39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lay bed hour point back pain get work minut cl...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dlitedaili dont play wouldnt want anyth world</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rewebcoach hey handsom time get day go</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>get readi tenni maryyi</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hear song band almost never play favourit radi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          clean_text  label\n",
       "0  lay bed hour point back pain get work minut cl...      1\n",
       "1      dlitedaili dont play wouldnt want anyth world      0\n",
       "2             rewebcoach hey handsom time get day go      0\n",
       "3                             get readi tenni maryyi      0\n",
       "4  hear song band almost never play favourit radi...      0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#bring your data!\n",
    "path = '/home/lucaspancotto/code/JoacoSoulez/mental_health_first_aid_evaluation/data/twitter_reddit_text.csv'\n",
    "data = pd.read_csv(f'{path}')\n",
    "data.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f83e42",
   "metadata": {},
   "source": [
    "## train test split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7954f8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.clean_text\n",
    "y = data.label\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X , y ,test_size = 0.3 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64326438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of X_train 17757 length of X_test 7611 length of y_train 17757 length of y_test 7611\n"
     ]
    }
   ],
   "source": [
    "print('length of X_train',len(X_train), 'length of X_test',len(X_test), 'length of y_train',len(y_train), 'length of y_test',len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86f8fc3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000     like lone consciou fish small fish tank cat tr...\n",
       "7389     feel like want die need dad understand happen ...\n",
       "22786    anxieti told meto keep move mindwil stopworri ...\n",
       "21919    may may heard term peopl friend easi time anyo...\n",
       "23136                                         happi gilmor\n",
       "                               ...                        \n",
       "8696     friend use hangout fun bore sit apart liter no...\n",
       "2923     peopl seem like life mostli tire stress appli ...\n",
       "15953    richardbair good see tell u javafx dream fit r...\n",
       "9497     normal feel physic pain chest part sad depress...\n",
       "20458    deal inevit bad time know cycl life make wors ...\n",
       "Name: clean_text, Length: 17757, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd7aa37",
   "metadata": {},
   "source": [
    "## word tokenize and word 2 vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20b2dad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "adc36e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "X_train = [word_tokenize(str(_)) for _ in X_train]\n",
    "X_test = [word_tokenize(str(_)) for _ in X_test]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe15b950",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# This line trains an entire embedding for the words in your train set\n",
    "word2vec = Word2Vec(sentences=X_train, vector_size=100, window=5, min_count=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8369ffc8",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.8452655 ,  0.25012982, -1.2978383 , -0.4793716 ,  0.14725228,\n",
       "        -0.20574152,  0.44082004,  0.5420447 , -0.95198196,  1.1453902 ,\n",
       "        -0.10333741,  0.5087391 , -1.158776  ,  0.7944987 , -0.86729294,\n",
       "        -0.50141287, -0.36737028,  0.84271556,  0.6059754 , -1.6074985 ,\n",
       "         1.0747947 , -0.19607568,  0.17277852,  0.6352158 ,  0.02664622,\n",
       "         0.13611922, -0.04729357,  0.46771288, -1.2753919 , -0.47647512,\n",
       "         1.2539909 ,  0.85608655, -0.09079319, -0.7827334 ,  0.7846839 ,\n",
       "        -0.5276445 , -0.54759645,  0.81686765,  0.9883455 , -0.38821372,\n",
       "        -0.13513185, -0.6919716 , -0.24768063,  0.8319789 ,  0.70519066,\n",
       "        -0.49736762, -0.453065  ,  0.13204856,  1.8063902 ,  0.1575513 ,\n",
       "         0.93316925,  0.9479029 , -0.2389582 , -0.01122373, -0.6230514 ,\n",
       "         0.01590823, -1.1017666 , -1.1215179 , -0.13300225, -0.3430948 ,\n",
       "         0.2863206 , -0.07312767,  0.08149474, -0.24313377, -0.20461252,\n",
       "         0.42688867,  1.525485  ,  0.9619453 , -0.56131256,  1.2186394 ,\n",
       "        -0.6577121 ,  0.70351493,  0.555926  ,  0.9833427 ,  0.7932801 ,\n",
       "         0.9305336 , -0.23990753,  0.95811874, -0.6962953 , -0.7695662 ,\n",
       "        -0.45266792, -0.50968105, -1.0213052 ,  0.13411923, -0.5690854 ,\n",
       "        -0.19959632,  1.2989837 ,  0.04972772, -1.2798401 , -0.71795696,\n",
       "         0.63415504,  0.25821605, -0.4667803 , -0.8341939 , -0.0793286 ,\n",
       "        -0.90717053,  0.49761626,  1.0237608 , -0.3742779 , -0.05072518],\n",
       "       [-0.55991405,  0.548817  , -0.36354145, -0.07237271,  0.19361259,\n",
       "        -0.38843542, -0.32035694,  0.6812738 , -0.0559911 ,  0.30500576,\n",
       "        -0.30461863, -0.4469161 , -0.18686555,  0.16913378,  0.2058417 ,\n",
       "        -0.16960746, -0.5466521 ,  0.04170131,  0.12772113, -0.46507186,\n",
       "         0.3956577 , -0.30079633, -0.07799735,  0.48005074,  0.0562507 ,\n",
       "        -0.04098029,  0.26638108, -0.65155935, -0.02459803, -0.0411759 ,\n",
       "         0.21041948,  0.22057296,  0.32989892,  0.22305833,  0.51496655,\n",
       "         0.24288222,  0.15909705,  0.3286641 , -0.02701549, -0.24860574,\n",
       "        -0.02973243,  0.22004376, -0.6716761 ,  0.44083875, -0.4438145 ,\n",
       "         0.04247067,  0.20279033, -0.14383839,  0.25450212,  0.29502323,\n",
       "         0.10344357, -0.6032156 , -0.08773927,  0.16536267,  0.2488297 ,\n",
       "         0.0133836 , -0.01165722, -0.23326667, -0.6301959 , -0.01364767,\n",
       "        -0.1391811 ,  0.3936769 , -0.25890854, -0.66406924, -0.484385  ,\n",
       "        -0.18925564,  0.3375688 ,  0.1636672 , -0.49607462,  0.58122236,\n",
       "        -0.09801063, -0.14446187,  0.24117474, -0.03979491,  0.44638705,\n",
       "         0.23213229, -0.38755283, -0.29092053, -0.39226323, -0.07550927,\n",
       "        -0.6486834 ,  0.2669025 ,  0.02253787,  0.37877446, -0.28955752,\n",
       "         0.08688483, -0.14025994,  0.41860363,  0.41693226, -0.28739795,\n",
       "         0.5865199 , -0.05233834,  0.5090061 ,  0.5279503 ,  0.4291177 ,\n",
       "         0.33216363,  0.16437414, -0.28123227,  0.14827502,  0.13666703]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.wv['depress' , 'hello']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ac643f",
   "metadata": {},
   "source": [
    "# data embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907d7996",
   "metadata": {},
   "source": [
    "## embed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac2928cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_sentence(word2vec, sentence):\n",
    "    # $CHALLENGIFY_BEGIN\n",
    "    embedded_sentence = []\n",
    "    for word in sentence:\n",
    "        if word in word2vec.wv:\n",
    "            embedded_sentence.append(word2vec.wv[word])\n",
    "        \n",
    "    return np.array(embedded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e6d47c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding(word2vec, sentences):\n",
    "    # $CHALLENGIFY_BEGIN\n",
    "    embed = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        embedded_sentence = embed_sentence(word2vec, sentence)\n",
    "        embed.append(embedded_sentence)\n",
    "        \n",
    "    return embed\n",
    "    # $CHALLENGIFY_END\n",
    "    \n",
    "X_train = embedding(word2vec, X_train)\n",
    "X_test = embedding(word2vec, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d9eb3b",
   "metadata": {},
   "source": [
    "## pad data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c1e6fcc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''X_train_pad = pad_sequences(X_train, dtype='float', padding='post')\n",
    "X_test_pad = pad_sequences(X_test, dtype='float', padding='post')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825d7dba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92dfeba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85ea357",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2ef7f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace33d74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168ba4ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebf7817",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Let's build the Neural network now\n",
    "from tensorflow.keras import layers, Sequential\n",
    "\n",
    "# Size of your embedding space = size to represent each word\n",
    "embedding_size = 100\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(layers.LSTM(20))\n",
    "model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
