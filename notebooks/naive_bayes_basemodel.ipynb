{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6054c8d2",
   "metadata": {},
   "source": [
    "# Bring data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "526689db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "93cdc1b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "please type your path to the database: /home/lucaspancotto/code/JoacoSoulez/mental_health_first_aid_evaluation/data/clean_depressionvspossitive\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_24394/1042925164.py:3: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv(f'{path}')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>ids</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>799999</td>\n",
       "      <td>1467822272</td>\n",
       "      <td>love health4uandpets u guys r best</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>800000</td>\n",
       "      <td>1467822273</td>\n",
       "      <td>im meeting one besties tonight cant wait girl ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>800001</td>\n",
       "      <td>1467822283</td>\n",
       "      <td>darealsunisakim thanks twitter add sunisa got ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>800002</td>\n",
       "      <td>1467822287</td>\n",
       "      <td>sick really cheap hurts much eat real food plu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>800003</td>\n",
       "      <td>1467822293</td>\n",
       "      <td>lovesbrooklyn2 effect everyone</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0         ids                                              Tweet  \\\n",
       "0      799999  1467822272                 love health4uandpets u guys r best   \n",
       "1      800000  1467822273  im meeting one besties tonight cant wait girl ...   \n",
       "2      800001  1467822283  darealsunisakim thanks twitter add sunisa got ...   \n",
       "3      800002  1467822287  sick really cheap hurts much eat real food plu...   \n",
       "4      800003  1467822293                     lovesbrooklyn2 effect everyone   \n",
       "\n",
       "   label  \n",
       "0      0  \n",
       "1      0  \n",
       "2      0  \n",
       "3      0  \n",
       "4      0  "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#bring your data!\n",
    "path = input('please type your path to the database: ')\n",
    "data = pd.read_csv(f'{path}')\n",
    "data.head()\n",
    "\n",
    "#/home/lucaspancotto/code/JoacoSoulez/mental_health_first_aid_evaluation/data/clean_depressionvspossitive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "670627dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_data = data[data.label == 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5b64541e",
   "metadata": {},
   "outputs": [],
   "source": [
    "depression_data = data[data.label==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "097e7972",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_positive_data = positive_data.sample( n  = 2345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "749314ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_24394/2366412613.py:1: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  sample_data = sample_positive_data.append(depression_data)\n"
     ]
    }
   ],
   "source": [
    "sample_data = sample_positive_data.append(depression_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d271f5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data =  sample_data.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "538b89e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "choose the text column of your dataTweet\n",
      "choose the target column of your datalabel\n"
     ]
    }
   ],
   "source": [
    "text_col = input('choose the text column of your data')\n",
    "X = sample_data[f'{text_col}']\n",
    "\n",
    "target_col = input('choose the target column of your data')\n",
    "y = sample_data[f'{target_col}']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ecfccc",
   "metadata": {},
   "source": [
    "# naive bayes model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd01e52",
   "metadata": {},
   "source": [
    "## cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "56419c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV , cross_val_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "84a04a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfid = TfidfVectorizer(ngram_range=(4,5))\n",
    "nb = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a64ea47f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "147717    way les paradise looking bk bridge togoski han...\n",
       "802123    tropical depression gon na steal sun away mm f...\n",
       "801293    smile cured depression emoji heavy red heart e...\n",
       "800991    kudos michael phelps breaking stigma around ta...\n",
       "110237    retrorewind pls play journey stop believin jou...\n",
       "                                ...                        \n",
       "273227                                 hey everybody follow\n",
       "801045    next time think deepika padukone uses depressi...\n",
       "801381    supporting mental illness limited depression a...\n",
       "801491    emoji police cars revolving light tickets stil...\n",
       "138185               venkateshkumar pleeessss help dont say\n",
       "Name: Tweet, Length: 4690, dtype: object"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = X.apply(str)\n",
    "#X.apply(word_tokenize)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1177ecc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = tfid.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "61de14b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X = pd.DataFrame(X.toarray(),columns = tfid.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "110bcc62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:    1.0s finished\n"
     ]
    }
   ],
   "source": [
    "simple_cross_val_recall = cross_val_score(nb, vector , y , scoring= 'recall', cv=3,\n",
    "               n_jobs = -1 , verbose = 1\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7b754447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the recall:  0.15950652375853197\n"
     ]
    }
   ],
   "source": [
    "print('the recall: ' ,np.mean(simple_cross_val_recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c09e2822",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:    0.6s finished\n"
     ]
    }
   ],
   "source": [
    "simple_cross_val_accuracy = cross_val_score(nb, vector , y , scoring= 'accuracy', cv=3,\n",
    "               n_jobs = -1 , verbose = 1\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cbaf1aff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the accuracy:  0.5554359689298402\n"
     ]
    }
   ],
   "source": [
    "print('the accuracy: ' , np.mean(simple_cross_val_accuracy) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3649cb0",
   "metadata": {},
   "source": [
    "## grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "0fa522e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "463236                                  bhunt get tacky day\n",
       "801463    spent th grade crying absolutely reason wish s...\n",
       "801367    added video youtube playlist http youtu kfxvmu...\n",
       "800891    everyone war going inside happiness depression...\n",
       "800290    friends struggle app helps avoid potentially h...\n",
       "                                ...                        \n",
       "59105                            holani nah overate da luau\n",
       "800628                             welp im goign depression\n",
       "105932    learned aobut thus quot movies han solo shot f...\n",
       "157753    junkinthetrunkk great video amber looked like ...\n",
       "801344    protects onset https goo gl xswslg amjpsychiat...\n",
       "Name: Tweet, Length: 4690, dtype: object"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfid2 = TfidfVectorizer(ngram_range=(4,5))\n",
    "nb2 = MultinomialNB()\n",
    "X = X.apply(str)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "760852be",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    ('TfidfVectorizer', tfid2),\n",
    "    ('MultinomialNB()' , nb2)\n",
    "    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "8ea64c7e",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('TfidfVectorizer',\n",
       "   TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                   min_df=1, ngram_range=(4, 5), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True, stop_words=None, strip_accents=None,\n",
       "                   sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=None, use_idf=True, vocabulary=None)),\n",
       "  ('MultinomialNB()',\n",
       "   MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       " 'verbose': False,\n",
       " 'TfidfVectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=1, ngram_range=(4, 5), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True, stop_words=None, strip_accents=None,\n",
       "                 sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=None, use_idf=True, vocabulary=None),\n",
       " 'MultinomialNB()': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       " 'TfidfVectorizer__analyzer': 'word',\n",
       " 'TfidfVectorizer__binary': False,\n",
       " 'TfidfVectorizer__decode_error': 'strict',\n",
       " 'TfidfVectorizer__dtype': numpy.float64,\n",
       " 'TfidfVectorizer__encoding': 'utf-8',\n",
       " 'TfidfVectorizer__input': 'content',\n",
       " 'TfidfVectorizer__lowercase': True,\n",
       " 'TfidfVectorizer__max_df': 1.0,\n",
       " 'TfidfVectorizer__max_features': None,\n",
       " 'TfidfVectorizer__min_df': 1,\n",
       " 'TfidfVectorizer__ngram_range': (4, 5),\n",
       " 'TfidfVectorizer__norm': 'l2',\n",
       " 'TfidfVectorizer__preprocessor': None,\n",
       " 'TfidfVectorizer__smooth_idf': True,\n",
       " 'TfidfVectorizer__stop_words': None,\n",
       " 'TfidfVectorizer__strip_accents': None,\n",
       " 'TfidfVectorizer__sublinear_tf': False,\n",
       " 'TfidfVectorizer__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       " 'TfidfVectorizer__tokenizer': None,\n",
       " 'TfidfVectorizer__use_idf': True,\n",
       " 'TfidfVectorizer__vocabulary': None,\n",
       " 'MultinomialNB()__alpha': 1.0,\n",
       " 'MultinomialNB()__class_prior': None,\n",
       " 'MultinomialNB()__fit_prior': True}"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "97bb330d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_grid = {\n",
    "    'TfidfVectorizer__ngram_range': [(1,2) , (2,3), (3,4),(4, 5)],\n",
    "    'MultinomialNB()__alpha': [0.1 , 0.5 , 1.0]\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "84ce48a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_recall= GridSearchCV(pipe,\n",
    "    pipe_grid,\n",
    "    scoring='recall',\n",
    "    n_jobs=-1,\n",
    "    \n",
    "   \n",
    "    cv=3,\n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "fc927baf",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  36 out of  36 | elapsed:    4.8s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score=nan,\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('TfidfVectorizer',\n",
       "                                        TfidfVectorizer(analyzer='word',\n",
       "                                                        binary=False,\n",
       "                                                        decode_error='strict',\n",
       "                                                        dtype=<class 'numpy.float64'>,\n",
       "                                                        encoding='utf-8',\n",
       "                                                        input='content',\n",
       "                                                        lowercase=True,\n",
       "                                                        max_df=1.0,\n",
       "                                                        max_features=None,\n",
       "                                                        min_df=1,\n",
       "                                                        ngram_range=(4, 5),\n",
       "                                                        norm='l2',\n",
       "                                                        preprocessor=None,\n",
       "                                                        smooth_idf=True,\n",
       "                                                        stop_words=None...\n",
       "                                                        tokenizer=None,\n",
       "                                                        use_idf=True,\n",
       "                                                        vocabulary=None)),\n",
       "                                       ('MultinomialNB()',\n",
       "                                        MultinomialNB(alpha=1.0,\n",
       "                                                      class_prior=None,\n",
       "                                                      fit_prior=True))],\n",
       "                                verbose=False),\n",
       "             iid='deprecated', n_jobs=-1,\n",
       "             param_grid={'MultinomialNB()__alpha': [0.1, 0.5, 1.0],\n",
       "                         'TfidfVectorizer__ngram_range': [(1, 2), (2, 3),\n",
       "                                                          (3, 4), (4, 5)]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='recall', verbose=1)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_recall.fit(X , y )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "e3edd241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the best params for recall:  {'MultinomialNB()__alpha': 1.0, 'TfidfVectorizer__ngram_range': (1, 2)}\n",
      "the best recall score:  0.9773958016096268\n"
     ]
    }
   ],
   "source": [
    "print('the best params for recall: ',search_recall.best_params_)\n",
    "print('the best recall score: ' , search_recall.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "2cc21a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_recall = search_recall.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "97a5109d",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'MultinomialNB' object has no attribute 'params'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_24394/1917458273.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbest_model_recall\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"MultinomialNB()\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'MultinomialNB' object has no attribute 'params'"
     ]
    }
   ],
   "source": [
    "best_model_recall[\"MultinomialNB()\"].params()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959bfdb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecb5957",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "0ebcecdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_accuracy= GridSearchCV(pipe,\n",
    "    pipe_grid,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    \n",
    "   \n",
    "    cv=3,\n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "a7067f15",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  36 out of  36 | elapsed:    3.1s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score=nan,\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('TfidfVectorizer',\n",
       "                                        TfidfVectorizer(analyzer='word',\n",
       "                                                        binary=False,\n",
       "                                                        decode_error='strict',\n",
       "                                                        dtype=<class 'numpy.float64'>,\n",
       "                                                        encoding='utf-8',\n",
       "                                                        input='content',\n",
       "                                                        lowercase=True,\n",
       "                                                        max_df=1.0,\n",
       "                                                        max_features=None,\n",
       "                                                        min_df=1,\n",
       "                                                        ngram_range=(4, 5),\n",
       "                                                        norm='l2',\n",
       "                                                        preprocessor=None,\n",
       "                                                        smooth_idf=True,\n",
       "                                                        stop_words=None...\n",
       "                                                        tokenizer=None,\n",
       "                                                        use_idf=True,\n",
       "                                                        vocabulary=None)),\n",
       "                                       ('MultinomialNB()',\n",
       "                                        MultinomialNB(alpha=1.0,\n",
       "                                                      class_prior=None,\n",
       "                                                      fit_prior=True))],\n",
       "                                verbose=False),\n",
       "             iid='deprecated', n_jobs=-1,\n",
       "             param_grid={'MultinomialNB()__alpha': [0.1, 0.5, 1.0],\n",
       "                         'TfidfVectorizer__ngram_range': [(1, 2), (2, 3),\n",
       "                                                          (3, 4), (4, 5)]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='accuracy', verbose=1)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_accuracy.fit(X , y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "d33c2450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the best params for accuracy:  {'MultinomialNB()__alpha': 1.0, 'TfidfVectorizer__ngram_range': (1, 2)}\n",
      "the best accuracy score:  0.9057575574111255\n"
     ]
    }
   ],
   "source": [
    "print('the best params for accuracy: ',search_accuracy.best_params_)\n",
    "print('the best accuracy score: ' , search_accuracy.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325125af",
   "metadata": {},
   "source": [
    "# testeo y analisis del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915f9bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''{'MultinomialNB()__alpha': 1.0, 'TfidfVectorizer__ngram_range': (1, 2)}'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "02a0ca3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfid3 = TfidfVectorizer(ngram_range=(1,2))\n",
    "nb3 = MultinomialNB(alpha = 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "619fab53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                min_df=1, ngram_range=(1, 2), norm='l2', preprocessor=None,\n",
       "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
       "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, use_idf=True, vocabulary=None)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfid3.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "6c3e6e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector3 = tfid3.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "da103aeb",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['100',\n",
       " '100 flashbacks',\n",
       " '11',\n",
       " '11 status',\n",
       " '111',\n",
       " '111 think',\n",
       " '15',\n",
       " '180425',\n",
       " '180425 mscpedit',\n",
       " '1xe',\n",
       " '1xe great',\n",
       " '2009',\n",
       " '2009 alright',\n",
       " '22',\n",
       " '22 jessleal7',\n",
       " '23275879',\n",
       " '2d3dilumdu',\n",
       " '2d3dilumdu barbara',\n",
       " '33',\n",
       " '33 kaotic2',\n",
       " '3eb0f4cc',\n",
       " '3eb0f4cc fd6',\n",
       " '4562',\n",
       " '4562 src',\n",
       " '53',\n",
       " '53 depression',\n",
       " '5adf8637e4b061c0bfa27b19',\n",
       " '5adf8637e4b061c0bfa27b19 ncid',\n",
       " '6cd2la93',\n",
       " '6ticzy',\n",
       " '6ticzy funniest',\n",
       " '76795',\n",
       " '76795 real',\n",
       " '83',\n",
       " '83 yeah',\n",
       " '86',\n",
       " '86 smrutimishra7',\n",
       " 'a10066808',\n",
       " 'a10066808 utm',\n",
       " 'a1j3hy',\n",
       " 'a1j3hy rekyewflst',\n",
       " 'a1o',\n",
       " 'a1o alot',\n",
       " 'a2xa',\n",
       " 'a690a0cebade',\n",
       " 'a6ly',\n",
       " 'a6yvb33h2y',\n",
       " 'a6zft61se',\n",
       " 'a8319686',\n",
       " 'a8319686 html',\n",
       " 'a9b8c66acf80fca040784e5e9cd',\n",
       " 'aa',\n",
       " 'aa poc',\n",
       " 'aaa',\n",
       " 'aaa shooters',\n",
       " 'aaaaaaaand',\n",
       " 'aaaaaaaand back',\n",
       " 'aaaaaah',\n",
       " 'aaaaaah https',\n",
       " 'aaaaaand',\n",
       " 'aaaaaand comes',\n",
       " 'aaaaayyyyy',\n",
       " 'aaaaayyyyy momma',\n",
       " 'aaaah',\n",
       " 'aaaah ok',\n",
       " 'aaahhhh',\n",
       " 'aaahhhh hypocrisy',\n",
       " 'aac3a',\n",
       " 'aakash',\n",
       " 'aakash great',\n",
       " 'aamir',\n",
       " 'aamir khan',\n",
       " 'aaronisanerd',\n",
       " 'aaronisanerd goodnight',\n",
       " 'aawjt3y',\n",
       " 'aawjt3y ocid',\n",
       " 'ab',\n",
       " 'ab challenges',\n",
       " 'ab months',\n",
       " 'abandoned',\n",
       " 'abandoned heartbroken',\n",
       " 'abatement',\n",
       " 'abatement depression',\n",
       " 'abatevintage',\n",
       " 'abatevintage listing',\n",
       " 'abba',\n",
       " 'abba ks',\n",
       " 'abbioth',\n",
       " 'abbioth thank',\n",
       " 'abcdefgolden',\n",
       " 'abcdefgolden nope',\n",
       " 'abdulla',\n",
       " 'abdulla shaikh',\n",
       " 'abe',\n",
       " 'abe lincoln',\n",
       " 'abel',\n",
       " 'abel love',\n",
       " 'abend',\n",
       " 'abend gestern',\n",
       " 'ability',\n",
       " 'ability manage',\n",
       " 'ability think',\n",
       " 'abit',\n",
       " 'abit breezy',\n",
       " 'able',\n",
       " 'able accomplish',\n",
       " 'able catch',\n",
       " 'able dry',\n",
       " 'able finish',\n",
       " 'able get',\n",
       " 'able leave',\n",
       " 'able pay',\n",
       " 'able pull',\n",
       " 'able relax',\n",
       " 'able running',\n",
       " 'able set',\n",
       " 'able sleepdepression',\n",
       " 'able start',\n",
       " 'able take',\n",
       " 'able turn',\n",
       " 'able wear',\n",
       " 'able without',\n",
       " 'able work',\n",
       " 'able write',\n",
       " 'abnormal',\n",
       " 'abnormal functional',\n",
       " 'abokssignature',\n",
       " 'abokssignature come',\n",
       " 'abort',\n",
       " 'abort child',\n",
       " 'abortion',\n",
       " 'abortion child',\n",
       " 'abortion considered',\n",
       " 'aboutdepressionfacts',\n",
       " 'aboutdepressionfacts com',\n",
       " 'abouts',\n",
       " 'abouts billingham',\n",
       " 'aboutt',\n",
       " 'aboutt watt',\n",
       " 'abraham',\n",
       " 'abraham fantastic',\n",
       " 'abraham lincoln',\n",
       " 'abroad',\n",
       " 'abroad depression',\n",
       " 'abs',\n",
       " 'abs gps',\n",
       " 'absence',\n",
       " 'absence cedar',\n",
       " 'absolut',\n",
       " 'absolut citron',\n",
       " 'absolutely',\n",
       " 'absolutely beautiful',\n",
       " 'absolutely forever',\n",
       " 'absolutely great',\n",
       " 'absolutely loved',\n",
       " 'absolutely reason',\n",
       " 'absolutely shit',\n",
       " 'absolutely terrible',\n",
       " 'absolutely true',\n",
       " 'absorbed',\n",
       " 'absorbed wood',\n",
       " 'abstract',\n",
       " 'abstract utm',\n",
       " 'abt',\n",
       " 'abt blk',\n",
       " 'abt depression',\n",
       " 'abt dis',\n",
       " 'abt drug',\n",
       " 'abt recovery',\n",
       " 'abt tell',\n",
       " 'abt voting',\n",
       " 'abt wan',\n",
       " 'abundance',\n",
       " 'abundance junk',\n",
       " 'abuse',\n",
       " 'abuse alochol',\n",
       " 'abuse anxiety',\n",
       " 'abuse bullying',\n",
       " 'abuse depression',\n",
       " 'abuse etc',\n",
       " 'abuse low',\n",
       " 'abuse old',\n",
       " 'abuse rape',\n",
       " 'abuse survivor',\n",
       " 'abuse ur',\n",
       " 'abused',\n",
       " 'abused support',\n",
       " 'abused teenager',\n",
       " 'ac',\n",
       " 'ac car',\n",
       " 'ac mccann',\n",
       " 'ac uk',\n",
       " 'academic',\n",
       " 'academic pressure',\n",
       " 'acb',\n",
       " 'acb aph',\n",
       " 'accept',\n",
       " 'accept experience',\n",
       " 'accept otherwise',\n",
       " 'accept refuse',\n",
       " 'accept stop',\n",
       " 'acceptance',\n",
       " 'acceptance phase',\n",
       " 'acceptance semi',\n",
       " 'acceptancei',\n",
       " 'acceptancei still',\n",
       " 'accepted',\n",
       " 'accepted cuz',\n",
       " 'accepting',\n",
       " 'accepting flickr',\n",
       " 'access',\n",
       " 'access agani',\n",
       " 'access drugs',\n",
       " 'access mental',\n",
       " 'accessory',\n",
       " 'accessory lmao',\n",
       " 'accident',\n",
       " 'accident dropped',\n",
       " 'accident suffering',\n",
       " 'accidentally',\n",
       " 'accidentally made',\n",
       " 'accidentally see',\n",
       " 'accidentally sent',\n",
       " 'accidentally took',\n",
       " 'accidently',\n",
       " 'accidently following',\n",
       " 'accname',\n",
       " 'accname guest',\n",
       " 'accommodate',\n",
       " 'accommodate iran',\n",
       " 'accommodation',\n",
       " 'accommodation mongol',\n",
       " 'accompanied',\n",
       " 'accompanied many',\n",
       " 'accomplish',\n",
       " 'accomplish since',\n",
       " 'accomplish something',\n",
       " 'accomplishments',\n",
       " 'accomplishments one',\n",
       " 'according',\n",
       " 'according cdcp',\n",
       " 'according new',\n",
       " 'according someone',\n",
       " 'account',\n",
       " 'account depression',\n",
       " 'account one',\n",
       " 'account queenxxlarray',\n",
       " 'account report',\n",
       " 'account shipped',\n",
       " 'account years',\n",
       " 'accountability',\n",
       " 'accountability transparency',\n",
       " 'accountin',\n",
       " 'accountin teacher',\n",
       " 'accounts',\n",
       " 'accounts pic',\n",
       " 'accurately',\n",
       " 'accurately media',\n",
       " 'accused',\n",
       " 'accused crap',\n",
       " 'accustomed',\n",
       " 'accustomed state',\n",
       " 'acforbes',\n",
       " 'acforbes wow',\n",
       " 'aches',\n",
       " 'aches sleeping',\n",
       " 'achieve',\n",
       " 'achieve dreams',\n",
       " 'achieve get',\n",
       " 'achieve goals',\n",
       " 'achieve high',\n",
       " 'achievements',\n",
       " 'achievements duplicated',\n",
       " 'achive',\n",
       " 'achive goals',\n",
       " 'acht',\n",
       " 'acht drei',\n",
       " 'acl7zpbbwf',\n",
       " 'acme',\n",
       " 'acme cartoons',\n",
       " 'acmephoto',\n",
       " 'acmephoto partay',\n",
       " 'acne',\n",
       " 'acne causing',\n",
       " 'acne cured',\n",
       " 'acne depression',\n",
       " 'acoustic',\n",
       " 'acoustic yumm',\n",
       " 'across',\n",
       " 'across happiness',\n",
       " 'across street',\n",
       " 'acrylic',\n",
       " 'acrylic charms',\n",
       " 'acs',\n",
       " 'acs relay',\n",
       " 'act',\n",
       " 'act forsure',\n",
       " 'act like',\n",
       " 'act self',\n",
       " 'act thoughts',\n",
       " 'acted',\n",
       " 'acted pre',\n",
       " 'acted violently',\n",
       " 'acting',\n",
       " 'acting depression',\n",
       " 'action',\n",
       " 'action gain',\n",
       " 'action one',\n",
       " 'action steps',\n",
       " 'actions',\n",
       " 'actions attention',\n",
       " 'actions know',\n",
       " 'activated',\n",
       " 'activated tweet',\n",
       " 'activation',\n",
       " 'activation much',\n",
       " 'active',\n",
       " 'active combat',\n",
       " 'active helps',\n",
       " 'active lately',\n",
       " 'active lesser',\n",
       " 'active lifestyle',\n",
       " 'active motivation',\n",
       " 'active oomph',\n",
       " 'active phase',\n",
       " 'active recent',\n",
       " 'active rejection',\n",
       " 'active robust',\n",
       " 'active viewer',\n",
       " 'activities',\n",
       " 'activities even',\n",
       " 'activity',\n",
       " 'activity decreases',\n",
       " 'activity excited',\n",
       " 'activity found',\n",
       " 'activity go',\n",
       " 'activity incident',\n",
       " 'activity moments',\n",
       " 'activity pic',\n",
       " 'activity protect',\n",
       " 'activity receptors',\n",
       " 'activity serotonin',\n",
       " 'actor',\n",
       " 'actor regret',\n",
       " 'actor verne',\n",
       " 'actors',\n",
       " 'actors symptoms',\n",
       " 'actress',\n",
       " 'actress attending',\n",
       " 'actress todd',\n",
       " 'acts',\n",
       " 'acts injustice',\n",
       " 'acts next',\n",
       " 'actual',\n",
       " 'actual chemical',\n",
       " 'actual definition',\n",
       " 'actual depression',\n",
       " 'actual father',\n",
       " 'actual imbalance',\n",
       " 'actual interaction',\n",
       " 'actual picture',\n",
       " 'actual vile',\n",
       " 'actually',\n",
       " 'actually believe',\n",
       " 'actually bipolar',\n",
       " 'actually commit',\n",
       " 'actually cures',\n",
       " 'actually curing',\n",
       " 'actually done',\n",
       " 'actually edits',\n",
       " 'actually everyone',\n",
       " 'actually feel',\n",
       " 'actually fighting',\n",
       " 'actually get',\n",
       " 'actually going',\n",
       " 'actually happy',\n",
       " 'actually helped',\n",
       " 'actually helps',\n",
       " 'actually high',\n",
       " 'actually literally',\n",
       " 'actually looks',\n",
       " 'actually lucky',\n",
       " 'actually makes',\n",
       " 'actually matter',\n",
       " 'actually one',\n",
       " 'actually poor',\n",
       " 'actually quite',\n",
       " 'actually real',\n",
       " 'actually realised',\n",
       " 'actually really',\n",
       " 'actually see',\n",
       " 'actually smiley',\n",
       " 'actually something',\n",
       " 'actually starts',\n",
       " 'actually struggle',\n",
       " 'actually thought',\n",
       " 'actually tickets',\n",
       " 'actually understands',\n",
       " 'actually working',\n",
       " 'actually yes',\n",
       " 'acupuncture',\n",
       " 'acute',\n",
       " 'acute became',\n",
       " 'acute depression',\n",
       " 'acute mumps',\n",
       " 'ad',\n",
       " 'ad aamir',\n",
       " 'ad airing',\n",
       " 'ad featuring',\n",
       " 'ad hominem',\n",
       " 'ad typecasting',\n",
       " 'adam',\n",
       " 'adam eve',\n",
       " 'adamconnor',\n",
       " 'adamconnor x4',\n",
       " 'adamdaily',\n",
       " 'adamdaily com',\n",
       " 'adamhoward2000',\n",
       " 'adamhoward2000 seriously',\n",
       " 'adammilo',\n",
       " 'adammilo depression',\n",
       " 'adamsilvera',\n",
       " 'adamsilvera books',\n",
       " 'add',\n",
       " 'add another',\n",
       " 'add anxiety',\n",
       " 'add banished',\n",
       " 'add course',\n",
       " 'add depression',\n",
       " 'add everyone',\n",
       " 'add facebook',\n",
       " 'add hair',\n",
       " 'add http',\n",
       " 'add mental',\n",
       " 'add music',\n",
       " 'add nfsw',\n",
       " 'add pain',\n",
       " 'add severe',\n",
       " 'add us',\n",
       " 'add ws',\n",
       " 'addams',\n",
       " 'addams family',\n",
       " 'added',\n",
       " 'added another',\n",
       " 'added chatroom',\n",
       " 'added diagnostic',\n",
       " 'added difficulties',\n",
       " 'added post',\n",
       " 'added video',\n",
       " 'adderall',\n",
       " 'addict',\n",
       " 'addict criminal',\n",
       " 'addict inspiration',\n",
       " 'addicted',\n",
       " 'addicted game',\n",
       " 'addicted like',\n",
       " 'addicted loving',\n",
       " 'addicted music',\n",
       " 'addicting',\n",
       " 'addicting theme',\n",
       " 'addiction',\n",
       " 'addiction anxiety',\n",
       " 'addiction arent',\n",
       " 'addiction blend',\n",
       " 'addiction chronic',\n",
       " 'addiction daily',\n",
       " 'addiction depression',\n",
       " 'addiction domestic',\n",
       " 'addiction dont',\n",
       " 'addiction eating',\n",
       " 'addiction good',\n",
       " 'addiction hotline844',\n",
       " 'addiction one',\n",
       " 'addiction psychoses',\n",
       " 'addiction selective',\n",
       " 'addictive',\n",
       " 'adding',\n",
       " 'adding flickr',\n",
       " 'addition',\n",
       " 'addition exercise',\n",
       " 'additional',\n",
       " 'additional forget',\n",
       " 'address',\n",
       " 'address bipolar',\n",
       " 'address invite',\n",
       " 'address mental',\n",
       " 'address questions',\n",
       " 'address send',\n",
       " 'address within',\n",
       " 'addresses',\n",
       " 'addresses abe',\n",
       " 'addresses pendemic',\n",
       " 'adds',\n",
       " 'adds depression',\n",
       " 'adds emoji',\n",
       " 'adecembertruth',\n",
       " 'adecembertruth thinking',\n",
       " 'adfe73e30fb043711926868',\n",
       " 'adfe73e30fb043711926868 utm',\n",
       " 'adhd',\n",
       " 'adhd affects',\n",
       " 'adhd depression',\n",
       " 'adhd get',\n",
       " 'adhd horrible',\n",
       " 'adhd one',\n",
       " 'adhd sad',\n",
       " 'adhe',\n",
       " 'adhe nice',\n",
       " 'adiemer',\n",
       " 'adiemer hi',\n",
       " 'aditya',\n",
       " 'aditya foxes',\n",
       " 'adjusted',\n",
       " 'adjusted im',\n",
       " 'adjusting',\n",
       " 'adjusting know',\n",
       " 'adkinskate',\n",
       " 'adkinskate sheffieldpsy',\n",
       " 'administration',\n",
       " 'administration lead',\n",
       " 'admiral',\n",
       " 'admiral jackson',\n",
       " 'admiration',\n",
       " 'admiration night',\n",
       " 'admire',\n",
       " 'admire sharing',\n",
       " 'admire strength',\n",
       " 'admit',\n",
       " 'admit depression',\n",
       " 'admit fault',\n",
       " 'admit hospital',\n",
       " 'admit others',\n",
       " 'admit problem',\n",
       " 'admit shocked',\n",
       " 'admits',\n",
       " 'admits ate',\n",
       " 'admitted',\n",
       " 'admitted patients',\n",
       " 'admitting',\n",
       " 'admitting higher',\n",
       " 'admitting problemyou',\n",
       " 'adms',\n",
       " 'adms getting',\n",
       " 'adnankhun',\n",
       " 'adnankhun nurulnxha',\n",
       " 'adobespark',\n",
       " 'adobespark create',\n",
       " 'adoizv',\n",
       " 'adoizv pic',\n",
       " 'adolescence',\n",
       " 'adolescence according',\n",
       " 'adolescence makes',\n",
       " 'adolescence year',\n",
       " 'adolescent',\n",
       " 'adolescent department',\n",
       " 'adolescent depression',\n",
       " 'adolescent may',\n",
       " 'adolescents',\n",
       " 'adolescents gay',\n",
       " 'adolescents higher',\n",
       " 'adolescents solution',\n",
       " 'adolescents university',\n",
       " 'adoption',\n",
       " 'adoption lgbqt',\n",
       " 'adorable',\n",
       " 'adorable af',\n",
       " 'adorable chicken',\n",
       " 'adorable keep',\n",
       " 'adorable usually',\n",
       " 'adorable want',\n",
       " 'adorablee',\n",
       " 'adorablee ill',\n",
       " 'adore',\n",
       " 'adore ddlovato',\n",
       " 'adragonswinging',\n",
       " 'adragonswinging absolutely',\n",
       " 'adreanainlb',\n",
       " 'adreanainlb mairinmurphy',\n",
       " 'adree',\n",
       " 'adree maggie',\n",
       " 'adrian',\n",
       " 'adrian music',\n",
       " 'adrian proud',\n",
       " 'adrianstevenson',\n",
       " 'adrianstevenson yep',\n",
       " 'adrianvziegler',\n",
       " 'adrianvziegler hi',\n",
       " 'adrift',\n",
       " 'adrift frequently',\n",
       " 'adsl',\n",
       " 'adsl story',\n",
       " 'aduc',\n",
       " 'aduc alf9qrd1524673800810',\n",
       " 'adulation',\n",
       " 'adulation demons',\n",
       " 'adult',\n",
       " 'adult comics',\n",
       " 'adult depression',\n",
       " 'adult expectations',\n",
       " 'adult life',\n",
       " 'adult men',\n",
       " 'adult supervision',\n",
       " 'adultery',\n",
       " 'adultery even',\n",
       " 'adults',\n",
       " 'adults addition',\n",
       " 'adults autism',\n",
       " 'adults diagnosis',\n",
       " 'adults happy',\n",
       " 'adults nesdo',\n",
       " 'advanced',\n",
       " 'advanced cia',\n",
       " 'advanced structure',\n",
       " 'advanced swear',\n",
       " 'advantage',\n",
       " 'advantage https',\n",
       " 'advantage people',\n",
       " 'adventure',\n",
       " 'adventure playground',\n",
       " 'adventure tmr',\n",
       " 'adventurer',\n",
       " 'adventurer total',\n",
       " 'adversity',\n",
       " 'adversity like',\n",
       " 'advertising',\n",
       " 'advertising us',\n",
       " 'advice',\n",
       " 'advice hrer',\n",
       " 'advice knowing',\n",
       " 'advice let',\n",
       " 'advice offered',\n",
       " 'advice relapse',\n",
       " 'advice suffering',\n",
       " 'advice want',\n",
       " 'advisor',\n",
       " 'advisor depression',\n",
       " 'advisor one',\n",
       " 'advocate',\n",
       " 'advocate causes',\n",
       " 'adweek',\n",
       " 'adweek friends',\n",
       " 'adweek pic',\n",
       " 'adweek qy03r6',\n",
       " 'aec',\n",
       " 'aec pa',\n",
       " 'aemtrt94vp',\n",
       " 'aerobics',\n",
       " 'aerobics strengthen',\n",
       " 'aesthetic',\n",
       " 'aesthetic depression',\n",
       " 'af',\n",
       " 'af depression',\n",
       " 'af though',\n",
       " 'af twice',\n",
       " 'af via',\n",
       " 'afaik',\n",
       " 'afaik know',\n",
       " 'affair',\n",
       " 'affairs',\n",
       " 'affairs start',\n",
       " 'affect',\n",
       " 'affect anyone',\n",
       " 'affect beliefs',\n",
       " 'affect ch',\n",
       " 'affect levels',\n",
       " 'affect millions',\n",
       " 'affect oughtibridge',\n",
       " 'affect people',\n",
       " 'affect quality',\n",
       " 'affect writing',\n",
       " 'affected',\n",
       " 'affected depression',\n",
       " 'affected severe',\n",
       " 'affected sleeping',\n",
       " 'affecting',\n",
       " 'affecting health',\n",
       " 'affective',\n",
       " 'affective disorder',\n",
       " 'affects',\n",
       " 'affects ability',\n",
       " 'affects patients',\n",
       " 'affects person',\n",
       " 'affects productivity',\n",
       " 'affects us',\n",
       " 'affirmations',\n",
       " 'affirmations http',\n",
       " 'affirming',\n",
       " 'affirming big',\n",
       " 'afford',\n",
       " 'afford bus',\n",
       " 'afford fucking',\n",
       " 'afford time',\n",
       " 'affordable',\n",
       " 'affordable cure',\n",
       " 'afg',\n",
       " 'afg soon',\n",
       " 'afloat',\n",
       " 'afraid',\n",
       " 'afraid admit',\n",
       " 'afraid clowns',\n",
       " 'afraid kill',\n",
       " 'afraid legally',\n",
       " 'afraid love',\n",
       " 'afraid ride',\n",
       " 'afraid try',\n",
       " 'afraid use',\n",
       " 'africa',\n",
       " 'africa r232',\n",
       " 'african',\n",
       " 'african americans',\n",
       " 'african suffer',\n",
       " 'afsfh',\n",
       " 'afsfh https',\n",
       " 'afsfh status',\n",
       " 'afteral',\n",
       " 'afteral aussie',\n",
       " 'afternoon',\n",
       " 'afternoon drank',\n",
       " 'afternoon laundry',\n",
       " 'aftershow',\n",
       " 'afterwards',\n",
       " 'afterwards love',\n",
       " 'afzaal',\n",
       " 'afzaal depression',\n",
       " 'aga930jikxz',\n",
       " 'aga930jikxz pic',\n",
       " 'agani',\n",
       " 'age',\n",
       " 'age gender',\n",
       " 'age geographical',\n",
       " 'age joined',\n",
       " 'age people',\n",
       " 'age raise',\n",
       " 'age wealth',\n",
       " 'age yeah',\n",
       " 'age yoga',\n",
       " 'agebettersheffield',\n",
       " 'agebettersheffield https',\n",
       " 'aged',\n",
       " 'aged fight',\n",
       " 'aged well',\n",
       " 'agenciaajn',\n",
       " 'agenciaajn com',\n",
       " 'agencies',\n",
       " 'agencies want',\n",
       " 'agency',\n",
       " 'agency novel',\n",
       " 'ages',\n",
       " 'ages good',\n",
       " 'ages pretty',\n",
       " 'ages quite',\n",
       " 'ages researchers',\n",
       " 'aggression',\n",
       " 'aggression never',\n",
       " 'aggressive',\n",
       " 'aggressive behavior',\n",
       " 'aggressive medication',\n",
       " 'aggressively',\n",
       " 'aggressively argumentative',\n",
       " 'aging',\n",
       " 'aging process',\n",
       " 'aging wrinkles',\n",
       " 'agitation',\n",
       " 'agitation nervousness',\n",
       " 'agnosticquest',\n",
       " 'agnosticquest onfiremission',\n",
       " 'ago',\n",
       " 'ago destroyed',\n",
       " 'ago excuse',\n",
       " 'ago experiencing',\n",
       " 'ago gladly',\n",
       " 'ago goes',\n",
       " 'ago gon',\n",
       " 'ago happened',\n",
       " 'ago happy',\n",
       " 'ago havent',\n",
       " 'ago memes',\n",
       " 'ago realized',\n",
       " 'ago scratching',\n",
       " 'ago speaking',\n",
       " 'ago took',\n",
       " 'ago years',\n",
       " 'agony',\n",
       " 'agree',\n",
       " 'agree disagree',\n",
       " 'agree lets',\n",
       " 'agree luna',\n",
       " 'agree nezvalenzuela',\n",
       " 'agree one',\n",
       " 'agree opinion',\n",
       " 'agree sing',\n",
       " 'agree tuna',\n",
       " 'agreed',\n",
       " 'agreed bullied',\n",
       " 'agreed emoji',\n",
       " 'agreed nick',\n",
       " 'agreements',\n",
       " 'agreements collectively',\n",
       " 'agynaathavaasi',\n",
       " 'agynaathavaasi core',\n",
       " 'ah',\n",
       " 'ah feeling',\n",
       " 'ah football',\n",
       " 'ah look',\n",
       " 'ah might',\n",
       " 'ah mike',\n",
       " 'ah never',\n",
       " 'ah old',\n",
       " 'ah vision',\n",
       " 'ah well',\n",
       " 'ah worth',\n",
       " 'ah yes',\n",
       " 'aha',\n",
       " 'aha cheers',\n",
       " 'ahahaha',\n",
       " 'ahahahahahahahahahahaha',\n",
       " 'ahahh',\n",
       " 'ahead',\n",
       " 'ahead admit',\n",
       " 'ahead darkness',\n",
       " 'ahead still',\n",
       " 'ahead ur',\n",
       " 'aherman2006',\n",
       " 'aherman2006 depression',\n",
       " 'ahh',\n",
       " 'ahh excited',\n",
       " 'ahh love',\n",
       " 'ahh remember',\n",
       " 'ahhaaahahahahaha',\n",
       " 'ahhaaahahahahaha baby',\n",
       " 'ahhh',\n",
       " 'ahhh lovin',\n",
       " 'ahhh tell',\n",
       " 'ahhh yay',\n",
       " 'ahhh1',\n",
       " 'ahhh1 sting',\n",
       " 'ahhhh',\n",
       " 'ahhhh daluckyme',\n",
       " 'ahja',\n",
       " 'ahja liebe',\n",
       " 'ahmedaniyal',\n",
       " 'ahmedaniyal toddtrotter3',\n",
       " 'ahmnohere',\n",
       " 'ahmnohere talk',\n",
       " 'ahold',\n",
       " 'ahold taking',\n",
       " 'ahoy',\n",
       " 'ahukewirjnbe7dtaahvgkgmkhu58bx4q',\n",
       " 'ahukewirjnbe7dtaahvgkgmkhu58bx4q auieygd',\n",
       " 'ai',\n",
       " 'ai ff6yu7',\n",
       " 'ai lin',\n",
       " 'aid',\n",
       " 'aid content',\n",
       " 'aight',\n",
       " 'aight wearing',\n",
       " 'aightindiaaaaaa',\n",
       " 'aightindiaaaaaa gives',\n",
       " 'aiin6b',\n",
       " 'aikenstandard',\n",
       " 'aikenstandard com',\n",
       " 'ails',\n",
       " 'aim',\n",
       " 'aim com',\n",
       " 'aim dd',\n",
       " 'aimee',\n",
       " 'aimee grz',\n",
       " 'aiming',\n",
       " 'aiming reduce',\n",
       " 'aims',\n",
       " 'aims help',\n",
       " 'aineprendo',\n",
       " 'aineprendo love',\n",
       " 'aint',\n",
       " 'aint nothing',\n",
       " 'aint really',\n",
       " 'air',\n",
       " 'air ambulance',\n",
       " 'air con',\n",
       " 'air conditioned',\n",
       " 'air listening',\n",
       " 'airene',\n",
       " 'airene intergrated',\n",
       " 'airing',\n",
       " 'airing week',\n",
       " 'airkarinabx23',\n",
       " 'airkarinabx23 kind',\n",
       " 'airplane',\n",
       " 'airplane departure',\n",
       " 'airplane depression',\n",
       " 'airplane early',\n",
       " 'airplane intense',\n",
       " 'airport',\n",
       " 'airport spoiled',\n",
       " 'airport wait',\n",
       " 'airtime',\n",
       " 'airtime pro',\n",
       " 'aish',\n",
       " 'aish achievements',\n",
       " 'aish fans',\n",
       " 'ajackielarsen',\n",
       " 'ajackielarsen greatdaneuj',\n",
       " 'ajackielarsen idk',\n",
       " 'ajackielarsen nemosanartist',\n",
       " 'ajackielarsen status',\n",
       " 'ajak',\n",
       " 'ajak haryaton',\n",
       " 'ajax',\n",
       " 'ajax code',\n",
       " 'ajbrzski',\n",
       " 'ajbrzski angrierthanmost',\n",
       " 'ajit9988',\n",
       " 'ajit9988 intriguing',\n",
       " 'ajp',\n",
       " 'ajp doi',\n",
       " 'ajp exerciseworks',\n",
       " 'ajp including',\n",
       " 'ajp platform',\n",
       " 'ajp psychiatryonline',\n",
       " 'ajsgmajc',\n",
       " 'ajsgmajc donnawithrow2',\n",
       " 'ak2f5ryoo',\n",
       " 'aka',\n",
       " 'aka conciquences',\n",
       " 'aka democratic',\n",
       " 'aka depression',\n",
       " 'akbar',\n",
       " 'akbar thank',\n",
       " 'akcounsellor',\n",
       " 'akcounsellor knows',\n",
       " 'akiralove',\n",
       " 'akiralove wow',\n",
       " 'ako',\n",
       " 'ako mag',\n",
       " 'ako ng',\n",
       " 'ako within',\n",
       " 'akoomatsu',\n",
       " 'akoomatsu status',\n",
       " 'akp',\n",
       " 'akp turkey',\n",
       " 'aladdin',\n",
       " 'aladdin oh',\n",
       " 'alamat',\n",
       " 'alamat payaso',\n",
       " 'alan',\n",
       " 'alan tate',\n",
       " 'alarming',\n",
       " 'alarming rise',\n",
       " 'alas',\n",
       " 'alas depression',\n",
       " 'albany',\n",
       " 'albany going',\n",
       " 'albertwenger',\n",
       " 'albertwenger deal',\n",
       " 'albino',\n",
       " 'albino structure',\n",
       " 'album',\n",
       " 'album artist',\n",
       " 'album bateman',\n",
       " 'album concerns',\n",
       " 'album flows',\n",
       " 'album forgotten',\n",
       " 'album full',\n",
       " 'album greedy',\n",
       " 'album like',\n",
       " 'album listen',\n",
       " 'album lol',\n",
       " 'album quot',\n",
       " 'album sevilla',\n",
       " 'album till',\n",
       " 'albums',\n",
       " 'albums mania',\n",
       " 'alcohol',\n",
       " 'alcohol abuse',\n",
       " 'alcohol addiction',\n",
       " 'alcohol cocaine',\n",
       " 'alcohol drug',\n",
       " 'alcohol easy',\n",
       " 'alcohol poisoning',\n",
       " 'alcoholic',\n",
       " 'alcoholic cross',\n",
       " 'alcoholic never',\n",
       " 'alcoholic ur',\n",
       " 'alcoholism',\n",
       " 'alcoholism marijuana',\n",
       " 'alcoholism potentially',\n",
       " 'aldo',\n",
       " 'aldo shoes',\n",
       " 'aldridtl',\n",
       " 'aldridtl thanks',\n",
       " 'ale',\n",
       " 'ale grillita',\n",
       " 'aleashinn',\n",
       " 'aleashinn woohoo',\n",
       " 'aleksmot',\n",
       " 'aleksmot haha',\n",
       " 'alert',\n",
       " 'alert active',\n",
       " 'alert suddenly',\n",
       " 'alert time',\n",
       " ...]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfid3.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "60138d06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb3.fit(vector3, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "b567109f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-10.89164741, -10.89164741, -10.83601501, ..., -10.85551731,\n",
       "        -11.03384898, -11.03384898]])"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb3.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "021fe3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs = pd.Series(nb3.coef_[0], index = tfid3.get_feature_names())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "49bc93c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "zzzzzzzzzz love   -11.033849\n",
       "retweet great     -11.033849\n",
       "retweet hope      -11.033849\n",
       "marathon woke     -11.033849\n",
       "edit couple       -11.033849\n",
       "                     ...    \n",
       "twitter com        -7.341041\n",
       "https              -7.334380\n",
       "twitter            -7.328687\n",
       "com                -7.174650\n",
       "depression         -6.182616\n",
       "Length: 50908, dtype: float64"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lucaspancotto/.pyenv/versions/3.8.12/envs/lewagon/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:532: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: could not convert string to float: 'manic depression jimmy tough'\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/home/lucaspancotto/.pyenv/versions/3.8.12/envs/lewagon/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:532: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: could not convert string to float: 'bhunt get tacky day'\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/home/lucaspancotto/.pyenv/versions/3.8.12/envs/lewagon/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:532: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: could not convert string to float: 'bhunt get tacky day'\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/home/lucaspancotto/.pyenv/versions/3.8.12/envs/lewagon/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:532: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: could not convert string to float: 'bhunt get tacky day'\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/home/lucaspancotto/.pyenv/versions/3.8.12/envs/lewagon/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:532: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: could not convert string to float: 'bhunt get tacky day'\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/home/lucaspancotto/.pyenv/versions/3.8.12/envs/lewagon/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:532: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: could not convert string to float: 'bhunt get tacky day'\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/home/lucaspancotto/.pyenv/versions/3.8.12/envs/lewagon/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:532: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: could not convert string to float: 'manic depression jimmy tough'\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/home/lucaspancotto/.pyenv/versions/3.8.12/envs/lewagon/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:532: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: could not convert string to float: 'bhunt get tacky day'\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/home/lucaspancotto/.pyenv/versions/3.8.12/envs/lewagon/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:532: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: could not convert string to float: 'bhunt get tacky day'\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/home/lucaspancotto/.pyenv/versions/3.8.12/envs/lewagon/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:532: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: could not convert string to float: 'bhunt get tacky day'\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n"
     ]
    }
   ],
   "source": [
    "coefs.sort_values()\n",
    "\n",
    "#agregar palabras a las stopwords , por ejemplo 'twitter', 'com'\n",
    "#agregar precision, accuracy, recall , f1 , etc\n",
    "#min, max \n",
    "#max_features:overfitting? ~ regularizacion\n",
    "\n",
    "#to do: traduccion?\n",
    "#red de deeplearning para buscar mejor score\n",
    "#testeos\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd61f83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
