{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p8H2QUYgRpCH",
        "outputId": "295322f3-3a88-451f-b21a-ce81498eef52"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import sklearn.metrics\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import SVC\n",
        "from nltk.tokenize import word_tokenize\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.metrics import classification_report\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "import re\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load the Data from .csv Source and display it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gzC6QIG0pTS_",
        "outputId": "e110f1d7-b8b6-4a5b-f008-2634c53c3a0c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(107, 12)"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = pd.read_csv('train_split_Depression_AVEC2017.csv',delimiter=',',encoding='utf-8')\n",
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HjBuVaJ6qQkh",
        "outputId": "4ab18350-1254-4a50-fac7-39485f760ad8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(47, 2)"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df2 = pd.read_csv('test_split_Depression_AVEC2017.csv',delimiter=',',encoding='utf-8')\n",
        "df2.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yc9LEqPyqQYl",
        "outputId": "bfa43ac3-bd1a-4241-e469-051dcdd221aa"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(35, 12)"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df3 = pd.read_csv('dev_split_Depression_AVEC2017.csv',delimiter=',',encoding='utf-8')\n",
        "df3.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "fM3BUGKD8DhG",
        "outputId": "264a1760-dfa6-45fc-f08b-1bdd2e1f8e57"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-ade8104a-53f3-4cb0-8eff-bcb939c23f55\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Participant_ID</th>\n",
              "      <th>PHQ_Binary</th>\n",
              "      <th>PHQ_Score</th>\n",
              "      <th>Gender</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>300</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>301</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>306</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>308</td>\n",
              "      <td>1</td>\n",
              "      <td>22</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>309</td>\n",
              "      <td>1</td>\n",
              "      <td>15</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ade8104a-53f3-4cb0-8eff-bcb939c23f55')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ade8104a-53f3-4cb0-8eff-bcb939c23f55 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ade8104a-53f3-4cb0-8eff-bcb939c23f55');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "   Participant_ID  PHQ_Binary  PHQ_Score  Gender\n",
              "0             300           0          2       1\n",
              "1             301           0          3       1\n",
              "2             306           0          0       0\n",
              "3             308           1         22       0\n",
              "4             309           1         15       1"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df3 = pd.read_csv('full_test_split.csv',delimiter=',',encoding='utf-8')\n",
        "df3.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#Preparing Data\n",
        "we will get rid of unused columns which are irrelevant for this project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "B0twETvVR25Y"
      },
      "outputs": [],
      "source": [
        "dataset1 = np.array(pd.read_csv('dev_split_Depression_AVEC2017.csv',delimiter=',',encoding='utf-8'))[:, 0:2]\n",
        "dataset2 = np.array(pd.read_csv('full_test_split.csv',delimiter=',',encoding='utf-8'))[:, 0:2]\n",
        "dataset3 = np.array(pd.read_csv('train_split_Depression_AVEC2017.csv',delimiter=',',encoding='utf-8'))[:, 0:2]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "AXGRjaYFSdbc"
      },
      "outputs": [],
      "source": [
        "dataset = np.concatenate((dataset1, np.concatenate((dataset2, dataset3))))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "vZQsOEuzTP30"
      },
      "outputs": [],
      "source": [
        "pos = []\n",
        "neg = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "NrKSeu9aTQJA"
      },
      "outputs": [],
      "source": [
        "def checkPosNeg(dataset, index):\n",
        "    for i in range(0, len(dataset)):\n",
        "        if(dataset[i][0] == index):\n",
        "            return dataset[i][1]\n",
        "    return 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "HXArzf0QTXib"
      },
      "outputs": [],
      "source": [
        "for i in range(len(dataset)):\n",
        "        if(dataset[i][1] == 1):\n",
        "            neg.append(dataset[i][0])\n",
        "        else:\n",
        "            pos.append(dataset[i][0])\n",
        "            \n",
        "pos = np.array(pos)\n",
        "neg = np.array(neg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xO5ptzo-8JDS",
        "outputId": "f0d11d31-92f9-4796-b99e-d20cb09c14b8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((133,), (56,))"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pos.shape, neg.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We import all the transcripts and append the PHQ score as our Y, and the text in a list called Data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "7pcOJtgpTjo6"
      },
      "outputs": [],
      "source": [
        "Data = []\n",
        "Y_train = []\n",
        "Data_test = []\n",
        "Y_test = []\n",
        "index = -1\n",
        "for i in range(0, len(dataset3)):\n",
        "    val = checkPosNeg(dataset, dataset3[i][0])\n",
        "    Y.append(val)\n",
        "    fileName = str(int(dataset3[i][0])) + \"_TRANSCRIPT.csv\"\n",
        "    Data.append(np.array(pd.read_csv(fileName,delimiter='\\t',encoding='utf-8'))[:, 2:4])\n",
        "\n",
        "Y_val = []\n",
        "Data_val = []\n",
        "for i in range(0, len(dataset1)):\n",
        "    val = checkPosNeg(dataset, dataset1[i][0])\n",
        "    Y_val.append(val)\n",
        "    fileName = str(int(dataset1[i][0])) + \"_TRANSCRIPT.csv\"\n",
        "    Data_val.append(np.array(pd.read_csv(fileName,delimiter='\\t',encoding='utf-8'))[:, 2:4])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Hi-FU5upzW3V"
      },
      "outputs": [],
      "source": [
        "for i in range(0, len(dataset2)):\n",
        "    Y_test.append(checkPosNeg(dataset, dataset2[i][0]))\n",
        "    fileName = str(int(dataset2[i][0])) + \"_TRANSCRIPT.csv\"\n",
        "    Data_test.append(np.array(pd.read_csv(fileName,delimiter='\\t',encoding='utf-8'))[:, 2:4])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "l7RuaP_mUBHp"
      },
      "outputs": [],
      "source": [
        "Y_train = np.array(Y_train)\n",
        "Data2 = []\n",
        "Data2_val = []\n",
        "Data2_test = []\n",
        "Y_test = np.array(Y_test)\n",
        "Y_val = np.array(Y_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sj8BgSgU0UZw",
        "outputId": "b8397b36-29ef-4520-d966-e9a2b6390612"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((array([0., 1.]), array([33, 14])),\n",
              " (array([0., 1.]), array([77, 30])),\n",
              " (array([0., 1.]), array([23, 12])))"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.unique(Y_test, return_counts = True), np.unique(Y, return_counts = True), np.unique(Y_val, return_counts = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We filter only the patient's answers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QKKKiqJPUDyu",
        "outputId": "4d4b296e-0663-4221-88a0-661e340299e4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:22: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:23: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:24: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
          ]
        }
      ],
      "source": [
        "for i in range(0, len(Data)):\n",
        "    script = []\n",
        "    for k in range(1, len(Data[i])):\n",
        "        if(Data[i][k][0] == \"Participant\"):\n",
        "            script.append(Data[i][k][1])\n",
        "    Data2.append(script)\n",
        "    \n",
        "for i in range(0, len(Data_test)):\n",
        "    script = []\n",
        "    for k in range(1, len(Data_test[i])):\n",
        "        if(Data_test[i][k][0] == \"Participant\"):\n",
        "            script.append(Data_test[i][k][1])\n",
        "    Data2_test.append(script)\n",
        "\n",
        "for i in range(0, len(Data_val)):\n",
        "    script = []\n",
        "    for k in range(1, len(Data_val[i])):\n",
        "        if(Data_val[i][k][0] == \"Participant\"):\n",
        "            script.append(Data_val[i][k][1])\n",
        "    Data2_val.append(script)\n",
        "        \n",
        "Data2 = np.array(Data2)\n",
        "Data2_test = np.array(Data2_test)\n",
        "Data2_val = np.array(Data2_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZ-TDyt6L6j5"
      },
      "source": [
        "Text Cleaning\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "Wh_X18PCMFk8"
      },
      "outputs": [],
      "source": [
        "def stop_words_and_lemmatizer(text):\n",
        "    \"\"\" Remove Stop words from text \"\"\"\n",
        "    stopwords = nltk.corpus.stopwords.words('english')\n",
        "    stopwords.append('m')\n",
        "    stopwords.append('yes')\n",
        "    stopwords.append('uh')\n",
        "    stopwords.append('eh')\n",
        "    \n",
        "    stop_words = set(stopwords)\n",
        "    \n",
        "    word_tokens = nltk.word_tokenize(text) \n",
        "    \n",
        "    without_stopwords = [w for w in word_tokens if not w in stop_words]\n",
        "    \n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    \n",
        "    lemmatized = [lemmatizer.lemmatize(word) for word in without_stopwords]\n",
        "    \n",
        "    return ' '.join(lemmatized)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "tSvaqPw9MPh2"
      },
      "outputs": [],
      "source": [
        "def to_lower(text):\n",
        "    return text.lower()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "uoK9jOc1NEFV"
      },
      "outputs": [],
      "source": [
        "def remove_context_symbol(text):\n",
        "    import re\n",
        "    return re.sub('<[^>]+>', '', text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "EYJj9LoxMQc8"
      },
      "outputs": [],
      "source": [
        "def remove_bad_symbols(text):\n",
        "    \"\"\"Remove unwanted symbols from text\"\"\"\n",
        "    bad_symbols = re.compile('[^0-9a-z #+_]')\n",
        "    return bad_symbols.sub(' ', text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "eOfqvjuVMUtH"
      },
      "outputs": [],
      "source": [
        "def remove_punctuation(text):\n",
        "    for punctuation in string.punctuation:\n",
        "        text = text.replace(punctuation, '')  \n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "FzP2S7gcMY1q"
      },
      "outputs": [],
      "source": [
        "def expand_contractions(text):\n",
        "    \"\"\" Replace contractions in the english language by the complete phrase\"\"\"\n",
        "    # Contraction list\n",
        "    contractions = {\n",
        "      \"ain't\": \"am not\",\n",
        "      \"aren't\": \"are not\",\n",
        "      \"can't\": \"cannot\",\n",
        "      \"can't've\": \"cannot have\",\n",
        "      \"'cause\": \"because\",\n",
        "      \"could've\": \"could have\",\n",
        "      \"couldn't\": \"could not\",\n",
        "      \"couldn't've\": \"could not have\",\n",
        "      \"didn't\": \"did not\",\n",
        "      \"doesn't\": \"does not\",\n",
        "      \"don't\": \"do not\",\n",
        "      \"hadn't\": \"had not\",\n",
        "      \"hadn't've\": \"had not have\",\n",
        "      \"hasn't\": \"has not\",\n",
        "      \"haven't\": \"have not\",\n",
        "      \"he'd\": \"he would\",\n",
        "      \"he'd've\": \"he would have\",\n",
        "      \"he'll\": \"he will\",\n",
        "      \"he'll've\": \"he will have\",\n",
        "      \"he's\": \"he is\",\n",
        "      \"how'd\": \"how did\",\n",
        "      \"how'd'y\": \"how do you\",\n",
        "      \"how'll\": \"how will\",\n",
        "      \"how's\": \"how is\",\n",
        "      \"I'd\": \"I would\",\n",
        "      \"I'd've\": \"I would have\",\n",
        "      \"I'll\": \"I will\",\n",
        "      \"I'll've\": \"I will have\",\n",
        "      \"I'm\": \"I am\",\n",
        "      \"I've\": \"I have\",\n",
        "      \"isn't\": \"is not\",\n",
        "      \"it'd\": \"it had\",\n",
        "      \"it'd've\": \"it would have\",\n",
        "      \"it'll\": \"it will\",\n",
        "      \"it'll've\": \"it will have\",\n",
        "      \"it's\": \"it is\",\n",
        "      \"let's\": \"let us\",\n",
        "      \"ma'am\": \"madam\",\n",
        "      \"mayn't\": \"may not\",\n",
        "      \"might've\": \"might have\",\n",
        "      \"mightn't\": \"might not\",\n",
        "      \"mightn't've\": \"might not have\",\n",
        "      \"must've\": \"must have\",\n",
        "      \"mustn't\": \"must not\",\n",
        "      \"mustn't've\": \"must not have\",\n",
        "      \"needn't\": \"need not\",\n",
        "      \"needn't've\": \"need not have\",\n",
        "      \"o'clock\": \"of the clock\",\n",
        "      \"oughtn't\": \"ought not\",\n",
        "      \"oughtn't've\": \"ought not have\",\n",
        "      \"shan't\": \"shall not\",\n",
        "      \"sha'n't\": \"shall not\",\n",
        "      \"shan't've\": \"shall not have\",\n",
        "      \"she'd\": \"she would\",\n",
        "      \"she'd've\": \"she would have\",\n",
        "      \"she'll\": \"she will\",\n",
        "      \"she'll've\": \"she will have\",\n",
        "      \"she's\": \"she is\",\n",
        "      \"should've\": \"should have\",\n",
        "      \"shouldn't\": \"should not\",\n",
        "      \"shouldn't've\": \"should not have\",\n",
        "      \"so've\": \"so have\",\n",
        "      \"so's\": \"so is\",\n",
        "      \"that'd\": \"that would\",\n",
        "      \"that'd've\": \"that would have\",\n",
        "      \"that's\": \"that is\",\n",
        "      \"there'd\": \"there had\",\n",
        "      \"there'd've\": \"there would have\",\n",
        "      \"there's\": \"there is\",\n",
        "      \"they'd\": \"they would\",\n",
        "      \"they'd've\": \"they would have\",\n",
        "      \"they'll\": \"they will\",\n",
        "      \"they'll've\": \"they will have\",\n",
        "      \"they're\": \"they are\",\n",
        "      \"they've\": \"they have\",\n",
        "      \"to've\": \"to have\",\n",
        "      \"wasn't\": \"was not\",\n",
        "      \"we'd\": \"we had\",\n",
        "      \"we'd've\": \"we would have\",\n",
        "      \"we'll\": \"we will\",\n",
        "      \"we'll've\": \"we will have\",\n",
        "      \"we're\": \"we are\",\n",
        "      \"we've\": \"we have\",\n",
        "      \"weren't\": \"were not\",\n",
        "      \"what'll\": \"what will\",\n",
        "      \"what'll've\": \"what will have\",\n",
        "      \"what're\": \"what are\",\n",
        "      \"what's\": \"what is\",\n",
        "      \"what've\": \"what have\",\n",
        "      \"when's\": \"when is\",\n",
        "      \"when've\": \"when have\",\n",
        "      \"where'd\": \"where did\",\n",
        "      \"where's\": \"where is\",\n",
        "      \"where've\": \"where have\",\n",
        "      \"who'll\": \"who will\",\n",
        "      \"who'll've\": \"who will have\",\n",
        "      \"who's\": \"who is\",\n",
        "      \"who've\": \"who have\",\n",
        "      \"why's\": \"why is\",\n",
        "      \"why've\": \"why have\",\n",
        "      \"will've\": \"will have\",\n",
        "      \"won't\": \"will not\",\n",
        "      \"won't've\": \"will not have\",\n",
        "      \"would've\": \"would have\",\n",
        "      \"wouldn't\": \"would not\",\n",
        "      \"wouldn't've\": \"would not have\",\n",
        "      \"y'all\": \"you all\",\n",
        "      \"y'alls\": \"you alls\",\n",
        "      \"y'all'd\": \"you all would\",\n",
        "      \"y'all'd've\": \"you all would have\",\n",
        "      \"y'all're\": \"you all are\",\n",
        "      \"y'all've\": \"you all have\",\n",
        "      \"you'd\": \"you had\",\n",
        "      \"you'd've\": \"you would have\",\n",
        "      \"you'll\": \"you will\",\n",
        "      \"you'll've\": \"you will have\",\n",
        "      \"you're\": \"you are\",\n",
        "      \"you've\": \"you have\"}\n",
        "    \n",
        "    contractions = dict((k.lower(), v.lower()) for k,v in contractions.items())\n",
        "\n",
        "    c_re = re.compile('(%s)' % '|'.join(contractions.keys()))\n",
        "    \n",
        "    def replace(match):\n",
        "        return contractions[match.group(0)]\n",
        "    return c_re.sub(replace, text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "Q7jI6FJA3usq"
      },
      "outputs": [],
      "source": [
        "import string \n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "for n in range(len(Data2)):\n",
        "  Data2[n] = to_lower(str(Data2[n]))\n",
        "  Data2[n] = expand_contractions(str(Data2[n]))\n",
        "  Data2[n] = remove_context_symbol(str(Data2[n]))\n",
        "  Data2[n] = remove_bad_symbols(str(Data2[n]))\n",
        "  Data2[n] = remove_punctuation(str(Data2[n]))\n",
        "  Data2[n] = stop_words_and_lemmatizer(str(Data2[n]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "3cLnxmneGPmM"
      },
      "outputs": [],
      "source": [
        "for n in range(len(Data2_test)):\n",
        "  Data2_test[n] = to_lower(str(Data2_test[n]))\n",
        "  Data2_test[n] = expand_contractions(str(Data2_test[n]))\n",
        "  Data2_test[n] = remove_context_symbol(str(Data2_test[n]))\n",
        "  Data2_test[n] = remove_bad_symbols(str(Data2_test[n]))\n",
        "  Data2_test[n] = remove_punctuation(str(Data2_test[n]))\n",
        "  Data2_test[n] = stop_words_and_lemmatizer(str(Data2_test[n]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "DmkORujo_6mo"
      },
      "outputs": [],
      "source": [
        "for n in range(len(Data2_val)):\n",
        "  Data2_val[n] = to_lower(str(Data2_val[n]))\n",
        "  Data2_val[n] = expand_contractions(str(Data2_val[n]))\n",
        "  Data2_val[n] = remove_context_symbol(str(Data2_val[n]))\n",
        "  Data2_val[n] = remove_bad_symbols(str(Data2_val[n]))\n",
        "  Data2_val[n] = remove_punctuation(str(Data2_val[n]))\n",
        "  Data2_val[n] = stop_words_and_lemmatizer(str(Data2_val[n]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "mYHo3vTxR9rq"
      },
      "outputs": [],
      "source": [
        "X_test = Data2_test\n",
        "X_train = Data2\n",
        "X_val = Data2_val"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVyZzLFMNxJq"
      },
      "source": [
        "Encoding the target variable using Label Encoder from the sklearn library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "omZdrZyo7eoN"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "lb=LabelEncoder()\n",
        "Y_train = lb.fit_transform(Y_train)\n",
        "Y_val = lb.transform(Y_val)\n",
        "Y_test = lb.transform(Y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Tokenizing and converting the reviews into numerical vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "i5LFgqNK7yLt"
      },
      "outputs": [],
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "tokenizer = Tokenizer(num_words=500, split=' ') \n",
        "\n",
        "\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "X_train = tokenizer.texts_to_sequences(X_train)\n",
        "X_train = pad_sequences(X_train, value=-1000, dtype='float64')\n",
        "\n",
        "\n",
        "X_test = tokenizer.texts_to_sequences(X_test)\n",
        "X_test = pad_sequences(X_test, maxlen = X_train.shape[1], value=-1000, dtype='float64')\n",
        "\n",
        "\n",
        "\n",
        "X_val = tokenizer.texts_to_sequences(X_val)\n",
        "X_val = pad_sequences(X_val, maxlen = X_train.shape[1], value=-1000, dtype='float64')\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "tDUpvL0Mdx7A"
      },
      "outputs": [],
      "source": [
        "X_train = np.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1]))\n",
        "X_test = np.reshape(X_test, (X_test.shape[0], 1, X_test.shape[1]))\n",
        "X_val = np.reshape(X_val, (X_val.shape[0], 1, X_val.shape[1]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_isIyLdTyicL",
        "outputId": "df00d135-21d7-4fee-c949-f97f8a11caf4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total words 5175\n"
          ]
        }
      ],
      "source": [
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print(\"Total words\", vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UYQhJ1wUMmbQ",
        "outputId": "770c2c6b-d198-4927-f559-9edcca0c3745"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1, 107, 1, 1348)"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# X_experimental = np.expand_dims(X ,axis = 0)\n",
        "\n",
        "# X_experimental.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RVkQgatAcygC",
        "outputId": "6952b265-a271-4ab6-ceff-24cf6069310f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
              "       1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0,\n",
              "       0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1,\n",
              "       1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,\n",
              "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Y_train = Y_train.astype(int)\n",
        "Y_train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Building the GRU model using the Keras library. This step involves model initialization, adding required GRU layers, and model compilation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yjp3TeJx43dk",
        "outputId": "e1fa0854-67b3-4452-e757-a06f267057f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "4/4 [==============================] - 55s 4s/step - loss: 0.6808 - accuracy: 0.6449 - auc: 0.4710 - precision: 0.3333 - recall: 0.2667 - val_loss: 0.6489 - val_accuracy: 0.6571 - val_auc: 0.7391 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 2/100\n",
            "4/4 [==============================] - 0s 69ms/step - loss: 0.6101 - accuracy: 0.7290 - auc: 0.5141 - precision: 1.0000 - recall: 0.0333 - val_loss: 0.6688 - val_accuracy: 0.6571 - val_auc: 0.7283 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 3/100\n",
            "4/4 [==============================] - 0s 73ms/step - loss: 0.5996 - accuracy: 0.7290 - auc: 0.4543 - precision: 1.0000 - recall: 0.0333 - val_loss: 0.6908 - val_accuracy: 0.6571 - val_auc: 0.5199 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 4/100\n",
            "4/4 [==============================] - 0s 67ms/step - loss: 0.5975 - accuracy: 0.7290 - auc: 0.3913 - precision: 1.0000 - recall: 0.0333 - val_loss: 0.6681 - val_accuracy: 0.6571 - val_auc: 0.5000 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 5/100\n",
            "4/4 [==============================] - 0s 70ms/step - loss: 0.5837 - accuracy: 0.7290 - auc: 0.5792 - precision: 1.0000 - recall: 0.0333 - val_loss: 0.6527 - val_accuracy: 0.6571 - val_auc: 0.5000 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 6/100\n",
            "4/4 [==============================] - 0s 66ms/step - loss: 0.5850 - accuracy: 0.7290 - auc: 0.5236 - precision: 1.0000 - recall: 0.0333 - val_loss: 0.6464 - val_accuracy: 0.6571 - val_auc: 0.5000 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 7/100\n",
            "4/4 [==============================] - 0s 79ms/step - loss: 0.5872 - accuracy: 0.7290 - auc: 0.5067 - precision: 1.0000 - recall: 0.0333 - val_loss: 0.6473 - val_accuracy: 0.6571 - val_auc: 0.5000 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 8/100\n",
            "4/4 [==============================] - 1s 136ms/step - loss: 0.5861 - accuracy: 0.7290 - auc: 0.4695 - precision: 1.0000 - recall: 0.0333 - val_loss: 0.6542 - val_accuracy: 0.6571 - val_auc: 0.5000 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 9/100\n",
            "4/4 [==============================] - 1s 158ms/step - loss: 0.5840 - accuracy: 0.7290 - auc: 0.5364 - precision: 1.0000 - recall: 0.0333 - val_loss: 0.6584 - val_accuracy: 0.6571 - val_auc: 0.5000 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 10/100\n",
            "4/4 [==============================] - 1s 166ms/step - loss: 0.5837 - accuracy: 0.7290 - auc: 0.5400 - precision: 1.0000 - recall: 0.0333 - val_loss: 0.6630 - val_accuracy: 0.6571 - val_auc: 0.5435 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 11/100\n",
            "4/4 [==============================] - 1s 152ms/step - loss: 0.5834 - accuracy: 0.7290 - auc: 0.6736 - precision: 1.0000 - recall: 0.0333 - val_loss: 0.6643 - val_accuracy: 0.6571 - val_auc: 0.5344 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 12/100\n",
            "4/4 [==============================] - 1s 197ms/step - loss: 0.5846 - accuracy: 0.7290 - auc: 0.5604 - precision: 1.0000 - recall: 0.0333 - val_loss: 0.6644 - val_accuracy: 0.6571 - val_auc: 0.4348 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 13/100\n",
            "4/4 [==============================] - 1s 162ms/step - loss: 0.5823 - accuracy: 0.7290 - auc: 0.6236 - precision: 1.0000 - recall: 0.0333 - val_loss: 0.6575 - val_accuracy: 0.6571 - val_auc: 0.4583 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 14/100\n",
            "4/4 [==============================] - 0s 128ms/step - loss: 0.5712 - accuracy: 0.7290 - auc: 0.6829 - precision: 1.0000 - recall: 0.0333 - val_loss: 0.6532 - val_accuracy: 0.6571 - val_auc: 0.4891 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 15/100\n",
            "4/4 [==============================] - 1s 134ms/step - loss: 0.5698 - accuracy: 0.7290 - auc: 0.6621 - precision: 1.0000 - recall: 0.0333 - val_loss: 0.6460 - val_accuracy: 0.6571 - val_auc: 0.5018 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 16/100\n",
            "4/4 [==============================] - 1s 134ms/step - loss: 0.5653 - accuracy: 0.7196 - auc: 0.6994 - precision: 0.5000 - recall: 0.2333 - val_loss: 0.6640 - val_accuracy: 0.6571 - val_auc: 0.5036 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 17/100\n",
            "4/4 [==============================] - 1s 147ms/step - loss: 0.5815 - accuracy: 0.7290 - auc: 0.6400 - precision: 1.0000 - recall: 0.0333 - val_loss: 0.7013 - val_accuracy: 0.6571 - val_auc: 0.4656 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 18/100\n",
            "4/4 [==============================] - 1s 136ms/step - loss: 0.5949 - accuracy: 0.7290 - auc: 0.5827 - precision: 1.0000 - recall: 0.0333 - val_loss: 0.6811 - val_accuracy: 0.6571 - val_auc: 0.4891 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 19/100\n",
            "4/4 [==============================] - 1s 167ms/step - loss: 0.5832 - accuracy: 0.7290 - auc: 0.6861 - precision: 1.0000 - recall: 0.0333 - val_loss: 0.6653 - val_accuracy: 0.6571 - val_auc: 0.4946 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 20/100\n",
            "4/4 [==============================] - 1s 149ms/step - loss: 0.5772 - accuracy: 0.7290 - auc: 0.6773 - precision: 1.0000 - recall: 0.0333 - val_loss: 0.6518 - val_accuracy: 0.6571 - val_auc: 0.5399 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 21/100\n",
            "4/4 [==============================] - 1s 137ms/step - loss: 0.5718 - accuracy: 0.7290 - auc: 0.7225 - precision: 1.0000 - recall: 0.0333 - val_loss: 0.6451 - val_accuracy: 0.6571 - val_auc: 0.5797 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 22/100\n",
            "4/4 [==============================] - 1s 161ms/step - loss: 0.5706 - accuracy: 0.7290 - auc: 0.6970 - precision: 1.0000 - recall: 0.0333 - val_loss: 0.6375 - val_accuracy: 0.6571 - val_auc: 0.6123 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 23/100\n",
            "4/4 [==============================] - 1s 153ms/step - loss: 0.5570 - accuracy: 0.7290 - auc: 0.7175 - precision: 1.0000 - recall: 0.0333 - val_loss: 0.6379 - val_accuracy: 0.6571 - val_auc: 0.5942 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 24/100\n",
            "4/4 [==============================] - 1s 176ms/step - loss: 0.5411 - accuracy: 0.7664 - auc: 0.7368 - precision: 0.8571 - recall: 0.2000 - val_loss: 0.6312 - val_accuracy: 0.6571 - val_auc: 0.5924 - val_precision: 0.5000 - val_recall: 0.0833\n",
            "Epoch 25/100\n",
            "4/4 [==============================] - 1s 144ms/step - loss: 0.5410 - accuracy: 0.7664 - auc: 0.6963 - precision: 0.7273 - recall: 0.2667 - val_loss: 0.6610 - val_accuracy: 0.6571 - val_auc: 0.5199 - val_precision: 0.5000 - val_recall: 0.0833\n",
            "Epoch 26/100\n",
            "4/4 [==============================] - 1s 161ms/step - loss: 0.5521 - accuracy: 0.7103 - auc: 0.7266 - precision: 0.4737 - recall: 0.3000 - val_loss: 0.6893 - val_accuracy: 0.6857 - val_auc: 0.4746 - val_precision: 0.6000 - val_recall: 0.2500\n",
            "Epoch 27/100\n",
            "4/4 [==============================] - 1s 134ms/step - loss: 0.5365 - accuracy: 0.7570 - auc: 0.7325 - precision: 0.7500 - recall: 0.2000 - val_loss: 0.7021 - val_accuracy: 0.6571 - val_auc: 0.4221 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 28/100\n",
            "4/4 [==============================] - 0s 115ms/step - loss: 0.4933 - accuracy: 0.7570 - auc: 0.8045 - precision: 0.7000 - recall: 0.2333 - val_loss: 0.7781 - val_accuracy: 0.4571 - val_auc: 0.4565 - val_precision: 0.2941 - val_recall: 0.4167\n",
            "Epoch 29/100\n",
            "4/4 [==============================] - 0s 110ms/step - loss: 0.5028 - accuracy: 0.7850 - auc: 0.7621 - precision: 0.6842 - recall: 0.4333 - val_loss: 0.7359 - val_accuracy: 0.6571 - val_auc: 0.4348 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 30/100\n",
            "4/4 [==============================] - 0s 130ms/step - loss: 0.5154 - accuracy: 0.7664 - auc: 0.7957 - precision: 1.0000 - recall: 0.1667 - val_loss: 0.7190 - val_accuracy: 0.6286 - val_auc: 0.4583 - val_precision: 0.3333 - val_recall: 0.0833\n",
            "Epoch 31/100\n",
            "4/4 [==============================] - 1s 159ms/step - loss: 0.4817 - accuracy: 0.7664 - auc: 0.7957 - precision: 0.6667 - recall: 0.3333 - val_loss: 0.7585 - val_accuracy: 0.6286 - val_auc: 0.4293 - val_precision: 0.3333 - val_recall: 0.0833\n",
            "Epoch 32/100\n",
            "4/4 [==============================] - 0s 119ms/step - loss: 0.4717 - accuracy: 0.7757 - auc: 0.8000 - precision: 0.8000 - recall: 0.2667 - val_loss: 0.7882 - val_accuracy: 0.6857 - val_auc: 0.3569 - val_precision: 1.0000 - val_recall: 0.0833\n",
            "Epoch 33/100\n",
            "4/4 [==============================] - 1s 145ms/step - loss: 0.4742 - accuracy: 0.7477 - auc: 0.8294 - precision: 0.5455 - recall: 0.6000 - val_loss: 0.8452 - val_accuracy: 0.4857 - val_auc: 0.3605 - val_precision: 0.1250 - val_recall: 0.0833\n",
            "Epoch 34/100\n",
            "4/4 [==============================] - 1s 137ms/step - loss: 0.4593 - accuracy: 0.7477 - auc: 0.8173 - precision: 0.5789 - recall: 0.3667 - val_loss: 0.8347 - val_accuracy: 0.5714 - val_auc: 0.3659 - val_precision: 0.2000 - val_recall: 0.0833\n",
            "Epoch 35/100\n",
            "4/4 [==============================] - 0s 119ms/step - loss: 0.4341 - accuracy: 0.7850 - auc: 0.8271 - precision: 0.6667 - recall: 0.4667 - val_loss: 0.9087 - val_accuracy: 0.5714 - val_auc: 0.3822 - val_precision: 0.2857 - val_recall: 0.1667\n",
            "Epoch 36/100\n",
            "4/4 [==============================] - 1s 155ms/step - loss: 0.4205 - accuracy: 0.7944 - auc: 0.8394 - precision: 0.6667 - recall: 0.5333 - val_loss: 0.9172 - val_accuracy: 0.6000 - val_auc: 0.3406 - val_precision: 0.2500 - val_recall: 0.0833\n",
            "Epoch 37/100\n",
            "4/4 [==============================] - 0s 130ms/step - loss: 0.4241 - accuracy: 0.8131 - auc: 0.8461 - precision: 0.6786 - recall: 0.6333 - val_loss: 0.9119 - val_accuracy: 0.6000 - val_auc: 0.3188 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 38/100\n",
            "4/4 [==============================] - 0s 123ms/step - loss: 0.5387 - accuracy: 0.7383 - auc: 0.7939 - precision: 0.7500 - recall: 0.1000 - val_loss: 0.9258 - val_accuracy: 0.4857 - val_auc: 0.3388 - val_precision: 0.1250 - val_recall: 0.0833\n",
            "Epoch 39/100\n",
            "4/4 [==============================] - 1s 137ms/step - loss: 0.5262 - accuracy: 0.6542 - auc: 0.8065 - precision: 0.4340 - recall: 0.7667 - val_loss: 0.8129 - val_accuracy: 0.6571 - val_auc: 0.3460 - val_precision: 0.5000 - val_recall: 0.0833\n",
            "Epoch 40/100\n",
            "4/4 [==============================] - 1s 145ms/step - loss: 0.4591 - accuracy: 0.7850 - auc: 0.8141 - precision: 0.8889 - recall: 0.2667 - val_loss: 0.7950 - val_accuracy: 0.6571 - val_auc: 0.3478 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 41/100\n",
            "4/4 [==============================] - 0s 120ms/step - loss: 0.4828 - accuracy: 0.7570 - auc: 0.8712 - precision: 1.0000 - recall: 0.1333 - val_loss: 0.7825 - val_accuracy: 0.6571 - val_auc: 0.3659 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 42/100\n",
            "4/4 [==============================] - 1s 154ms/step - loss: 0.4674 - accuracy: 0.7664 - auc: 0.8740 - precision: 1.0000 - recall: 0.1667 - val_loss: 0.7918 - val_accuracy: 0.6571 - val_auc: 0.3533 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 43/100\n",
            "4/4 [==============================] - 0s 119ms/step - loss: 0.4563 - accuracy: 0.7850 - auc: 0.8433 - precision: 1.0000 - recall: 0.2333 - val_loss: 0.8298 - val_accuracy: 0.6571 - val_auc: 0.3551 - val_precision: 0.5000 - val_recall: 0.0833\n",
            "Epoch 44/100\n",
            "4/4 [==============================] - 0s 113ms/step - loss: 0.4315 - accuracy: 0.7850 - auc: 0.8463 - precision: 0.8182 - recall: 0.3000 - val_loss: 0.9040 - val_accuracy: 0.6571 - val_auc: 0.3460 - val_precision: 0.5000 - val_recall: 0.0833\n",
            "Epoch 45/100\n",
            "4/4 [==============================] - 0s 121ms/step - loss: 0.4322 - accuracy: 0.7944 - auc: 0.8355 - precision: 0.7857 - recall: 0.3667 - val_loss: 0.9438 - val_accuracy: 0.6286 - val_auc: 0.3170 - val_precision: 0.3333 - val_recall: 0.0833\n",
            "Epoch 46/100\n",
            "4/4 [==============================] - 0s 102ms/step - loss: 0.3986 - accuracy: 0.8131 - auc: 0.8764 - precision: 0.7500 - recall: 0.5000 - val_loss: 0.9952 - val_accuracy: 0.4571 - val_auc: 0.3424 - val_precision: 0.1111 - val_recall: 0.0833\n",
            "Epoch 47/100\n",
            "4/4 [==============================] - 0s 107ms/step - loss: 0.4143 - accuracy: 0.8131 - auc: 0.8491 - precision: 0.6667 - recall: 0.6667 - val_loss: 1.0299 - val_accuracy: 0.5429 - val_auc: 0.3768 - val_precision: 0.2500 - val_recall: 0.1667\n",
            "Epoch 48/100\n",
            "4/4 [==============================] - 0s 74ms/step - loss: 0.4039 - accuracy: 0.8131 - auc: 0.8532 - precision: 0.6923 - recall: 0.6000 - val_loss: 1.0588 - val_accuracy: 0.6571 - val_auc: 0.3442 - val_precision: 0.5000 - val_recall: 0.0833\n",
            "Epoch 49/100\n",
            "4/4 [==============================] - 0s 129ms/step - loss: 0.4299 - accuracy: 0.7757 - auc: 0.8494 - precision: 0.6250 - recall: 0.5000 - val_loss: 1.0679 - val_accuracy: 0.5714 - val_auc: 0.3043 - val_precision: 0.2000 - val_recall: 0.0833\n",
            "Epoch 50/100\n",
            "4/4 [==============================] - 0s 121ms/step - loss: 0.3780 - accuracy: 0.8505 - auc: 0.8868 - precision: 0.7500 - recall: 0.7000 - val_loss: 1.1003 - val_accuracy: 0.4286 - val_auc: 0.2880 - val_precision: 0.2500 - val_recall: 0.3333\n",
            "Epoch 51/100\n",
            "4/4 [==============================] - 0s 113ms/step - loss: 0.4074 - accuracy: 0.7944 - auc: 0.8816 - precision: 0.6000 - recall: 0.8000 - val_loss: 1.0481 - val_accuracy: 0.5714 - val_auc: 0.3116 - val_precision: 0.2000 - val_recall: 0.0833\n",
            "Epoch 52/100\n",
            "4/4 [==============================] - 0s 104ms/step - loss: 0.3738 - accuracy: 0.8318 - auc: 0.8868 - precision: 0.7500 - recall: 0.6000 - val_loss: 1.0540 - val_accuracy: 0.5714 - val_auc: 0.3261 - val_precision: 0.2000 - val_recall: 0.0833\n",
            "Epoch 53/100\n",
            "4/4 [==============================] - 0s 131ms/step - loss: 0.3660 - accuracy: 0.8411 - auc: 0.8965 - precision: 0.7241 - recall: 0.7000 - val_loss: 1.0892 - val_accuracy: 0.5714 - val_auc: 0.3062 - val_precision: 0.2857 - val_recall: 0.1667\n",
            "Epoch 54/100\n",
            "4/4 [==============================] - 0s 108ms/step - loss: 0.3533 - accuracy: 0.8411 - auc: 0.8963 - precision: 0.6970 - recall: 0.7667 - val_loss: 1.0607 - val_accuracy: 0.6286 - val_auc: 0.2699 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 55/100\n",
            "4/4 [==============================] - 0s 109ms/step - loss: 0.3949 - accuracy: 0.7944 - auc: 0.8623 - precision: 0.6538 - recall: 0.5667 - val_loss: 1.1170 - val_accuracy: 0.5429 - val_auc: 0.2899 - val_precision: 0.1667 - val_recall: 0.0833\n",
            "Epoch 56/100\n",
            "4/4 [==============================] - 0s 100ms/step - loss: 0.3494 - accuracy: 0.8318 - auc: 0.9078 - precision: 0.8000 - recall: 0.5333 - val_loss: 1.1604 - val_accuracy: 0.4857 - val_auc: 0.2935 - val_precision: 0.1250 - val_recall: 0.0833\n",
            "Epoch 57/100\n",
            "4/4 [==============================] - 0s 117ms/step - loss: 0.3345 - accuracy: 0.8598 - auc: 0.8989 - precision: 0.7273 - recall: 0.8000 - val_loss: 1.1902 - val_accuracy: 0.4571 - val_auc: 0.2899 - val_precision: 0.1111 - val_recall: 0.0833\n",
            "Epoch 58/100\n",
            "4/4 [==============================] - 0s 99ms/step - loss: 0.3277 - accuracy: 0.8411 - auc: 0.9028 - precision: 0.7097 - recall: 0.7333 - val_loss: 1.1333 - val_accuracy: 0.6571 - val_auc: 0.2717 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 59/100\n",
            "4/4 [==============================] - 0s 112ms/step - loss: 0.3967 - accuracy: 0.8037 - auc: 0.8872 - precision: 0.6667 - recall: 0.6000 - val_loss: 1.2111 - val_accuracy: 0.3429 - val_auc: 0.2663 - val_precision: 0.1333 - val_recall: 0.1667\n",
            "Epoch 60/100\n",
            "4/4 [==============================] - 0s 106ms/step - loss: 0.3656 - accuracy: 0.8598 - auc: 0.9035 - precision: 0.6923 - recall: 0.9000 - val_loss: 1.1467 - val_accuracy: 0.6000 - val_auc: 0.2536 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 61/100\n",
            "4/4 [==============================] - 0s 71ms/step - loss: 0.4071 - accuracy: 0.8037 - auc: 0.8652 - precision: 0.6364 - recall: 0.7000 - val_loss: 1.1901 - val_accuracy: 0.4571 - val_auc: 0.2880 - val_precision: 0.1111 - val_recall: 0.0833\n",
            "Epoch 62/100\n",
            "4/4 [==============================] - 0s 70ms/step - loss: 0.3852 - accuracy: 0.8037 - auc: 0.8864 - precision: 0.7143 - recall: 0.5000 - val_loss: 1.1529 - val_accuracy: 0.5714 - val_auc: 0.2917 - val_precision: 0.2000 - val_recall: 0.0833\n",
            "Epoch 63/100\n",
            "4/4 [==============================] - 0s 70ms/step - loss: 0.3385 - accuracy: 0.8505 - auc: 0.9078 - precision: 0.7333 - recall: 0.7333 - val_loss: 1.0916 - val_accuracy: 0.5143 - val_auc: 0.3225 - val_precision: 0.1429 - val_recall: 0.0833\n",
            "Epoch 64/100\n",
            "4/4 [==============================] - 0s 71ms/step - loss: 0.3388 - accuracy: 0.8037 - auc: 0.9035 - precision: 0.7368 - recall: 0.4667 - val_loss: 1.1368 - val_accuracy: 0.6571 - val_auc: 0.3025 - val_precision: 0.5000 - val_recall: 0.0833\n",
            "Epoch 65/100\n",
            "4/4 [==============================] - 0s 67ms/step - loss: 0.3329 - accuracy: 0.8411 - auc: 0.9104 - precision: 0.7600 - recall: 0.6333 - val_loss: 1.1423 - val_accuracy: 0.5143 - val_auc: 0.3007 - val_precision: 0.1429 - val_recall: 0.0833\n",
            "Epoch 66/100\n",
            "4/4 [==============================] - 0s 71ms/step - loss: 0.3386 - accuracy: 0.8318 - auc: 0.9149 - precision: 0.7143 - recall: 0.6667 - val_loss: 1.1627 - val_accuracy: 0.4857 - val_auc: 0.2609 - val_precision: 0.1250 - val_recall: 0.0833\n",
            "Epoch 67/100\n",
            "4/4 [==============================] - 0s 69ms/step - loss: 0.4475 - accuracy: 0.7664 - auc: 0.8649 - precision: 0.5581 - recall: 0.8000 - val_loss: 1.1192 - val_accuracy: 0.4286 - val_auc: 0.2971 - val_precision: 0.1000 - val_recall: 0.0833\n",
            "Epoch 68/100\n",
            "4/4 [==============================] - 0s 68ms/step - loss: 0.3559 - accuracy: 0.8785 - auc: 0.9037 - precision: 0.8696 - recall: 0.6667 - val_loss: 1.1512 - val_accuracy: 0.6571 - val_auc: 0.2971 - val_precision: 0.5000 - val_recall: 0.0833\n",
            "Epoch 69/100\n",
            "4/4 [==============================] - 0s 70ms/step - loss: 0.3904 - accuracy: 0.8505 - auc: 0.8835 - precision: 0.8889 - recall: 0.5333 - val_loss: 1.0046 - val_accuracy: 0.4000 - val_auc: 0.3605 - val_precision: 0.1538 - val_recall: 0.1667\n",
            "Epoch 70/100\n",
            "4/4 [==============================] - 0s 72ms/step - loss: 0.4027 - accuracy: 0.8224 - auc: 0.8963 - precision: 0.6341 - recall: 0.8667 - val_loss: 1.0733 - val_accuracy: 0.6571 - val_auc: 0.2935 - val_precision: 0.5000 - val_recall: 0.0833\n",
            "Epoch 71/100\n",
            "4/4 [==============================] - 0s 66ms/step - loss: 0.3375 - accuracy: 0.8131 - auc: 0.9234 - precision: 1.0000 - recall: 0.3333 - val_loss: 1.0950 - val_accuracy: 0.6571 - val_auc: 0.2808 - val_precision: 0.5000 - val_recall: 0.0833\n",
            "Epoch 72/100\n",
            "4/4 [==============================] - 0s 68ms/step - loss: 0.3343 - accuracy: 0.8037 - auc: 0.9210 - precision: 0.7647 - recall: 0.4333 - val_loss: 1.1340 - val_accuracy: 0.4571 - val_auc: 0.2935 - val_precision: 0.1111 - val_recall: 0.0833\n",
            "Epoch 73/100\n",
            "4/4 [==============================] - 0s 65ms/step - loss: 0.3496 - accuracy: 0.8411 - auc: 0.9186 - precision: 0.6667 - recall: 0.8667 - val_loss: 1.1988 - val_accuracy: 0.5143 - val_auc: 0.2554 - val_precision: 0.1429 - val_recall: 0.0833\n",
            "Epoch 74/100\n",
            "4/4 [==============================] - 0s 74ms/step - loss: 0.3014 - accuracy: 0.8598 - auc: 0.9236 - precision: 0.7586 - recall: 0.7333 - val_loss: 1.2455 - val_accuracy: 0.5714 - val_auc: 0.2536 - val_precision: 0.2000 - val_recall: 0.0833\n",
            "Epoch 75/100\n",
            "4/4 [==============================] - 0s 70ms/step - loss: 0.3336 - accuracy: 0.8131 - auc: 0.9063 - precision: 0.6667 - recall: 0.6667 - val_loss: 1.2952 - val_accuracy: 0.5143 - val_auc: 0.2500 - val_precision: 0.1429 - val_recall: 0.0833\n",
            "Epoch 76/100\n",
            "4/4 [==============================] - 0s 68ms/step - loss: 0.3380 - accuracy: 0.8037 - auc: 0.9195 - precision: 0.6667 - recall: 0.6000 - val_loss: 1.2958 - val_accuracy: 0.4857 - val_auc: 0.2373 - val_precision: 0.2000 - val_recall: 0.1667\n",
            "Epoch 77/100\n",
            "4/4 [==============================] - 0s 68ms/step - loss: 0.3547 - accuracy: 0.8598 - auc: 0.9281 - precision: 0.6829 - recall: 0.9333 - val_loss: 1.1444 - val_accuracy: 0.4286 - val_auc: 0.3732 - val_precision: 0.1667 - val_recall: 0.1667\n",
            "Epoch 78/100\n",
            "4/4 [==============================] - 0s 69ms/step - loss: 0.3511 - accuracy: 0.8224 - auc: 0.9043 - precision: 0.6486 - recall: 0.8000 - val_loss: 1.2876 - val_accuracy: 0.6571 - val_auc: 0.2935 - val_precision: 0.5000 - val_recall: 0.0833\n",
            "Epoch 79/100\n",
            "4/4 [==============================] - 0s 71ms/step - loss: 0.3613 - accuracy: 0.8224 - auc: 0.9227 - precision: 0.8667 - recall: 0.4333 - val_loss: 1.2458 - val_accuracy: 0.4571 - val_auc: 0.2645 - val_precision: 0.1111 - val_recall: 0.0833\n",
            "Epoch 80/100\n",
            "4/4 [==============================] - 0s 67ms/step - loss: 0.3643 - accuracy: 0.8411 - auc: 0.9253 - precision: 0.6512 - recall: 0.9333 - val_loss: 1.2420 - val_accuracy: 0.4857 - val_auc: 0.2518 - val_precision: 0.2000 - val_recall: 0.1667\n",
            "Epoch 81/100\n",
            "4/4 [==============================] - 0s 71ms/step - loss: 0.3070 - accuracy: 0.8692 - auc: 0.9377 - precision: 0.7222 - recall: 0.8667 - val_loss: 1.2852 - val_accuracy: 0.5714 - val_auc: 0.2554 - val_precision: 0.2000 - val_recall: 0.0833\n",
            "Epoch 82/100\n",
            "4/4 [==============================] - 0s 70ms/step - loss: 0.4031 - accuracy: 0.7664 - auc: 0.8963 - precision: 0.6923 - recall: 0.3000 - val_loss: 1.3181 - val_accuracy: 0.3429 - val_auc: 0.2808 - val_precision: 0.0769 - val_recall: 0.0833\n",
            "Epoch 83/100\n",
            "4/4 [==============================] - 0s 67ms/step - loss: 0.4281 - accuracy: 0.7944 - auc: 0.8976 - precision: 0.5833 - recall: 0.9333 - val_loss: 1.0916 - val_accuracy: 0.4286 - val_auc: 0.4094 - val_precision: 0.2500 - val_recall: 0.3333\n",
            "Epoch 84/100\n",
            "4/4 [==============================] - 0s 69ms/step - loss: 0.3456 - accuracy: 0.8224 - auc: 0.9154 - precision: 0.6897 - recall: 0.6667 - val_loss: 1.2507 - val_accuracy: 0.6571 - val_auc: 0.2899 - val_precision: 0.5000 - val_recall: 0.0833\n",
            "Epoch 85/100\n",
            "4/4 [==============================] - 0s 72ms/step - loss: 0.3853 - accuracy: 0.8037 - auc: 0.9338 - precision: 1.0000 - recall: 0.3000 - val_loss: 1.1925 - val_accuracy: 0.6286 - val_auc: 0.2681 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 86/100\n",
            "4/4 [==============================] - 0s 72ms/step - loss: 0.3611 - accuracy: 0.8131 - auc: 0.9141 - precision: 1.0000 - recall: 0.3333 - val_loss: 1.1118 - val_accuracy: 0.5714 - val_auc: 0.2663 - val_precision: 0.2000 - val_recall: 0.0833\n",
            "Epoch 87/100\n",
            "4/4 [==============================] - 0s 70ms/step - loss: 0.3553 - accuracy: 0.8131 - auc: 0.9216 - precision: 0.6923 - recall: 0.6000 - val_loss: 1.2527 - val_accuracy: 0.6286 - val_auc: 0.2572 - val_precision: 0.3333 - val_recall: 0.0833\n",
            "Epoch 88/100\n",
            "4/4 [==============================] - 0s 68ms/step - loss: 0.3096 - accuracy: 0.8131 - auc: 0.9314 - precision: 0.7500 - recall: 0.5000 - val_loss: 1.2805 - val_accuracy: 0.5429 - val_auc: 0.2663 - val_precision: 0.1667 - val_recall: 0.0833\n",
            "Epoch 89/100\n",
            "4/4 [==============================] - 0s 69ms/step - loss: 0.2959 - accuracy: 0.8692 - auc: 0.9385 - precision: 0.7353 - recall: 0.8333 - val_loss: 1.3363 - val_accuracy: 0.4571 - val_auc: 0.2500 - val_precision: 0.1111 - val_recall: 0.0833\n",
            "Epoch 90/100\n",
            "4/4 [==============================] - 0s 69ms/step - loss: 0.3055 - accuracy: 0.8505 - auc: 0.9338 - precision: 0.6667 - recall: 0.9333 - val_loss: 1.4035 - val_accuracy: 0.5143 - val_auc: 0.2681 - val_precision: 0.1429 - val_recall: 0.0833\n",
            "Epoch 91/100\n",
            "4/4 [==============================] - 0s 67ms/step - loss: 0.3026 - accuracy: 0.8505 - auc: 0.9316 - precision: 0.7333 - recall: 0.7333 - val_loss: 1.4409 - val_accuracy: 0.5143 - val_auc: 0.2609 - val_precision: 0.1429 - val_recall: 0.0833\n",
            "Epoch 92/100\n",
            "4/4 [==============================] - 0s 67ms/step - loss: 0.3299 - accuracy: 0.8411 - auc: 0.9247 - precision: 0.6512 - recall: 0.9333 - val_loss: 1.4452 - val_accuracy: 0.4857 - val_auc: 0.2554 - val_precision: 0.1250 - val_recall: 0.0833\n",
            "Epoch 93/100\n",
            "4/4 [==============================] - 0s 69ms/step - loss: 0.2907 - accuracy: 0.8598 - auc: 0.9396 - precision: 0.7778 - recall: 0.7000 - val_loss: 1.4156 - val_accuracy: 0.5714 - val_auc: 0.2591 - val_precision: 0.2000 - val_recall: 0.0833\n",
            "Epoch 94/100\n",
            "4/4 [==============================] - 0s 71ms/step - loss: 0.2867 - accuracy: 0.8598 - auc: 0.9374 - precision: 0.7419 - recall: 0.7667 - val_loss: 1.3911 - val_accuracy: 0.4571 - val_auc: 0.2536 - val_precision: 0.1111 - val_recall: 0.0833\n",
            "Epoch 95/100\n",
            "4/4 [==============================] - 0s 67ms/step - loss: 0.3223 - accuracy: 0.8224 - auc: 0.9368 - precision: 0.6222 - recall: 0.9333 - val_loss: 1.4162 - val_accuracy: 0.5714 - val_auc: 0.2464 - val_precision: 0.2000 - val_recall: 0.0833\n",
            "Epoch 96/100\n",
            "4/4 [==============================] - 0s 73ms/step - loss: 0.3086 - accuracy: 0.8318 - auc: 0.9346 - precision: 0.7308 - recall: 0.6333 - val_loss: 1.4091 - val_accuracy: 0.5714 - val_auc: 0.2373 - val_precision: 0.2000 - val_recall: 0.0833\n",
            "Epoch 97/100\n",
            "4/4 [==============================] - 0s 70ms/step - loss: 0.2647 - accuracy: 0.8598 - auc: 0.9394 - precision: 0.7273 - recall: 0.8000 - val_loss: 1.4085 - val_accuracy: 0.4571 - val_auc: 0.2446 - val_precision: 0.1111 - val_recall: 0.0833\n",
            "Epoch 98/100\n",
            "4/4 [==============================] - 0s 72ms/step - loss: 0.2687 - accuracy: 0.8692 - auc: 0.9422 - precision: 0.7353 - recall: 0.8333 - val_loss: 1.4348 - val_accuracy: 0.5714 - val_auc: 0.2210 - val_precision: 0.2000 - val_recall: 0.0833\n",
            "Epoch 99/100\n",
            "4/4 [==============================] - 0s 69ms/step - loss: 0.2635 - accuracy: 0.8598 - auc: 0.9422 - precision: 0.7419 - recall: 0.7667 - val_loss: 1.4910 - val_accuracy: 0.4571 - val_auc: 0.2409 - val_precision: 0.1111 - val_recall: 0.0833\n",
            "Epoch 100/100\n",
            "4/4 [==============================] - 0s 72ms/step - loss: 0.3249 - accuracy: 0.8318 - auc: 0.9457 - precision: 0.6250 - recall: 1.0000 - val_loss: 1.4849 - val_accuracy: 0.4571 - val_auc: 0.2210 - val_precision: 0.1111 - val_recall: 0.0833\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras.layers import Activation, Dense\n",
        "from keras.layers import Input\n",
        "def init_model_2():\n",
        "  model = Sequential()\n",
        "  #input = Input(shape=(142, 1353))\n",
        "  initial_learning_rate = 0.001\n",
        "  lr_schedule = ExponentialDecay(initial_learning_rate, decay_steps=2000, decay_rate=0.5)\n",
        "  adam = Adam(learning_rate=lr_schedule)\n",
        "  model.add(layers.Masking(mask_value=-1000))\n",
        "  model.add(layers.GRU(512, return_sequences=True, activation='tanh'))\n",
        "  model.add(layers.GRU(256, return_sequences=True, activation='tanh'))\n",
        "  model.add(layers.GRU(128, return_sequences=True, activation='tanh'))\n",
        "  model.add(layers.GRU(256, return_sequences=True, activation='tanh'))\n",
        "  model.add(layers.GRU(128, return_sequences=True, activation='tanh', dropout= 0.03))\n",
        "  model.add(layers.GRU(64, return_sequences=True, activation='tanh', dropout=0.02))\n",
        "  model.add(layers.GRU(32, return_sequences=True, activation='tanh',dropout=0.01))\n",
        "  model.add(layers.Flatten())\n",
        "  model.add(Dense(16, activation = 'tanh'))\n",
        "  model.add(Dense(1, activation = 'sigmoid'))\n",
        "  \n",
        "  model.compile(\n",
        "      optimizer=adam,\n",
        "      loss=\"binary_crossentropy\",\n",
        "      metrics=['accuracy', 'AUC','Precision','Recall'])\n",
        "  return model\n",
        "\n",
        "model = init_model_2()\n",
        "\n",
        "\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "es = EarlyStopping(patience=5, restore_best_weights=True)\n",
        "history_fit = model.fit(X, Y, \n",
        "          batch_size = 32,\n",
        "          epochs=100,\n",
        "          validation_data = (X_val, Y_val),\n",
        "          #callbacks=[es],\n",
        "          verbose = 1\n",
        "         )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pW2GwLe2NPZM",
        "outputId": "9e8afbca-2731-40c9-ce7a-d55cb66f425a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " masking (Masking)           (None, 1, 1348)           0         \n",
            "                                                                 \n",
            " gru (GRU)                   (None, 1, 512)            2860032   \n",
            "                                                                 \n",
            " gru_1 (GRU)                 (None, 1, 256)            591360    \n",
            "                                                                 \n",
            " gru_2 (GRU)                 (None, 1, 128)            148224    \n",
            "                                                                 \n",
            " gru_3 (GRU)                 (None, 1, 256)            296448    \n",
            "                                                                 \n",
            " gru_4 (GRU)                 (None, 1, 128)            148224    \n",
            "                                                                 \n",
            " gru_5 (GRU)                 (None, 1, 64)             37248     \n",
            "                                                                 \n",
            " gru_6 (GRU)                 (None, 1, 32)             9408      \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 32)                0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 16)                528       \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,091,489\n",
            "Trainable params: 4,091,489\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary() "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "JDu33lH7TzWp"
      },
      "outputs": [],
      "source": [
        "Y_pred = model.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "kjB0nzvmeQRu"
      },
      "outputs": [],
      "source": [
        "y_pred = [1 if value[0] >= 0.5 else 0 for value in Y_pred]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j8PTk0qUsgcF",
        "outputId": "69072832-2b1a-4dc4-fcb4-a761dd2bd2ab"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((array([0, 1]), array([44,  3])), (array([0, 1]), array([33, 14])))"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.unique(y_pred, return_counts = True), np.unique(Y_test, return_counts = True) #array([25081,   277]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "JZdduN-Qhrzt"
      },
      "outputs": [],
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# val_acc = history_fit.history['val_accuracy']\n",
        "# loss = history_fit.history['loss']\n",
        "# val_loss = history_fit.history['val_loss']\n",
        "# acc = history_fit.history['accuracy'] \n",
        "# epochs = range(len(acc))\n",
        " \n",
        "# plt.plot(epochs, acc, 'b', label='Training acc')\n",
        "# plt.plot(epochs, val_acc, 'r', label='Validation acc')\n",
        "# plt.title('Training and validation accuracy')\n",
        "# plt.legend()\n",
        " \n",
        "# plt.figure()\n",
        " \n",
        "# plt.plot(epochs, loss, 'b', label='Training loss')\n",
        "# plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
        "# plt.title('Training and validation loss')\n",
        "# plt.legend()\n",
        " \n",
        "# plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Depresion - Modelo RNN.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
