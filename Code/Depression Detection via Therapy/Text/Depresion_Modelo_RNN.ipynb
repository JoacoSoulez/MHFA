{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p8H2QUYgRpCH",
    "outputId": "295322f3-3a88-451f-b21a-ce81498eef52"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import sklearn.metrics\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.metrics import classification_report\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the Data from .csv Source and display it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gzC6QIG0pTS_",
    "outputId": "e110f1d7-b8b6-4a5b-f008-2634c53c3a0c"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('train_split_Depression_AVEC2017.csv',delimiter=',',encoding='utf-8')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HjBuVaJ6qQkh",
    "outputId": "4ab18350-1254-4a50-fac7-39485f760ad8"
   },
   "outputs": [],
   "source": [
    "df2 = pd.read_csv('test_split_Depression_AVEC2017.csv',delimiter=',',encoding='utf-8')\n",
    "df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Yc9LEqPyqQYl",
    "outputId": "bfa43ac3-bd1a-4241-e469-051dcdd221aa"
   },
   "outputs": [],
   "source": [
    "df3 = pd.read_csv('dev_split_Depression_AVEC2017.csv',delimiter=',',encoding='utf-8')\n",
    "df3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "fM3BUGKD8DhG",
    "outputId": "264a1760-dfa6-45fc-f08b-1bdd2e1f8e57"
   },
   "outputs": [],
   "source": [
    "df3 = pd.read_csv('full_test_split.csv',delimiter=',',encoding='utf-8')\n",
    "df3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Preparing Data\n",
    "we will get rid of unused columns which are irrelevant for this project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B0twETvVR25Y"
   },
   "outputs": [],
   "source": [
    "dataset1 = np.array(pd.read_csv('dev_split_Depression_AVEC2017.csv',delimiter=',',encoding='utf-8'))[:, 0:2]\n",
    "dataset2 = np.array(pd.read_csv('full_test_split.csv',delimiter=',',encoding='utf-8'))[:, 0:2]\n",
    "dataset3 = np.array(pd.read_csv('train_split_Depression_AVEC2017.csv',delimiter=',',encoding='utf-8'))[:, 0:2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AXGRjaYFSdbc"
   },
   "outputs": [],
   "source": [
    "dataset = np.concatenate((dataset1, np.concatenate((dataset2, dataset3))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vZQsOEuzTP30"
   },
   "outputs": [],
   "source": [
    "pos = []\n",
    "neg = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NrKSeu9aTQJA"
   },
   "outputs": [],
   "source": [
    "def checkPosNeg(dataset, index):\n",
    "    for i in range(0, len(dataset)):\n",
    "        if(dataset[i][0] == index):\n",
    "            return dataset[i][1]\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HXArzf0QTXib"
   },
   "outputs": [],
   "source": [
    "for i in range(len(dataset)):\n",
    "        if(dataset[i][1] == 1):\n",
    "            neg.append(dataset[i][0])\n",
    "        else:\n",
    "            pos.append(dataset[i][0])\n",
    "            \n",
    "pos = np.array(pos)\n",
    "neg = np.array(neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xO5ptzo-8JDS",
    "outputId": "f0d11d31-92f9-4796-b99e-d20cb09c14b8"
   },
   "outputs": [],
   "source": [
    "pos.shape, neg.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import all the transcripts and append the PHQ score as our Y, and the text in a list called Data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7pcOJtgpTjo6"
   },
   "outputs": [],
   "source": [
    "Data = []\n",
    "Y_train = []\n",
    "Data_test = []\n",
    "Y_test = []\n",
    "index = -1\n",
    "for i in range(0, len(dataset3)):\n",
    "    val = checkPosNeg(dataset, dataset3[i][0])\n",
    "    Y.append(val)\n",
    "    fileName = str(int(dataset3[i][0])) + \"_TRANSCRIPT.csv\"\n",
    "    Data.append(np.array(pd.read_csv(fileName,delimiter='\\t',encoding='utf-8'))[:, 2:4])\n",
    "\n",
    "Y_val = []\n",
    "Data_val = []\n",
    "for i in range(0, len(dataset1)):\n",
    "    val = checkPosNeg(dataset, dataset1[i][0])\n",
    "    Y_val.append(val)\n",
    "    fileName = str(int(dataset1[i][0])) + \"_TRANSCRIPT.csv\"\n",
    "    Data_val.append(np.array(pd.read_csv(fileName,delimiter='\\t',encoding='utf-8'))[:, 2:4])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hi-FU5upzW3V"
   },
   "outputs": [],
   "source": [
    "for i in range(0, len(dataset2)):\n",
    "    Y_test.append(checkPosNeg(dataset, dataset2[i][0]))\n",
    "    fileName = str(int(dataset2[i][0])) + \"_TRANSCRIPT.csv\"\n",
    "    Data_test.append(np.array(pd.read_csv(fileName,delimiter='\\t',encoding='utf-8'))[:, 2:4])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l7RuaP_mUBHp"
   },
   "outputs": [],
   "source": [
    "Y_train = np.array(Y_train)\n",
    "Data2 = []\n",
    "Data2_val = []\n",
    "Data2_test = []\n",
    "Y_test = np.array(Y_test)\n",
    "Y_val = np.array(Y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Sj8BgSgU0UZw",
    "outputId": "b8397b36-29ef-4520-d966-e9a2b6390612"
   },
   "outputs": [],
   "source": [
    "np.unique(Y_test, return_counts = True), np.unique(Y, return_counts = True), np.unique(Y_val, return_counts = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We filter only the patient's answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QKKKiqJPUDyu",
    "outputId": "4d4b296e-0663-4221-88a0-661e340299e4"
   },
   "outputs": [],
   "source": [
    "for i in range(0, len(Data)):\n",
    "    script = []\n",
    "    for k in range(1, len(Data[i])):\n",
    "        if(Data[i][k][0] == \"Participant\"):\n",
    "            script.append(Data[i][k][1])\n",
    "    Data2.append(script)\n",
    "    \n",
    "for i in range(0, len(Data_test)):\n",
    "    script = []\n",
    "    for k in range(1, len(Data_test[i])):\n",
    "        if(Data_test[i][k][0] == \"Participant\"):\n",
    "            script.append(Data_test[i][k][1])\n",
    "    Data2_test.append(script)\n",
    "\n",
    "for i in range(0, len(Data_val)):\n",
    "    script = []\n",
    "    for k in range(1, len(Data_val[i])):\n",
    "        if(Data_val[i][k][0] == \"Participant\"):\n",
    "            script.append(Data_val[i][k][1])\n",
    "    Data2_val.append(script)\n",
    "        \n",
    "Data2 = np.array(Data2)\n",
    "Data2_test = np.array(Data2_test)\n",
    "Data2_val = np.array(Data2_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HZ-TDyt6L6j5"
   },
   "source": [
    "Text Cleaning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wh_X18PCMFk8"
   },
   "outputs": [],
   "source": [
    "def stop_words_and_lemmatizer(text):\n",
    "    \"\"\" Remove Stop words from text \"\"\"\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    stopwords.append('m')\n",
    "    stopwords.append('yes')\n",
    "    stopwords.append('uh')\n",
    "    stopwords.append('eh')\n",
    "    \n",
    "    stop_words = set(stopwords)\n",
    "    \n",
    "    word_tokens = nltk.word_tokenize(text) \n",
    "    \n",
    "    without_stopwords = [w for w in word_tokens if not w in stop_words]\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    lemmatized = [lemmatizer.lemmatize(word) for word in without_stopwords]\n",
    "    \n",
    "    return ' '.join(lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tSvaqPw9MPh2"
   },
   "outputs": [],
   "source": [
    "def to_lower(text):\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uoK9jOc1NEFV"
   },
   "outputs": [],
   "source": [
    "def remove_context_symbol(text):\n",
    "    import re\n",
    "    return re.sub('<[^>]+>', '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EYJj9LoxMQc8"
   },
   "outputs": [],
   "source": [
    "def remove_bad_symbols(text):\n",
    "    \"\"\"Remove unwanted symbols from text\"\"\"\n",
    "    bad_symbols = re.compile('[^0-9a-z #+_]')\n",
    "    return bad_symbols.sub(' ', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eOfqvjuVMUtH"
   },
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    for punctuation in string.punctuation:\n",
    "        text = text.replace(punctuation, '')  \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FzP2S7gcMY1q"
   },
   "outputs": [],
   "source": [
    "def expand_contractions(text):\n",
    "    \"\"\" Replace contractions in the english language by the complete phrase\"\"\"\n",
    "    # Contraction list\n",
    "    contractions = {\n",
    "      \"ain't\": \"am not\",\n",
    "      \"aren't\": \"are not\",\n",
    "      \"can't\": \"cannot\",\n",
    "      \"can't've\": \"cannot have\",\n",
    "      \"'cause\": \"because\",\n",
    "      \"could've\": \"could have\",\n",
    "      \"couldn't\": \"could not\",\n",
    "      \"couldn't've\": \"could not have\",\n",
    "      \"didn't\": \"did not\",\n",
    "      \"doesn't\": \"does not\",\n",
    "      \"don't\": \"do not\",\n",
    "      \"hadn't\": \"had not\",\n",
    "      \"hadn't've\": \"had not have\",\n",
    "      \"hasn't\": \"has not\",\n",
    "      \"haven't\": \"have not\",\n",
    "      \"he'd\": \"he would\",\n",
    "      \"he'd've\": \"he would have\",\n",
    "      \"he'll\": \"he will\",\n",
    "      \"he'll've\": \"he will have\",\n",
    "      \"he's\": \"he is\",\n",
    "      \"how'd\": \"how did\",\n",
    "      \"how'd'y\": \"how do you\",\n",
    "      \"how'll\": \"how will\",\n",
    "      \"how's\": \"how is\",\n",
    "      \"I'd\": \"I would\",\n",
    "      \"I'd've\": \"I would have\",\n",
    "      \"I'll\": \"I will\",\n",
    "      \"I'll've\": \"I will have\",\n",
    "      \"I'm\": \"I am\",\n",
    "      \"I've\": \"I have\",\n",
    "      \"isn't\": \"is not\",\n",
    "      \"it'd\": \"it had\",\n",
    "      \"it'd've\": \"it would have\",\n",
    "      \"it'll\": \"it will\",\n",
    "      \"it'll've\": \"it will have\",\n",
    "      \"it's\": \"it is\",\n",
    "      \"let's\": \"let us\",\n",
    "      \"ma'am\": \"madam\",\n",
    "      \"mayn't\": \"may not\",\n",
    "      \"might've\": \"might have\",\n",
    "      \"mightn't\": \"might not\",\n",
    "      \"mightn't've\": \"might not have\",\n",
    "      \"must've\": \"must have\",\n",
    "      \"mustn't\": \"must not\",\n",
    "      \"mustn't've\": \"must not have\",\n",
    "      \"needn't\": \"need not\",\n",
    "      \"needn't've\": \"need not have\",\n",
    "      \"o'clock\": \"of the clock\",\n",
    "      \"oughtn't\": \"ought not\",\n",
    "      \"oughtn't've\": \"ought not have\",\n",
    "      \"shan't\": \"shall not\",\n",
    "      \"sha'n't\": \"shall not\",\n",
    "      \"shan't've\": \"shall not have\",\n",
    "      \"she'd\": \"she would\",\n",
    "      \"she'd've\": \"she would have\",\n",
    "      \"she'll\": \"she will\",\n",
    "      \"she'll've\": \"she will have\",\n",
    "      \"she's\": \"she is\",\n",
    "      \"should've\": \"should have\",\n",
    "      \"shouldn't\": \"should not\",\n",
    "      \"shouldn't've\": \"should not have\",\n",
    "      \"so've\": \"so have\",\n",
    "      \"so's\": \"so is\",\n",
    "      \"that'd\": \"that would\",\n",
    "      \"that'd've\": \"that would have\",\n",
    "      \"that's\": \"that is\",\n",
    "      \"there'd\": \"there had\",\n",
    "      \"there'd've\": \"there would have\",\n",
    "      \"there's\": \"there is\",\n",
    "      \"they'd\": \"they would\",\n",
    "      \"they'd've\": \"they would have\",\n",
    "      \"they'll\": \"they will\",\n",
    "      \"they'll've\": \"they will have\",\n",
    "      \"they're\": \"they are\",\n",
    "      \"they've\": \"they have\",\n",
    "      \"to've\": \"to have\",\n",
    "      \"wasn't\": \"was not\",\n",
    "      \"we'd\": \"we had\",\n",
    "      \"we'd've\": \"we would have\",\n",
    "      \"we'll\": \"we will\",\n",
    "      \"we'll've\": \"we will have\",\n",
    "      \"we're\": \"we are\",\n",
    "      \"we've\": \"we have\",\n",
    "      \"weren't\": \"were not\",\n",
    "      \"what'll\": \"what will\",\n",
    "      \"what'll've\": \"what will have\",\n",
    "      \"what're\": \"what are\",\n",
    "      \"what's\": \"what is\",\n",
    "      \"what've\": \"what have\",\n",
    "      \"when's\": \"when is\",\n",
    "      \"when've\": \"when have\",\n",
    "      \"where'd\": \"where did\",\n",
    "      \"where's\": \"where is\",\n",
    "      \"where've\": \"where have\",\n",
    "      \"who'll\": \"who will\",\n",
    "      \"who'll've\": \"who will have\",\n",
    "      \"who's\": \"who is\",\n",
    "      \"who've\": \"who have\",\n",
    "      \"why's\": \"why is\",\n",
    "      \"why've\": \"why have\",\n",
    "      \"will've\": \"will have\",\n",
    "      \"won't\": \"will not\",\n",
    "      \"won't've\": \"will not have\",\n",
    "      \"would've\": \"would have\",\n",
    "      \"wouldn't\": \"would not\",\n",
    "      \"wouldn't've\": \"would not have\",\n",
    "      \"y'all\": \"you all\",\n",
    "      \"y'alls\": \"you alls\",\n",
    "      \"y'all'd\": \"you all would\",\n",
    "      \"y'all'd've\": \"you all would have\",\n",
    "      \"y'all're\": \"you all are\",\n",
    "      \"y'all've\": \"you all have\",\n",
    "      \"you'd\": \"you had\",\n",
    "      \"you'd've\": \"you would have\",\n",
    "      \"you'll\": \"you will\",\n",
    "      \"you'll've\": \"you will have\",\n",
    "      \"you're\": \"you are\",\n",
    "      \"you've\": \"you have\"}\n",
    "    \n",
    "    contractions = dict((k.lower(), v.lower()) for k,v in contractions.items())\n",
    "\n",
    "    c_re = re.compile('(%s)' % '|'.join(contractions.keys()))\n",
    "    \n",
    "    def replace(match):\n",
    "        return contractions[match.group(0)]\n",
    "    return c_re.sub(replace, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q7jI6FJA3usq"
   },
   "outputs": [],
   "source": [
    "import string \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "for n in range(len(Data2)):\n",
    "  Data2[n] = to_lower(str(Data2[n]))\n",
    "  Data2[n] = expand_contractions(str(Data2[n]))\n",
    "  Data2[n] = remove_context_symbol(str(Data2[n]))\n",
    "  Data2[n] = remove_bad_symbols(str(Data2[n]))\n",
    "  Data2[n] = remove_punctuation(str(Data2[n]))\n",
    "  Data2[n] = stop_words_and_lemmatizer(str(Data2[n]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3cLnxmneGPmM"
   },
   "outputs": [],
   "source": [
    "for n in range(len(Data2_test)):\n",
    "  Data2_test[n] = to_lower(str(Data2_test[n]))\n",
    "  Data2_test[n] = expand_contractions(str(Data2_test[n]))\n",
    "  Data2_test[n] = remove_context_symbol(str(Data2_test[n]))\n",
    "  Data2_test[n] = remove_bad_symbols(str(Data2_test[n]))\n",
    "  Data2_test[n] = remove_punctuation(str(Data2_test[n]))\n",
    "  Data2_test[n] = stop_words_and_lemmatizer(str(Data2_test[n]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DmkORujo_6mo"
   },
   "outputs": [],
   "source": [
    "for n in range(len(Data2_val)):\n",
    "  Data2_val[n] = to_lower(str(Data2_val[n]))\n",
    "  Data2_val[n] = expand_contractions(str(Data2_val[n]))\n",
    "  Data2_val[n] = remove_context_symbol(str(Data2_val[n]))\n",
    "  Data2_val[n] = remove_bad_symbols(str(Data2_val[n]))\n",
    "  Data2_val[n] = remove_punctuation(str(Data2_val[n]))\n",
    "  Data2_val[n] = stop_words_and_lemmatizer(str(Data2_val[n]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mYHo3vTxR9rq"
   },
   "outputs": [],
   "source": [
    "X_test = Data2_test\n",
    "X_train = Data2\n",
    "X_val = Data2_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SVyZzLFMNxJq"
   },
   "source": [
    "Encoding the target variable using Label Encoder from the sklearn library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "omZdrZyo7eoN"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "lb=LabelEncoder()\n",
    "Y_train = lb.fit_transform(Y_train)\n",
    "Y_val = lb.transform(Y_val)\n",
    "Y_test = lb.transform(Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizing and converting the reviews into numerical vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i5LFgqNK7yLt"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer = Tokenizer(num_words=500, split=' ') \n",
    "\n",
    "\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_train = pad_sequences(X_train, value=-1000, dtype='float64')\n",
    "\n",
    "\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "X_test = pad_sequences(X_test, maxlen = X_train.shape[1], value=-1000, dtype='float64')\n",
    "\n",
    "\n",
    "\n",
    "X_val = tokenizer.texts_to_sequences(X_val)\n",
    "X_val = pad_sequences(X_val, maxlen = X_train.shape[1], value=-1000, dtype='float64')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tDUpvL0Mdx7A"
   },
   "outputs": [],
   "source": [
    "X_train = np.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1]))\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], 1, X_test.shape[1]))\n",
    "X_val = np.reshape(X_val, (X_val.shape[0], 1, X_val.shape[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_isIyLdTyicL",
    "outputId": "df00d135-21d7-4fee-c949-f97f8a11caf4"
   },
   "outputs": [],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(\"Total words\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UYQhJ1wUMmbQ",
    "outputId": "770c2c6b-d198-4927-f559-9edcca0c3745"
   },
   "outputs": [],
   "source": [
    "# X_experimental = np.expand_dims(X ,axis = 0)\n",
    "\n",
    "# X_experimental.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RVkQgatAcygC",
    "outputId": "6952b265-a271-4ab6-ceff-24cf6069310f"
   },
   "outputs": [],
   "source": [
    "Y_train = Y_train.astype(int)\n",
    "Y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the GRU model using the Keras library. This step involves model initialization, adding required GRU layers, and model compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Yjp3TeJx43dk",
    "outputId": "e1fa0854-67b3-4452-e757-a06f267057f5"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.layers import Activation, Dense\n",
    "from keras.layers import Input\n",
    "def init_model_2():\n",
    "  model = Sequential()\n",
    "  #input = Input(shape=(142, 1353))\n",
    "  initial_learning_rate = 0.001\n",
    "  lr_schedule = ExponentialDecay(initial_learning_rate, decay_steps=2000, decay_rate=0.5)\n",
    "  adam = Adam(learning_rate=lr_schedule)\n",
    "  model.add(layers.Masking(mask_value=-1000))\n",
    "  model.add(layers.GRU(512, return_sequences=True, activation='tanh'))\n",
    "  model.add(layers.GRU(256, return_sequences=True, activation='tanh'))\n",
    "  model.add(layers.GRU(128, return_sequences=True, activation='tanh'))\n",
    "  model.add(layers.GRU(256, return_sequences=True, activation='tanh'))\n",
    "  model.add(layers.GRU(128, return_sequences=True, activation='tanh', dropout= 0.03))\n",
    "  model.add(layers.GRU(64, return_sequences=True, activation='tanh', dropout=0.02))\n",
    "  model.add(layers.GRU(32, return_sequences=True, activation='tanh',dropout=0.01))\n",
    "  model.add(layers.Flatten())\n",
    "  model.add(Dense(16, activation = 'tanh'))\n",
    "  model.add(Dense(1, activation = 'sigmoid'))\n",
    "  \n",
    "  model.compile(\n",
    "      optimizer=adam,\n",
    "      loss=\"binary_crossentropy\",\n",
    "      metrics=['accuracy', 'AUC','Precision','Recall'])\n",
    "  return model\n",
    "\n",
    "model = init_model_2()\n",
    "\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "es = EarlyStopping(patience=5, restore_best_weights=True)\n",
    "history_fit = model.fit(X, Y, \n",
    "          batch_size = 32,\n",
    "          epochs=100,\n",
    "          validation_data = (X_val, Y_val),\n",
    "          #callbacks=[es],\n",
    "          verbose = 1\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pW2GwLe2NPZM",
    "outputId": "9e8afbca-2731-40c9-ce7a-d55cb66f425a"
   },
   "outputs": [],
   "source": [
    "model.summary() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JDu33lH7TzWp"
   },
   "outputs": [],
   "source": [
    "Y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kjB0nzvmeQRu"
   },
   "outputs": [],
   "source": [
    "y_pred = [1 if value[0] >= 0.5 else 0 for value in Y_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j8PTk0qUsgcF",
    "outputId": "69072832-2b1a-4dc4-fcb4-a761dd2bd2ab"
   },
   "outputs": [],
   "source": [
    "np.unique(y_pred, return_counts = True), np.unique(Y_test, return_counts = True) #array([25081,   277]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JZdduN-Qhrzt"
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# val_acc = history_fit.history['val_accuracy']\n",
    "# loss = history_fit.history['loss']\n",
    "# val_loss = history_fit.history['val_loss']\n",
    "# acc = history_fit.history['accuracy'] \n",
    "# epochs = range(len(acc))\n",
    " \n",
    "# plt.plot(epochs, acc, 'b', label='Training acc')\n",
    "# plt.plot(epochs, val_acc, 'r', label='Validation acc')\n",
    "# plt.title('Training and validation accuracy')\n",
    "# plt.legend()\n",
    " \n",
    "# plt.figure()\n",
    " \n",
    "# plt.plot(epochs, loss, 'b', label='Training loss')\n",
    "# plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "# plt.title('Training and validation loss')\n",
    "# plt.legend()\n",
    " \n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Depresion - Modelo RNN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
